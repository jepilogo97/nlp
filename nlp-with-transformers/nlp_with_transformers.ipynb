{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377b63a1",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jepilogo97/nlp/blob/main/nlp-with-lstm/nlp_with_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "# NLP con Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "1"
   },
   "source": [
    "##### Jean Pierre Londoño González\n",
    "##### Mini-Proyecto de clasificación de texto con Transformers\n",
    "##### 30AGO2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "2"
   },
   "source": [
    "En este notebook se implementa un clasificador de conversaciones cotidianas en español utilizando transformers. El dataset empleado corresponde a Everyday Conversations LLaMA 3.1 - 2k, disponible en Hugging Face, el cual contiene diálogos en español sobre diferentes temas. Para la preparación de los datos se realiza la tokenización empleando las utilidades de la librería Hugging Face Transformers.\n",
    "\n",
    "#### Referencias\n",
    "- Dataset: https://huggingface.co/datasets/HuggingFaceTB/everyday-conversations-llama3.1-2k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "3"
   },
   "source": [
    "### 1. Importación de librerias y carga de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "4"
   },
   "source": [
    "Inicio importando las librerías necesarias para el procesamiento de lenguaje natural, la manipulación de datos y la construcción del modelo. Esto incluye NumPy y pandas para el manejo y análisis de datos; Hugging Face Datasets y Transformers para la carga de corpus y la tokenización; y PyTorch junto con PyTorch Lightning para definir, entrenar y evaluar el modelo de manera estructurada.\n",
    "\n",
    "Además, se emplean torchmetrics y scikit-learn para calcular métricas de rendimiento como precisión, matrices de confusión y reportes de clasificación, mientras que Optuna permite la optimización automática de hiperparámetros. \n",
    "\n",
    "Finalmente, tqdm facilita el seguimiento del progreso de los procesos iterativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "232f9b07-b6e9-4e9b-96fa-697d4f04b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "installed_packages = [package.key for package in pkg_resources.working_set]\n",
    "IN_COLAB = 'google-colab' in installed_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5",
    "outputId": "ab7bbdd7-dd8b-4b3d-8fa2-55f5628982a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "!wget -O requirements.txt https://raw.githubusercontent.com/jepilogo97/nlp/main/nlp-with-transformers/requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6",
   "metadata": {
    "id": "6"
   },
   "outputs": [],
   "source": [
    "# Procesamiento de lenguaje natural y utilidades\n",
    "import numpy as np  # Cálculo numérico y manejo de arreglos multidimensionales\n",
    "import pandas as pd  # Manipulación y análisis de datos en estructuras tipo DataFrame\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)     # Todas las filas\n",
    "pd.set_option(\"display.max_columns\", None)  # Todas las columnas\n",
    "pd.set_option(\"display.width\", None)        # No cortar líneas\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets  # Carga y combinación de datasets de Hugging Face\n",
    "from collections import Counter  # Conteo de frecuencias de elementos (tokens, palabras, etc.)\n",
    "import os  # Manejo de rutas, archivos y operaciones del sistema de archivos\n",
    "import math  # Funciones matemáticas avanzadas (logaritmos, potencias, trigonometría, etc.)\n",
    "\n",
    "# Deep Learning con PyTorch\n",
    "import torch  # Librería principal de tensores y operaciones en GPU/CPU\n",
    "import torch.nn as nn  # Definición de capas y módulos de redes neuronales\n",
    "import torch.nn.functional as F  # Funciones de activación y operaciones matemáticas de redes\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, Subset  # Utilidades para crear y dividir datasets, cargar lotes y trabajar con subconjuntos\n",
    "\n",
    "# Entrenamiento estructurado con PyTorch Lightning\n",
    "from pytorch_lightning import LightningModule, Trainer  # Clase base y manejador de entrenamiento de modelos\n",
    "from pytorch_lightning.loggers import TensorBoardLogger  # Registro de métricas e historial en TensorBoard\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping  # Detener entrenamiento si no mejora la métrica\n",
    "from torchmetrics import Accuracy  # Métrica de precisión para clasificación supervisada\n",
    "\n",
    "# Tipado para mayor legibilidad y validación de funciones\n",
    "from typing import Tuple, Dict, Optional  # Definición de tipos de datos para funciones y estructuras\n",
    "from enum import Enum  # Definición de enumeraciones (conjuntos de valores constantes con nombre)\n",
    "\n",
    "from tqdm.auto import tqdm  # Barra de progreso adaptable para bucles\n",
    "from transformers import AutoTokenizer  # Tokenizador automático de modelos preentrenados de Hugging Face\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode  # Conversión de bytes a caracteres Unicode (usado en tokenización tipo GPT-2)\n",
    "\n",
    "import optuna  # Optimización automática de hiperparámetros mediante búsquedas eficientes (Bayesian, TPE, etc.)\n",
    "\n",
    "# Métricas de evaluación con Scikit-learn\n",
    "from sklearn.model_selection import train_test_split  # División de datos en conjuntos de entrenamiento y prueba\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # Métricas de evaluación de modelos de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "7"
   },
   "source": [
    "### 2. Cargue de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "source": [
    "Este dataset contiene 2.2k conversaciones multi-turno generadas por Llama-3.1-70B-Instruct. Se le solicitó al modelo generar una conversación sencilla, con 3 a 4 intercambios breves, entre un Usuario y un Asistente de IA sobre un tema específico.\n",
    "\n",
    "De este dataset se utilizarán específicamente las conversaciones entre el usuario y el asistente de IA, con el fin de asignarlas al tópico correspondiente. Cada ejemplo consiste en un diálogo corto multi-turno, donde el modelo debe identificar el tema principal a partir del intercambio entre ambas partes.\n",
    "\n",
    "Está disponible en el Hugging Face Hub, lo que permite descargarlo fácilmente y utilizarlo directamente en entrenamientos o pruebas de modelos de NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9",
   "metadata": {
    "id": "9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_sft: Dataset({\n",
       "        features: ['topic', 'subtopic', 'subsubtopic', 'full_topic', 'prompt', 'completion', 'token_length', 'messages'],\n",
       "        num_rows: 2260\n",
       "    })\n",
       "    test_sft: Dataset({\n",
       "        features: ['topic', 'subtopic', 'subsubtopic', 'full_topic', 'prompt', 'completion', 'token_length', 'messages'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "dataset = load_dataset(\"HuggingFaceTB/everyday-conversations-llama3.1-2k\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "source": [
    "El dataset incluye varias columnas pero me centraré en dos de ellas: La conversación entre el user y la AI (completion), la categoría a la que pertenece la conversación (topic). Para entrenar y validar el modelo, se dispone de 2.260 filas en el conjunto de entrenamiento y 119 en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "11"
   },
   "source": [
    "Concatenos los dos datasets para tener mayor cantidad de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12",
   "metadata": {
    "id": "12"
   },
   "outputs": [],
   "source": [
    "tr_dataset = dataset[\"train_sft\"]\n",
    "te_dataset = dataset[\"test_sft\"]\n",
    "full_dataset = concatenate_datasets([tr_dataset, te_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "13"
   },
   "source": [
    "Observemos uno de sus registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14",
   "metadata": {
    "id": "14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Shopping',\n",
       " 'subtopic': 'Budgeting',\n",
       " 'subsubtopic': 'Tracking expenses',\n",
       " 'full_topic': 'Shopping/Budgeting/Tracking expenses',\n",
       " 'prompt': 'Generate a very simple multi-turn conversation between a User and an AI Assistant about Shopping/Budgeting/Tracking expenses. The conversation should start with a basic greeting like \"Hello\" or \"Hi\" and be straightforward. Include 3-4 short exchanges. The AI should give brief, clear answers. The User should ask simple questions.\\n\\nStart the conversation like this:\\n\\nUser: [Greeting]\\n\\nAI: Hello! How can I help you today?\\n\\nUser: [Continue with a simple question or statement]\\n\\nAI: [Respond briefly and clearly]\\n\\nUser: [Ask a follow-up question or make another simple statement]\\n\\nAI: [Provide a final helpful response]\\n\\nMake sure the entire conversation remains very simple and easy to understand, focusing on basic topics or requests.',\n",
       " 'completion': \"User: Hi\\n\\nAI: Hello! How can I help you today?\\n\\nUser: I'm trying to track my expenses. Can you help me with that?\\n\\nAI: Yes, I can help you track your expenses. You can start by telling me your income and fixed expenses, such as rent and utilities.\\n\\nUser: That sounds easy. How do I know if I'm staying within my budget?\\n\\nAI: Once you've entered your income and expenses, I can help you set a budget and alert you when you're going over. You can also categorize your spending to see where your money is going.\\n\\nUser: Okay, that sounds great. Can you remind me to review my budget regularly?\\n\\nAI: I can send you reminders to review your budget weekly, monthly, or at any interval you prefer.\",\n",
       " 'token_length': 163,\n",
       " 'messages': [{'content': 'Hey!', 'role': 'user'},\n",
       "  {'content': 'Hello! How can I help you today?', 'role': 'assistant'},\n",
       "  {'content': \"I'm trying to track my expenses. Can you help me with that?\",\n",
       "   'role': 'user'},\n",
       "  {'content': 'Yes, I can help you track your expenses. You can start by telling me your income and fixed expenses, such as rent and utilities.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"That sounds easy. How do I know if I'm staying within my budget?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"Once you've entered your income and expenses, I can help you set a budget and alert you when you're going over. You can also categorize your spending to see where your money is going.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Okay, that sounds great. Can you remind me to review my budget regularly?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'I can send you reminders to review your budget weekly, monthly, or at any interval you prefer.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d63b6-4c7e-406c-83b6-e874f859198f",
   "metadata": {},
   "source": [
    "Revisamos especificamente las 2 columnas de interes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdb4acdd-ae34-4c59-9ce5-43797cb784f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = full_dataset.select_columns([\"completion\", \"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dea978-374a-42cf-b9f8-e9af07ca24d9",
   "metadata": {},
   "source": [
    "Ajusto el nombre de las columnas a text y category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b8f4333-3d5b-490d-a927-f08660a48cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = full_dataset.rename_column(\"completion\", \"text\")\n",
    "full_dataset = full_dataset.rename_column(\"topic\", \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51575f90-963c-4abe-8563-c3eb9dcd7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = full_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba926f23-32f2-4cf5-bebe-4dc4293e9f5e",
   "metadata": {},
   "source": [
    "Ahora observemos uno de los registros ya ajustado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4339015d-c6f5-429b-8ea2-d75722e702ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"User: Hi\\n\\nAI: Hello! How can I help you today?\\n\\nUser: I'm trying to track my expenses. Can you help me with that?\\n\\nAI: Yes, I can help you track your expenses. You can start by telling me your income and fixed expenses, such as rent and utilities.\\n\\nUser: That sounds easy. How do I know if I'm staying within my budget?\\n\\nAI: Once you've entered your income and expenses, I can help you set a budget and alert you when you're going over. You can also categorize your spending to see where your money is going.\\n\\nUser: Okay, that sounds great. Can you remind me to review my budget regularly?\\n\\nAI: I can send you reminders to review your budget weekly, monthly, or at any interval you prefer.\",\n",
       " 'category': 'Shopping'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebfa4837-af92-498d-a3d7-1ae5ed59371b",
   "metadata": {
    "id": "15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cooking                  100\n",
       "Travel                   100\n",
       "Sports                   100\n",
       "Health                   100\n",
       "Pets                     100\n",
       "Technology               100\n",
       "Work                     100\n",
       "Education                100\n",
       "Weather                  100\n",
       "Fashion                  100\n",
       "Transportation           100\n",
       "Hobbies                  100\n",
       "Food                     100\n",
       "Sleep                    100\n",
       "Fitness                  100\n",
       "Home                     100\n",
       "Entertainment            100\n",
       "Family                   100\n",
       "Shopping                  99\n",
       "Music                     50\n",
       "communication             10\n",
       "culture                   10\n",
       "environmental science     10\n",
       "plants                    10\n",
       "food chains               10\n",
       "everyday objects          10\n",
       "simple machines           10\n",
       "sound                     10\n",
       "astronomy                 10\n",
       "genetics and heredity     10\n",
       "basic math concepts       10\n",
       "time                      10\n",
       "ecosystems                10\n",
       "light                     10\n",
       "animals                   10\n",
       "physics basics            10\n",
       "numbers                   10\n",
       "water cycle               10\n",
       "matter                    10\n",
       "pollution                 10\n",
       "history                   10\n",
       "climate                   10\n",
       "magnets                   10\n",
       "seasons                   10\n",
       "measurement               10\n",
       "health and hygiene        10\n",
       "geology                   10\n",
       "chemistry basics          10\n",
       "earth                     10\n",
       "community                 10\n",
       "shapes                    10\n",
       "human body                10\n",
       "energy                    10\n",
       "life cycles               10\n",
       "nutrition                 10\n",
       "language                  10\n",
       "adaptation                10\n",
       "colors                    10\n",
       "forces                    10\n",
       "arts and music            10\n",
       "geography                 10\n",
       "space                     10\n",
       "sports and exercise       10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dataset['category']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "16"
   },
   "source": [
    "Se encuentra que el dataset esta desbalanceado, pues algunas categorias tienes 100 registros pero otros 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b94d5e-7be9-4a71-ab48-f091786050a4",
   "metadata": {},
   "source": [
    "Para efectos de la tarea solo utilizaremos las categorias que tienen una muestra suficiente y permite tener el dataset balanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9841518e-b54c-41f8-852f-763440e1c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(dataset['category'])\n",
    "dataset = dataset.filter(lambda example: counts[example['category']] > 98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c77e89a5-c454-494c-a33e-3cd122f249e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cooking           100\n",
       "Travel            100\n",
       "Work              100\n",
       "Transportation    100\n",
       "Technology        100\n",
       "Pets              100\n",
       "Health            100\n",
       "Sports            100\n",
       "Weather           100\n",
       "Education         100\n",
       "Fashion           100\n",
       "Family            100\n",
       "Sleep             100\n",
       "Food              100\n",
       "Hobbies           100\n",
       "Entertainment     100\n",
       "Fitness           100\n",
       "Home              100\n",
       "Shopping           99\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dataset['category']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52652b0d-8446-4e44-a9b8-d21e29f63e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset['category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee8321e1-edc9-4ed4-b1e1-2ccaec9fe0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1899, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c942c-853d-4abd-92cf-7b5fef88cb04",
   "metadata": {},
   "source": [
    "Finalmente quedamos con 19 categorias balanceadas y un total de 1899 registros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "17"
   },
   "source": [
    "A manera general, observemos que tan largos o cortos tienden a ser los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18",
   "metadata": {
    "id": "18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto más corto: 270\n",
      "Texto más largo: 1445\n",
      "Longitud promedio: 747.9057398630858\n"
     ]
    }
   ],
   "source": [
    "text_lengths = [len(row['text']) for row in dataset]\n",
    "print(f\"Texto más corto: {min(text_lengths)}\")\n",
    "print(f\"Texto más largo: {max(text_lengths)}\")\n",
    "print(f\"Longitud promedio: {sum(text_lengths) / len(text_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "19"
   },
   "source": [
    "Estos valores son la cantidad de *caractéres* que tiene las secuencias. Una decisión ingenua pero útil en este momento podría ser ajustar la longitud de las secuencias que vamos a usar para el entrenamiento a unos 350 tokens. Esto podría ser suficiente para capturar una porción significativa de los textos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "20"
   },
   "source": [
    "### 3. Definición del Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "21"
   },
   "source": [
    "Ahora, vamos a definir el tokenizer para nuestra tarea. Para ahorrarnos tiempo, vamos a entrenar uno basado en gpt2, pero ajustandolo a nuestro dataset. Para ello, debemos seleccionar una muestra representativa de nuestro dataset, como no es muy grande, casi que podemos usarlo todo. Luego, debemos definir el tamaño del vocabulario, es decir, cuantos tokens únicos queremos soportar en nuestro tokenizador. Para que un modelo de lenguaje funcione moderadamente bien para una tarea de clasificación, considerando el tamaño de nuestro corpus, deberíamos definir unos 50 mil tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22",
   "metadata": {
    "id": "22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 376.98it/s]\n"
     ]
    }
   ],
   "source": [
    "length = 350\n",
    "iter_dataset = iter(dataset)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "def batch_iterator(batch_size: int = 10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['text'] for _ in range(batch_size)]\n",
    "\n",
    "english_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=50000, initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "24"
   },
   "source": [
    "Exploremos ahora el tokenizador obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25",
   "metadata": {
    "id": "25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: 8811 tokens\n",
      "Primeros 15 tokens:\n",
      "['<|endoftext|>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.']\n",
      "15 tokens de en medio:\n",
      "[' might', 'ening', ' over', 'ish', ' hob', ' rem', 'ables', ' Ch', ' exper', ' mod', ' learn', ' avail', ' something', ' hobb', \"'ve\"]\n",
      "Últimos 15 tokens:\n",
      "[' lasagna', ' dynamics', ' Superhero', ' volunteering', ' deadheading', ' journalism', ' disagreements', ' Vincennes', 'Bridgerton', 'Cyberpunk', ' Queenstown', ' civilizations', ' pedestrian', ' Hedgehogs', ' inexperienced']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(english_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print(f\"Vocabulario: {english_tokenizer.vocab_size} tokens\")\n",
    "print(\"Primeros 15 tokens:\")\n",
    "print([f\"{english_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[:15]])\n",
    "print(\"15 tokens de en medio:\")\n",
    "print([f\"{english_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[1000:1015]])\n",
    "print(\"Últimos 15 tokens:\")\n",
    "print([f\"{english_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[-15:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "26"
   },
   "source": [
    "Vemos que los primeros tokens corresponden a caracteres especiales y puntiación. Luego en el medio tenemos una combinación entre palabras completas y cortadas, el tokenizador se encarga de encontrar las frecuencias más comunes y asi partir las palabras por aquellas partes que tienden a repetirse mas. Esto es muy útil para trabajar con modelos de lenguaje ya que el modelo se vuelve robusto a diferentes ramificaciones de palabras e incluso a errores de tipografía. Finalmente, al final, vemos que tenemos más palabras cortadas y palabras muy especiales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "28"
   },
   "source": [
    "### 4. Definición del dataset de pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "29"
   },
   "source": [
    "Ahora podemos proceder a definir el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30",
   "metadata": {
    "id": "30"
   },
   "outputs": [],
   "source": [
    "class EnglishCatDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, dataset, seq_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.pad_token = '[PAD]'\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.id_2_class_map = dict(enumerate(np.unique(dataset[:]['category'])))\n",
    "        self.class_2_id_map = {v: k for k, v in self.id_2_class_map.items()}\n",
    "        self.num_classes = len(self.id_2_class_map)\n",
    "\n",
    "    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n",
    "        text, y = self.dataset[index]['text'], self.dataset[index]['category']\n",
    "        y = self.class_2_id_map[y]\n",
    "        data = {k: torch.tensor(v) for k, v in self.tokenizer(text, max_length=self.seq_length, truncation=True, padding='max_length').items()}\n",
    "        data['y'] = torch.tensor(y)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "31"
   },
   "source": [
    "Ahora instanciaremos el dataset entero. Para este experimento, definiremos un tamaño máximo de secuencia de 2048 **tokens**. Que según nuestra intuición arriba, debería ser suficiente para la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32",
   "metadata": {
    "id": "32"
   },
   "outputs": [],
   "source": [
    "max_len = 350 \n",
    "english_cat_dataset = EnglishCatDataset(english_tokenizer, dataset, seq_length=max_len)\n",
    "assert len(english_cat_dataset) == len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "33"
   },
   "source": [
    "Y luego, procedemos a hacer el train-val-test split y crear los dataloaders. Se mantienen las proporciones de cada categoria en cada dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35",
   "metadata": {
    "id": "35"
   },
   "outputs": [],
   "source": [
    "batch_size = 4 if not IN_COLAB else 12\n",
    "train_dataset, val_dataset, test_dataset = random_split(english_cat_dataset, lengths=[0.8, 0.1, 0.1])\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0bf6ea-a2bb-4276-b355-9e2083eda65f",
   "metadata": {},
   "source": [
    "### Definición de los Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56251dff-d7d1-42dc-82d8-4cafa039d2dd",
   "metadata": {},
   "source": [
    "Según el paper, los autores agregan una secuencia sinusoidal a los embeddings de los tokens con el fin de inyectar información referente a la posición de cada token en las frases. Esto obedece a la definición:\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin(pos/10000^{2i/d_{model}}) \\\\\n",
    "PE(pos, 2i + 1) = \\cos(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "\n",
    "Donde: \n",
    "- $pos$ es la posición del *token* en la secuencia.\n",
    "- $i$ es la dimensión $i$ en el embedding $d$.\n",
    "- $d_model$ es la dimensionalidad total del embedding.\n",
    "\n",
    "Lo que los autores propusieron fue que para las posiciones pares, se calculara el seno de la posición, relativa a la dimensionalidad del embedding y para las posiciones impares, se calculara el coseno. Según los autores, estos tenían la hipótesis de que estas funciones inyectarían la información posicional relativa de forma eficiente, en parte porque se pueden pre-calcular e inyectar directamente durante el entrenamiento, evitando asi emplear recursos en entrenar estructuras para aprenderlos.\n",
    "\n",
    "Esto último es particularmente importante ya que se evita tanto hacer uso de recursos innecesarios como acelerar el proceso de entrenamiento al no tener que computar gradientes para esta parte. Sin embargo, los autores también mencionaron que es ciertamente posible aprender estos positional embeddings como parte del entrenamiento y que según sus resultados, no había mucha diferencia entre ambos enfoques, razón por la cual, se prefiere el positional encoding sinusoidal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b53a1b4-7491-46e9-b113-004a5b11d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEncodingType(Enum):\n",
    "    SINUSOID = 1\n",
    "    LEARNABLE = 2\n",
    "\n",
    "\n",
    "class SinusoidPE(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len: int, d_model: int):\n",
    "        super(SinusoidPE, self).__init__()\n",
    "        \n",
    "        # Definimos un vector columna con las posiciones de la secuencia de entrada (pos)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        # Definimos un vector de fila con las dimensiones del embedding (i)\n",
    "        i = torch.arange(d_model).unsqueeze(0)\n",
    "\n",
    "        # Calculamos el denominador segun la formula\n",
    "        div_term = 1 / torch.pow(10000, (2 * (i // 2)) / torch.tensor(d_model, dtype=torch.float32))\n",
    "        # Aplicamos el denominador a las posiciones\n",
    "        angle_rads = pos * div_term\n",
    "\n",
    "        # Inicializamos la matriz de positional encodings\n",
    "        pos_encoding = torch.zeros(max_len, d_model)\n",
    "        # Calculamos los embeddings para los numeros pares con seno: PE(pos, 2i)\n",
    "        pos_encoding[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "        # Calculamos los embdeddings para los numeros inpares con coseno: PE(pos, 2i+1)\n",
    "        pos_encoding[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        # Registramos la variable como atributo de clase\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding.unsqueeze(0), persistent=False)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pos_encoding[:, :x.size(1), :]\n",
    "    \n",
    "\n",
    "class LearnablePE(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, d_model: int, max_len: int = float('-inf')):\n",
    "        super(LearnablePE, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        positions = torch.arange(0, max(x.size(-1), self.max_len))\n",
    "        pos_emb = self.embedding(positions)\n",
    "        return x + pos_emb\n",
    "\n",
    "\n",
    "\n",
    "class TokenAndPosEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len: int, embed_dim: int, vocab_size: int, pos_encoding_type: PosEncodingType = PosEncodingType.SINUSOID):\n",
    "        super(TokenAndPosEmbedding, self).__init__()\n",
    "        self.token_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        if pos_encoding_type == PosEncodingType.SINUSOID:\n",
    "            self.pos_emb = SinusoidPE(max_len, embed_dim)\n",
    "        else:\n",
    "            self.pos_emb = LearnablePE(vocab_size, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.token_emb(x)\n",
    "        return self.pos_emb(token_emb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749e3e8-a2a6-4ea4-b622-74f08fcf5921",
   "metadata": {},
   "source": [
    "Ahora procedemos a instanciar el modulo que va a convertir los tokens en embeddings con positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25fc3ec3-7cd7-4bea-ab7b-506cb4a0bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128 if not IN_COLAB else 256\n",
    "tpe = TokenAndPosEmbedding(max_len, emb_dim, english_tokenizer.vocab_size)\n",
    "pos_encoding = tpe.pos_emb.pos_encoding.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db91b4b7-bff5-467d-9b72-d608ba85e2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 350, 128])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hola mundo!\"\n",
    "tokens = english_tokenizer(text, max_length=max_len, truncation=True, padding='max_length')\n",
    "x = torch.tensor(tokens['input_ids']).unsqueeze(0)\n",
    "mask = torch.tensor(tokens['input_ids']).unsqueeze(0)\n",
    "embedding = tpe(x)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7581aa6-0415-48f7-98d2-f043aa68ab6c",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866991e-cf92-4d6c-afd4-9fd1b4385bfb",
   "metadata": {},
   "source": [
    "Ahora procedemos a definir al núcleo del modelo. Recodemos que la atención se define por:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_K}})V\n",
    "$$\n",
    "\n",
    "Que es la definición de \"Scaled Dot-Product Attention\". Y Multi-Head Attention es la concatenación de varias cabezas ejecutando el mismo scaled dot-product sobre partes del input. Entonces tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b008da5-4797-43a3-8704-488aba982257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size: int, num_heads: int = 8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_size & num_heads == 0, 'El tamaño del embedding debería ser divisible por el numero de cabezas'\n",
    "        self.projection_dim = embed_size // num_heads\n",
    "        self.query = nn.Linear(emb_dim, emb_dim)\n",
    "        self.key = nn.Linear(emb_dim, emb_dim)\n",
    "        self.value = nn.Linear(emb_dim, emb_dim)\n",
    "        self.comibe_heads = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _scaled_dot_product(q, k, v, mask=None):\n",
    "        # d_k para el escalamiento\n",
    "        d_k = q.size()[-1]\n",
    "\n",
    "        # multiplicacion Q \\cdot K^T \n",
    "        attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "        # escalamiento\n",
    "        attn_logits = attn_logits / math.sqrt(d_k)\n",
    "        \n",
    "        # Se aplica la máscara\n",
    "        if mask is not None:\n",
    "            attn_logits = attn_logits.masked_fill(mask.reshape(mask.shape[0], 1, 1, -1) == 0, -9e-15)\n",
    "\n",
    "        # Se calcula el score de atención.\n",
    "        attention = torch.softmax(attn_logits, dim=-1)\n",
    "        # Se obtienen los valores tras el score de atención.\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "    \n",
    "\n",
    "    def _separate_heads(self, x, batch_size):\n",
    "        # Llega: (batch, seq_len, emb_dim)\n",
    "        x =  x.reshape(batch_size, -1, self.num_heads, self.projection_dim)  # (batch, seq_len, num_heads, emb_dim / num_heads)\n",
    "        return x.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, emb_dim / num_heads)\n",
    "    \n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        # x: (batch, seq_len, emb_dim)\n",
    "        batch_size, seq_len, emb_dim = x.size()\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        q = self._separate_heads(q, batch_size)\n",
    "        k = self._separate_heads(k, batch_size)\n",
    "        v = self._separate_heads(v, batch_size)\n",
    "\n",
    "        weights, attention = self._scaled_dot_product(q, k, v, mask)\n",
    "        weights = weights.permute(0, 2, 1, 3) # (batch, seq_len, num_heads, emb_dim / num_heads)\n",
    "        weights = weights.reshape(batch_size, seq_len, emb_dim)\n",
    "        output = self.comibe_heads(weights)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attention\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5aee7-ea38-4419-9db4-a7ac41e03feb",
   "metadata": {},
   "source": [
    "Podemos hacer una prueba rápida de que las operaciones funcionan a nivel de matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75d55402-b148-43d8-8021-7c3edacdd35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 350, 128])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(emb_dim)\n",
    "mha(embedding, mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317d971-a169-4f89-b093-9db4853aceca",
   "metadata": {},
   "source": [
    "### Definición del bloque transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ddfb7e-647c-4554-93fb-6861975a42a1",
   "metadata": {},
   "source": [
    "Finalmente, definimos el bloque de transformers. Recordemos que como esta es una tarea de clasificación, solamente necesitamos el encoder, por lo que esto es silamente la primera parte del diseño de arquitecura de red.\n",
    "\n",
    "En esta capa, simplemente ponemos una capa densa adicional junto con las normalizaciones a nivel de capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa61c142-a0a7-42cf-a0fc-ae82ed2c9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim: int, num_heads: int = 8):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.mhatt = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.mhatt_dropput = nn.Dropout(0.2)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, emb_dim)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.mhatt(x, mask)\n",
    "        attn_output = self.mhatt_dropput(attn_output)\n",
    "        attn_output = self.layer_norm1(attn_output)\n",
    "        ffn_out = self.ffn(attn_output)\n",
    "        return self.layer_norm2(ffn_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9737f17-95b2-44fd-8ad7-8c67844653df",
   "metadata": {},
   "source": [
    "Nuevamente, probamos rapidamente para asegurarnos que las capas operan correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a86a7a6e-a676-4ae3-b894-b541e7db51d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 350, 128])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb = TransformerBlock(emb_dim)\n",
    "tb(embedding, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f26309b-50b4-4b07-8a0a-43ff831c1266",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "vocab_size = english_tokenizer.vocab_size\n",
    "\n",
    "token_embeddings = TokenAndPosEmbedding(max_len, emb_dim, vocab_size)\n",
    "transformer = TransformerBlock(emb_dim, num_heads)\n",
    "ff = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(max_len * emb_dim, english_cat_dataset.num_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da72a44-230f-4cea-9f38-5a5c0289261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)\n",
    "batch = next(it)\n",
    "x, mask, y = batch['input_ids'], batch['attention_mask'], batch['y']\n",
    "\n",
    "embeddings = token_embeddings(x)\n",
    "assert embeddings.shape == (train_loader.batch_size, max_len, emb_dim)\n",
    "\n",
    "attention = transformer(embeddings, mask)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ce285-9124-40a8-88ab-7a9921279baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ff(attention)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "id": "39"
   },
   "source": [
    "### 6. Definición del clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "40"
   },
   "source": [
    "Finalmente, definimos el modelo en si. Este modelo constará de 3 capas:\n",
    "\n",
    "- La tokenización, tal como la definimos anteriormente.\n",
    "- El transformer, que acabamos de definir.\n",
    "- Una capa densa adicional que servirá como clasificador de aquello que nos entregue la capa del transformer.\n",
    "\n",
    "Como este es un LightningModule, aquí definiremos el resto de funciones utilitarias para el entrenamiento de la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "id": "41"
   },
   "outputs": [],
   "source": [
    "class EnglishClassifier(LightningModule):\n",
    "\n",
    "    def __init__(self, max_len: int, vocab_size: int, num_classes: int, emb_dim: int, num_heads: int = 8):\n",
    "        super(SpanishNewsClassifier, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.token_embeddings = TokenAndPosEmbedding(max_len, emb_dim, vocab_size)\n",
    "        self.transformer = TransformerBlock(emb_dim, num_heads)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(max_len * emb_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.train_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        embeddings = self.token_embeddings(x)\n",
    "        attention = self.transformer(embeddings, mask)\n",
    "        return self.classifier(attention)\n",
    "    \n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, mask, y = batch['input_ids'], batch['attention_mask'], batch['y']\n",
    "        y_hat = self(x, mask)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.train_acc(y_hat, y)\n",
    "        self.log('train-loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train-acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        x, mask, y = batch['input_ids'], batch['attention_mask'], batch['y']\n",
    "        y_hat = self(x, mask)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.val_acc(y_hat, y)\n",
    "        self.log('val-loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('val-acc', self.val_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        x, mask, y = batch['input_ids'], batch['attention_mask'], batch['y']\n",
    "        y_hat = self(x, mask)\n",
    "        self.test_acc(y_hat, y)\n",
    "        self.log('test-acc', self.test_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        x, mask = batch['input_ids'], batch['attention_mask']\n",
    "        return self(x, mask)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer =  torch.optim.AdamW(self.parameters(), lr=2e-5, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df10db-fd19-4718-b8b0-0021c34c800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = EnglishClassifier(max_len=english_cat_dataset.seq_length, vocab_size=english_tokenizer.vocab_size, num_classes=english_cat_dataset.num_classes, emb_dim=emb_dim)\n",
    "\n",
    "tb_logger = TensorBoardLogger('tb_logs', name='TransformersClassifier')\n",
    "callbacks=[EarlyStopping(monitor='train-loss', patience=3, mode='min')]\n",
    "trainer = Trainer(max_epochs=10, devices=1, logger=tb_logger, callbacks=callbacks, precision=\"16-mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7807b825-a3f7-495e-b179-2e7be68db4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "id": "42"
   },
   "source": [
    "Observemos el proceso de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "id": "43"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "id": "44"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir tb_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "45"
   },
   "source": [
    "### 7. Evaluación de desempeño del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "46"
   },
   "source": [
    "Realizamos la validación contra el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "id": "47"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "48"
   },
   "source": [
    "### 8. Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {
    "id": "49"
   },
   "source": [
    "Finalmente, vamos a hacer uso del modelo y ver que tan bueno es para la clasificación de titulares de noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "id": "50"
   },
   "outputs": [],
   "source": [
    "predictions = trainer.predict(model, test_loader)\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "predictions = torch.argmax(predictions, dim=-1)\n",
    "predictions = [spanish_news_dataset.id_2_class_map[pred] for pred in predictions.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "id": "51"
   },
   "outputs": [],
   "source": [
    "test_indices = test_dataset.indices\n",
    "df = pd.DataFrame(data={\n",
    "    \"texto\": full_dataset[test_indices]['text'],\n",
    "    \"tokens\": [tokenize_text(v) for v in full_dataset[test_indices]['text']],\n",
    "    \"categoría\": full_dataset[test_indices]['label'],\n",
    "    'predicción': predictions\n",
    "}, index=test_indices)\n",
    "\n",
    "id_2_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "df['tokens_string'] = df.tokens.apply(lambda t: ' '.join([id_2_token[i] for i in t]))\n",
    "df = df[[\"texto\", \"tokens\", \"tokens_string\", \"categoría\", \"predicción\"]]\n",
    "df.style.set_table_styles(\n",
    "    [\n",
    "        {'selector': 'td', 'props': [('word-wrap', 'break-word')]}\n",
    "    ]\n",
    ")\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "id": "52"
   },
   "outputs": [],
   "source": [
    "errors = df[df['categoría'] != df['predicción']]\n",
    "errors.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GC6LqPWli-z5",
   "metadata": {
    "id": "GC6LqPWli-z5"
   },
   "source": [
    "Se observa que, cuando los títulos no contienen palabras clave que diferencien claramente la categoría, el modelo tiende a mostrar mayor incertidumbre en su clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "id": "53"
   },
   "outputs": [],
   "source": [
    "df['predicción'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "id": "54"
   },
   "source": [
    "### 9. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "id": "55"
   },
   "source": [
    "#### Eficacia del flujo de análisis\n",
    "\n",
    "- Se implementó un pipeline de clasificación multiclase sobre títulos de noticias en español utilizando un modelo basado en LSTM.\n",
    "- El bloque LSTM actuó como featurizer, extrayendo representaciones de las secuencias de entrada a partir de las cuales se realizaron las predicciones.\n",
    "\n",
    "#### Rendimiento del modelo\n",
    "\n",
    "- El modelo alcanzó un accuracy de 0.9, lo que indica un desempeño bueno para la tarea de clasificación.\n",
    "- Este resultado refleja que, en su configuración actual y sin optimización de hiperparámetros, el modelo aún no logra capturar de manera suficiente las características del texto para diferenciar entre las clases.\n",
    "\n",
    "#### Limitaciones observadas\n",
    "\n",
    "- El tiempo de entrenamiento fue elevado, lo cual es consistente con la naturaleza secuencial de las LSTM, donde en cada paso temporal se deben calcular gradientes.\n",
    "- El modelo se entrenó sin realizar ajustes de hiperparámetros (tasa de aprendizaje, tamaño de batch, número de capas, etc.), lo cual limita su potencial de rendimiento.\n",
    "- La representación basada únicamente en tokens y padding puede ser insuficiente para capturar matices semánticos complejos.\n",
    "\n",
    "#### Áreas de mejora\n",
    "\n",
    "- Explorar técnicas de optimización de hiperparámetros y regularización para mejorar la capacidad predictiva.\n",
    "- Incorporar embeddings preentrenados en español (por ejemplo, FastText o Word2Vec) para enriquecer la representación semántica.\n",
    "- Evaluar arquitecturas más modernas como GRU, Transformers o modelos preentrenados que podrían ofrecer un mejor trade-off entre rendimiento y tiempo de cómputo.\n",
    "\n",
    "#### Valor práctico\n",
    "\n",
    "- Aunque el rendimiento inicial es intermedio, el experimento permite validar el pipeline y sentar bases para futuros ajustes.\n",
    "- Este método ofrece un punto de partida funcional para experimentar con arquitecturas más sofisticadas y con un mejor ajuste de hiperparámetros, lo cual podría elevar significativamente la precisión del clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a28e4-5609-4f01-b6f9-60343d6cb413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "libs = [\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"datasets\",\n",
    "    \"torch\",\n",
    "    \"pytorch-lightning\",\n",
    "    \"torchmetrics\",\n",
    "    \"tqdm\",\n",
    "    \"transformers\",\n",
    "    \"scikit-learn\"\n",
    "]\n",
    "\n",
    "for lib in libs:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(lib).version\n",
    "        print(f\"{lib}=={version}\")\n",
    "    except Exception:\n",
    "        print(f\"{lib}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
