{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jepilogo97/nlp/blob/main/nlp-with-transformers/nlp_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# NLP con Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "##### Jean Pierre Londoño González\n",
        "##### Mini-Proyecto de clasificación de texto con Transformers\n",
        "##### 30AGO2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "source": [
        "En este notebook se implementa un clasificador de conversaciones cotidianas en español utilizando transformers. El dataset empleado corresponde a Everyday Conversations LLaMA 3.1 - 2k, disponible en Hugging Face, el cual contiene diálogos en español sobre diferentes temas. Para la preparación de los datos se realiza la tokenización empleando las utilidades de la librería Hugging Face Transformers.\n",
        "\n",
        "#### Referencias\n",
        "- Dataset: https://huggingface.co/datasets/HuggingFaceTB/everyday-conversations-llama3.1-2k"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "source": [
        "### 1. Importación de librerias y carga de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "source": [
        "Inicio importando las librerías necesarias para el procesamiento de lenguaje natural, la manipulación de datos y la construcción del modelo. Esto incluye NumPy y pandas para el manejo y análisis de datos; Hugging Face Datasets y Transformers para la carga de corpus y la tokenización; y PyTorch junto con PyTorch Lightning para definir, entrenar y evaluar el modelo de manera estructurada.\n",
        "\n",
        "Además, se emplean torchmetrics y scikit-learn para calcular métricas de rendimiento como precisión, matrices de confusión y reportes de clasificación, mientras que Optuna permite la optimización automática de hiperparámetros.\n",
        "\n",
        "Finalmente, tqdm facilita el seguimiento del progreso de los procesos iterativos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "232f9b07-b6e9-4e9b-96fa-697d4f04b78a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "232f9b07-b6e9-4e9b-96fa-697d4f04b78a",
        "outputId": "ff1a6ca2-3dce-443c-8a97-423d176be3fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2396000874.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "import pkg_resources\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "installed_packages = [package.key for package in pkg_resources.working_set]\n",
        "IN_COLAB = 'google-colab' in installed_packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5",
        "outputId": "145708c4-3392-4e0b-bc22-08d71b888434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-30 23:45:57--  https://raw.githubusercontent.com/jepilogo97/nlp/main/nlp-with-transformers/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 181 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "\rrequirements.txt      0%[                    ]       0  --.-KB/s               \rrequirements.txt    100%[===================>]     181  --.-KB/s    in 0s      \n",
            "\n",
            "2025-08-30 23:45:57 (26.2 MB/s) - ‘requirements.txt’ saved [181/181]\n",
            "\n",
            "Requirement already satisfied: numpy==2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: pandas==2.3.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.3.1)\n",
            "Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.0.0)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.2.0)\n",
            "Requirement already satisfied: torchvision==0.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.17.0)\n",
            "Requirement already satisfied: lightning==2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.2.0)\n",
            "Requirement already satisfied: torchmetrics==1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (1.6.1)\n",
            "Requirement already satisfied: optuna==4.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.5.0)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: transformers==4.39.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (4.39.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.3.1->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.3.1->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.3.1->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 3)) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (2.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0->-r requirements.txt (line 5)) (11.3.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from lightning==2.2.0->-r requirements.txt (line 6)) (0.15.2)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.12/dist-packages (from lightning==2.2.0->-r requirements.txt (line 6)) (2.5.4)\n",
            "Requirement already satisfied: pretty-errors==1.2.25 in /usr/local/lib/python3.12/dist-packages (from torchmetrics==1.4.0->-r requirements.txt (line 7)) (1.2.25)\n"
          ]
        }
      ],
      "source": [
        "!wget -O requirements.txt https://raw.githubusercontent.com/jepilogo97/nlp/main/nlp-with-transformers/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "outputs": [],
      "source": [
        "# Procesamiento de lenguaje natural y utilidades\n",
        "import numpy as np  # Cálculo numérico y manejo de arreglos multidimensionales\n",
        "import pandas as pd  # Manipulación y análisis de datos en estructuras tipo DataFrame\n",
        "\n",
        "pd.set_option(\"display.max_rows\", None)     # Todas las filas\n",
        "pd.set_option(\"display.max_columns\", None)  # Todas las columnas\n",
        "pd.set_option(\"display.width\", None)        # No cortar líneas\n",
        "\n",
        "from datasets import load_dataset, concatenate_datasets  # Carga y combinación de datasets de Hugging Face\n",
        "from collections import Counter  # Conteo de frecuencias de elementos (tokens, palabras, etc.)\n",
        "import os  # Manejo de rutas, archivos y operaciones del sistema de archivos\n",
        "import math  # Funciones matemáticas avanzadas (logaritmos, potencias, trigonometría, etc.)\n",
        "\n",
        "# Deep Learning con PyTorch\n",
        "import torch  # Librería principal de tensores y operaciones en GPU/CPU\n",
        "import torch.nn as nn  # Definición de capas y módulos de redes neuronales\n",
        "import torch.nn.functional as F  # Funciones de activación y operaciones matemáticas de redes\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, Subset  # Utilidades para crear y dividir datasets, cargar lotes y trabajar con subconjuntos\n",
        "\n",
        "# Entrenamiento estructurado con PyTorch Lightning\n",
        "from pytorch_lightning import LightningModule, Trainer  # Clase base y manejador de entrenamiento de modelos\n",
        "from pytorch_lightning.loggers import TensorBoardLogger  # Registro de métricas e historial en TensorBoard\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint  # Detener entrenamiento si no mejora la métrica\n",
        "from torchmetrics import Accuracy  # Métrica de precisión para clasificación supervisada\n",
        "\n",
        "# Tipado para mayor legibilidad y validación de funciones\n",
        "from typing import Tuple, Dict, Optional  # Definición de tipos de datos para funciones y estructuras\n",
        "from enum import Enum  # Definición de enumeraciones (conjuntos de valores constantes con nombre)\n",
        "\n",
        "from tqdm.auto import tqdm  # Barra de progreso adaptable para bucles\n",
        "from transformers import AutoTokenizer  # Tokenizador automático de modelos preentrenados de Hugging Face\n",
        "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode  # Conversión de bytes a caracteres Unicode (usado en tokenización tipo GPT-2)\n",
        "\n",
        "import optuna  # Optimización automática de hiperparámetros mediante búsquedas eficientes (Bayesian, TPE, etc.)\n",
        "from optuna.importance import get_param_importances\n",
        "import optuna.visualization as vis\n",
        "\n",
        "# Métricas de evaluación con Scikit-learn\n",
        "from sklearn.model_selection import train_test_split  # División de datos en conjuntos de entrenamiento y prueba\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # Métricas de evaluación de modelos de clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "source": [
        "### 2. Cargue de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "source": [
        "Este dataset contiene 2.2k conversaciones multi-turno generadas por Llama-3.1-70B-Instruct. Se le solicitó al modelo generar una conversación sencilla, con 3 a 4 intercambios breves, entre un Usuario y un Asistente de IA sobre un tema específico.\n",
        "\n",
        "De este dataset se utilizarán específicamente las conversaciones entre el usuario y el asistente de IA, con el fin de asignarlas al tópico correspondiente. Cada ejemplo consiste en un diálogo corto multi-turno, donde el modelo debe identificar el tema principal a partir del intercambio entre ambas partes.\n",
        "\n",
        "Está disponible en el Hugging Face Hub, lo que permite descargarlo fácilmente y utilizarlo directamente en entrenamientos o pruebas de modelos de NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9",
      "metadata": {
        "id": "9"
      },
      "outputs": [],
      "source": [
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "dataset = load_dataset(\"HuggingFaceTB/everyday-conversations-llama3.1-2k\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "source": [
        "El dataset incluye varias columnas pero me centraré en dos de ellas: La conversación entre el user y la AI (completion), la categoría a la que pertenece la conversación (topic). Para entrenar y validar el modelo, se dispone de 2.260 filas en el conjunto de entrenamiento y 119 en el conjunto de prueba."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "source": [
        "Concatenos los dos datasets para tener mayor cantidad de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "outputs": [],
      "source": [
        "tr_dataset = dataset[\"train_sft\"]\n",
        "te_dataset = dataset[\"test_sft\"]\n",
        "full_dataset = concatenate_datasets([tr_dataset, te_dataset])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "source": [
        "Observemos uno de sus registros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "outputs": [],
      "source": [
        "full_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "957d63b6-4c7e-406c-83b6-e874f859198f",
      "metadata": {
        "id": "957d63b6-4c7e-406c-83b6-e874f859198f"
      },
      "source": [
        "Revisamos especificamente las 2 columnas de interes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdb4acdd-ae34-4c59-9ce5-43797cb784f3",
      "metadata": {
        "id": "fdb4acdd-ae34-4c59-9ce5-43797cb784f3"
      },
      "outputs": [],
      "source": [
        "full_dataset = full_dataset.select_columns([\"completion\", \"topic\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22dea978-374a-42cf-b9f8-e9af07ca24d9",
      "metadata": {
        "id": "22dea978-374a-42cf-b9f8-e9af07ca24d9"
      },
      "source": [
        "Ajusto el nombre de las columnas a text y category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b8f4333-3d5b-490d-a927-f08660a48cee",
      "metadata": {
        "id": "0b8f4333-3d5b-490d-a927-f08660a48cee"
      },
      "outputs": [],
      "source": [
        "full_dataset = full_dataset.rename_column(\"completion\", \"text\")\n",
        "full_dataset = full_dataset.rename_column(\"topic\", \"category\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51575f90-963c-4abe-8563-c3eb9dcd7873",
      "metadata": {
        "id": "51575f90-963c-4abe-8563-c3eb9dcd7873"
      },
      "outputs": [],
      "source": [
        "dataset = full_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba926f23-32f2-4cf5-bebe-4dc4293e9f5e",
      "metadata": {
        "id": "ba926f23-32f2-4cf5-bebe-4dc4293e9f5e"
      },
      "source": [
        "Ahora observemos uno de los registros ya ajustado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4339015d-c6f5-429b-8ea2-d75722e702ca",
      "metadata": {
        "id": "4339015d-c6f5-429b-8ea2-d75722e702ca"
      },
      "outputs": [],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebfa4837-af92-498d-a3d7-1ae5ed59371b",
      "metadata": {
        "id": "ebfa4837-af92-498d-a3d7-1ae5ed59371b"
      },
      "outputs": [],
      "source": [
        "pd.Series(dataset['category']).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "source": [
        "Se encuentra que el dataset esta desbalanceado, pues algunas categorias tienes 100 registros pero otros 10."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0b94d5e-7be9-4a71-ab48-f091786050a4",
      "metadata": {
        "id": "d0b94d5e-7be9-4a71-ab48-f091786050a4"
      },
      "source": [
        "Para efectos de la tarea solo utilizaremos las categorias que tienen una muestra suficiente y permite tener el dataset balanceado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9841518e-b54c-41f8-852f-763440e1c13f",
      "metadata": {
        "id": "9841518e-b54c-41f8-852f-763440e1c13f"
      },
      "outputs": [],
      "source": [
        "counts = Counter(dataset['category'])\n",
        "dataset = dataset.filter(lambda example: counts[example['category']] > 98)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c77e89a5-c454-494c-a33e-3cd122f249e9",
      "metadata": {
        "id": "c77e89a5-c454-494c-a33e-3cd122f249e9"
      },
      "outputs": [],
      "source": [
        "pd.Series(dataset['category']).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52652b0d-8446-4e44-a9b8-d21e29f63e49",
      "metadata": {
        "id": "52652b0d-8446-4e44-a9b8-d21e29f63e49"
      },
      "outputs": [],
      "source": [
        "len(set(dataset['category']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8321e1-edc9-4ed4-b1e1-2ccaec9fe0b1",
      "metadata": {
        "id": "ee8321e1-edc9-4ed4-b1e1-2ccaec9fe0b1"
      },
      "outputs": [],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b1c942c-853d-4abd-92cf-7b5fef88cb04",
      "metadata": {
        "id": "2b1c942c-853d-4abd-92cf-7b5fef88cb04"
      },
      "source": [
        "Finalmente quedamos con 19 categorias balanceadas y un total de 1899 registros."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "source": [
        "A manera general, observemos que tan largos o cortos tienden a ser los textos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "outputs": [],
      "source": [
        "text_lengths = [len(row['text']) for row in dataset]\n",
        "print(f\"Texto más corto: {min(text_lengths)}\")\n",
        "print(f\"Texto más largo: {max(text_lengths)}\")\n",
        "print(f\"Longitud promedio: {sum(text_lengths) / len(text_lengths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19",
      "metadata": {
        "id": "19"
      },
      "source": [
        "Estos valores son la cantidad de *caractéres* que tiene las secuencias. Una decisión ingenua pero útil en este momento podría ser ajustar la longitud de las secuencias que vamos a usar para el entrenamiento a unos 350 tokens. Esto podría ser suficiente para capturar una porción significativa de los textos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "source": [
        "### 3. Definición del Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21",
      "metadata": {
        "id": "21"
      },
      "source": [
        "Ahora, vamos a definir el tokenizer para nuestra tarea. Para ahorrarnos tiempo, vamos a entrenar uno basado en gpt2, pero ajustandolo a nuestro dataset. Para ello, debemos seleccionar una muestra representativa de nuestro dataset, como no es muy grande, casi que podemos usarlo todo. Luego, debemos definir el tamaño del vocabulario, es decir, cuantos tokens únicos queremos soportar en nuestro tokenizador. Para que un modelo de lenguaje funcione moderadamente bien para una tarea de clasificación, considerando el tamaño de nuestro corpus, deberíamos definir unos 50 mil tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "outputs": [],
      "source": [
        "length = 350\n",
        "iter_dataset = iter(dataset)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "byte_to_unicode_map = bytes_to_unicode()\n",
        "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
        "base_vocab = list(unicode_to_byte_map.keys())\n",
        "\n",
        "def batch_iterator(batch_size: int = 10):\n",
        "    for _ in tqdm(range(0, length, batch_size)):\n",
        "        yield [next(iter_dataset)['text'] for _ in range(batch_size)]\n",
        "\n",
        "english_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=50000, initial_alphabet=base_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24",
      "metadata": {
        "id": "24"
      },
      "source": [
        "Exploremos ahora el tokenizador obtenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25",
      "metadata": {
        "id": "25"
      },
      "outputs": [],
      "source": [
        "tokens = sorted(english_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
        "print(f\"Vocabulario: {english_tokenizer.vocab_size} tokens\")\n",
        "print(\"Primeros 15 tokens:\")\n",
        "print([f\"{english_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[:15]])\n",
        "print(\"15 tokens de en medio:\")\n",
        "print([f\"{english_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[1000:1015]])\n",
        "print(\"Últimos 15 tokens:\")\n",
        "print([f\"{english_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[-15:]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26",
      "metadata": {
        "id": "26"
      },
      "source": [
        "Vemos que los primeros tokens corresponden a caracteres especiales y puntiación. Luego en el medio tenemos una combinación entre palabras completas y cortadas, el tokenizador se encarga de encontrar las frecuencias más comunes y asi partir las palabras por aquellas partes que tienden a repetirse mas. Esto es muy útil para trabajar con modelos de lenguaje ya que el modelo se vuelve robusto a diferentes ramificaciones de palabras e incluso a errores de tipografía. Finalmente, al final, vemos que tenemos más palabras cortadas y palabras muy especiales."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28",
      "metadata": {
        "id": "28"
      },
      "source": [
        "### 4. Definición del dataset de pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "source": [
        "Ahora podemos proceder a definir el dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30",
      "metadata": {
        "id": "30"
      },
      "outputs": [],
      "source": [
        "class EnglishCatDataset(Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, dataset, seq_length: int = 512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokenizer.pad_token = '[PAD]'\n",
        "        self.dataset = dataset\n",
        "        self.seq_length = seq_length\n",
        "        self.id_2_class_map = dict(enumerate(np.unique(dataset[:]['category'])))\n",
        "        self.class_2_id_map = {v: k for k, v in self.id_2_class_map.items()}\n",
        "        self.num_classes = len(self.id_2_class_map)\n",
        "\n",
        "    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n",
        "        text, y = self.dataset[index]['text'], self.dataset[index]['category']\n",
        "        y = self.class_2_id_map[y]\n",
        "        data = {k: torch.tensor(v) for k, v in self.tokenizer(text, max_length=self.seq_length, truncation=True, padding='max_length').items()}\n",
        "        data['y'] = torch.tensor(y)\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31",
      "metadata": {
        "id": "31"
      },
      "source": [
        "Ahora instanciaremos el dataset entero. Para este experimento, definiremos un tamaño máximo de secuencia de 2048 **tokens**. Que según nuestra intuición arriba, debería ser suficiente para la tarea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32",
      "metadata": {
        "id": "32"
      },
      "outputs": [],
      "source": [
        "max_len = 350\n",
        "english_cat_dataset = EnglishCatDataset(english_tokenizer, dataset, seq_length=max_len)\n",
        "assert len(english_cat_dataset) == len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33",
      "metadata": {
        "id": "33"
      },
      "source": [
        "Y luego, procedemos a hacer el train-val-test split y crear los dataloaders. Se mantienen las proporciones de cada categoria en cada dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35",
      "metadata": {
        "id": "35"
      },
      "outputs": [],
      "source": [
        "batch_size = 4 if not IN_COLAB else 12\n",
        "train_dataset, val_dataset, test_dataset = random_split(english_cat_dataset, lengths=[0.8, 0.1, 0.1])\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e0bf6ea-a2bb-4276-b355-9e2083eda65f",
      "metadata": {
        "id": "6e0bf6ea-a2bb-4276-b355-9e2083eda65f"
      },
      "source": [
        "### 5. Definición de los Positional Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56251dff-d7d1-42dc-82d8-4cafa039d2dd",
      "metadata": {
        "id": "56251dff-d7d1-42dc-82d8-4cafa039d2dd"
      },
      "source": [
        "Según el paper, los autores agregan una secuencia sinusoidal a los embeddings de los tokens con el fin de inyectar información referente a la posición de cada token en las frases. Esto obedece a la definición:\n",
        "\n",
        "$$\n",
        "PE(pos, 2i) = \\sin(pos/10000^{2i/d_{model}}) \\\\\n",
        "PE(pos, 2i + 1) = \\cos(pos/10000^{2i/d_{model}})\n",
        "$$\n",
        "\n",
        "Donde:\n",
        "- $pos$ es la posición del *token* en la secuencia.\n",
        "- $i$ es la dimensión $i$ en el embedding $d$.\n",
        "- $d_model$ es la dimensionalidad total del embedding.\n",
        "\n",
        "Lo que los autores propusieron fue que para las posiciones pares, se calculara el seno de la posición, relativa a la dimensionalidad del embedding y para las posiciones impares, se calculara el coseno. Según los autores, estos tenían la hipótesis de que estas funciones inyectarían la información posicional relativa de forma eficiente, en parte porque se pueden pre-calcular e inyectar directamente durante el entrenamiento, evitando asi emplear recursos en entrenar estructuras para aprenderlos.\n",
        "\n",
        "Esto último es particularmente importante ya que se evita tanto hacer uso de recursos innecesarios como acelerar el proceso de entrenamiento al no tener que computar gradientes para esta parte. Sin embargo, los autores también mencionaron que es ciertamente posible aprender estos positional embeddings como parte del entrenamiento y que según sus resultados, no había mucha diferencia entre ambos enfoques, razón por la cual, se prefiere el positional encoding sinusoidal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b53a1b4-7491-46e9-b113-004a5b11d707",
      "metadata": {
        "id": "8b53a1b4-7491-46e9-b113-004a5b11d707"
      },
      "outputs": [],
      "source": [
        "class PosEncodingType(Enum):\n",
        "    SINUSOID = 1\n",
        "    LEARNABLE = 2\n",
        "\n",
        "\n",
        "class SinusoidPE(nn.Module):\n",
        "\n",
        "    def __init__(self, max_len: int, d_model: int):\n",
        "        super(SinusoidPE, self).__init__()\n",
        "\n",
        "        # Definimos un vector columna con las posiciones de la secuencia de entrada (pos)\n",
        "        pos = torch.arange(max_len).unsqueeze(1)\n",
        "        # Definimos un vector de fila con las dimensiones del embedding (i)\n",
        "        i = torch.arange(d_model).unsqueeze(0)\n",
        "\n",
        "        # Calculamos el denominador segun la formula\n",
        "        div_term = 1 / torch.pow(10000, (2 * (i // 2)) / torch.tensor(d_model, dtype=torch.float32))\n",
        "        # Aplicamos el denominador a las posiciones\n",
        "        angle_rads = pos * div_term\n",
        "\n",
        "        # Inicializamos la matriz de positional encodings\n",
        "        pos_encoding = torch.zeros(max_len, d_model)\n",
        "        # Calculamos los embeddings para los numeros pares con seno: PE(pos, 2i)\n",
        "        pos_encoding[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
        "        # Calculamos los embdeddings para los numeros inpares con coseno: PE(pos, 2i+1)\n",
        "        pos_encoding[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        # Registramos la variable como atributo de clase\n",
        "        self.register_buffer(\"pos_encoding\", pos_encoding.unsqueeze(0), persistent=False)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x + self.pos_encoding[:, :x.size(1), :]\n",
        "\n",
        "\n",
        "class LearnablePE(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, d_model: int, max_len: int = float('-inf')):\n",
        "        super(LearnablePE, self).__init__()\n",
        "        self.max_len = max_len\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        positions = torch.arange(0, max(x.size(-1), self.max_len))\n",
        "        pos_emb = self.embedding(positions)\n",
        "        return x + pos_emb\n",
        "\n",
        "\n",
        "\n",
        "class TokenAndPosEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, max_len: int, embed_dim: int, vocab_size: int, pos_encoding_type: PosEncodingType = PosEncodingType.SINUSOID):\n",
        "        super(TokenAndPosEmbedding, self).__init__()\n",
        "        self.token_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
        "        if pos_encoding_type == PosEncodingType.SINUSOID:\n",
        "            self.pos_emb = SinusoidPE(max_len, embed_dim)\n",
        "        else:\n",
        "            self.pos_emb = LearnablePE(vocab_size, embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        token_emb = self.token_emb(x)\n",
        "        return self.pos_emb(token_emb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8749e3e8-a2a6-4ea4-b622-74f08fcf5921",
      "metadata": {
        "id": "8749e3e8-a2a6-4ea4-b622-74f08fcf5921"
      },
      "source": [
        "Ahora procedemos a instanciar el modulo que va a convertir los tokens en embeddings con positional embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25fc3ec3-7cd7-4bea-ab7b-506cb4a0bf3f",
      "metadata": {
        "id": "25fc3ec3-7cd7-4bea-ab7b-506cb4a0bf3f"
      },
      "outputs": [],
      "source": [
        "emb_dim = 128 if not IN_COLAB else 256\n",
        "tpe = TokenAndPosEmbedding(max_len, emb_dim, english_tokenizer.vocab_size)\n",
        "pos_encoding = tpe.pos_emb.pos_encoding.squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db91b4b7-bff5-467d-9b72-d608ba85e2fa",
      "metadata": {
        "id": "db91b4b7-bff5-467d-9b72-d608ba85e2fa"
      },
      "outputs": [],
      "source": [
        "text = \"hola mundo!\"\n",
        "tokens = english_tokenizer(text, max_length=max_len, truncation=True, padding='max_length')\n",
        "x = torch.tensor(tokens['input_ids']).unsqueeze(0)\n",
        "mask = torch.tensor(tokens['input_ids']).unsqueeze(0)\n",
        "embedding = tpe(x)\n",
        "embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7581aa6-0415-48f7-98d2-f043aa68ab6c",
      "metadata": {
        "id": "e7581aa6-0415-48f7-98d2-f043aa68ab6c"
      },
      "source": [
        "### 6. Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9866991e-cf92-4d6c-afd4-9fd1b4385bfb",
      "metadata": {
        "id": "9866991e-cf92-4d6c-afd4-9fd1b4385bfb"
      },
      "source": [
        "Ahora procedemos a definir al núcleo del modelo. Recodemos que la atención se define por:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_K}})V\n",
        "$$\n",
        "\n",
        "Que es la definición de \"Scaled Dot-Product Attention\". Y Multi-Head Attention es la concatenación de varias cabezas ejecutando el mismo scaled dot-product sobre partes del input. Entonces tenemos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b008da5-4797-43a3-8704-488aba982257",
      "metadata": {
        "id": "8b008da5-4797-43a3-8704-488aba982257"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size: int, num_heads: int = 8):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        assert embed_size & num_heads == 0, 'El tamaño del embedding debería ser divisible por el numero de cabezas'\n",
        "        self.projection_dim = embed_size // num_heads\n",
        "        self.query = nn.Linear(embed_size, embed_size)\n",
        "        self.key = nn.Linear(embed_size, embed_size)\n",
        "        self.value = nn.Linear(embed_size, embed_size)\n",
        "        self.comibe_heads = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _scaled_dot_product(q, k, v, mask=None):\n",
        "        # d_k para el escalamiento\n",
        "        d_k = q.size()[-1]\n",
        "\n",
        "        # multiplicacion Q \\cdot K^T\n",
        "        attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "        # escalamiento\n",
        "        attn_logits = attn_logits / math.sqrt(d_k)\n",
        "\n",
        "        # Se aplica la máscara\n",
        "        if mask is not None:\n",
        "            attn_logits = attn_logits.masked_fill(mask.reshape(mask.shape[0], 1, 1, -1) == 0, -9e-15)\n",
        "\n",
        "        # Se calcula el score de atención.\n",
        "        attention = torch.softmax(attn_logits, dim=-1)\n",
        "        # Se obtienen los valores tras el score de atención.\n",
        "        values = torch.matmul(attention, v)\n",
        "        return values, attention\n",
        "\n",
        "\n",
        "    def _separate_heads(self, x, batch_size):\n",
        "        # Llega: (batch, seq_len, emb_dim)\n",
        "        x =  x.reshape(batch_size, -1, self.num_heads, self.projection_dim)  # (batch, seq_len, num_heads, emb_dim / num_heads)\n",
        "        return x.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, emb_dim / num_heads)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        # x: (batch, seq_len, emb_dim)\n",
        "        batch_size, seq_len, embed_size = x.size()\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        q = self._separate_heads(q, batch_size)\n",
        "        k = self._separate_heads(k, batch_size)\n",
        "        v = self._separate_heads(v, batch_size)\n",
        "\n",
        "        weights, attention = self._scaled_dot_product(q, k, v, mask)\n",
        "        weights = weights.permute(0, 2, 1, 3) # (batch, seq_len, num_heads, emb_dim / num_heads)\n",
        "        weights = weights.reshape(batch_size, seq_len, embed_size)\n",
        "        output = self.comibe_heads(weights)\n",
        "\n",
        "        if return_attention:\n",
        "            return output, attention\n",
        "        else:\n",
        "            return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c5aee7-ea38-4419-9db4-a7ac41e03feb",
      "metadata": {
        "id": "51c5aee7-ea38-4419-9db4-a7ac41e03feb"
      },
      "source": [
        "Podemos hacer una prueba rápida de que las operaciones funcionan a nivel de matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d55402-b148-43d8-8021-7c3edacdd35c",
      "metadata": {
        "id": "75d55402-b148-43d8-8021-7c3edacdd35c"
      },
      "outputs": [],
      "source": [
        "mha = MultiHeadAttention(emb_dim)\n",
        "mha(embedding, mask).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e317d971-a169-4f89-b093-9db4853aceca",
      "metadata": {
        "id": "e317d971-a169-4f89-b093-9db4853aceca"
      },
      "source": [
        "### 7. Definición del bloque transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ddfb7e-647c-4554-93fb-6861975a42a1",
      "metadata": {
        "id": "06ddfb7e-647c-4554-93fb-6861975a42a1"
      },
      "source": [
        "Finalmente, definimos el bloque de transformers. Recordemos que como esta es una tarea de clasificación, solamente necesitamos el encoder, por lo que esto es silamente la primera parte del diseño de arquitecura de red.\n",
        "\n",
        "En esta capa, simplemente ponemos una capa densa adicional junto con las normalizaciones a nivel de capa."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, emb_dim: int, num_heads: int = 8, ffn_hidden_dim: int = 512, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.mhatt = MultiHeadAttention(emb_dim, num_heads)\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        ffn_hidden_dim = ffn_hidden_dim or 4 * emb_dim\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(emb_dim, ffn_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ffn_hidden_dim, emb_dim)\n",
        "        )\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.mhatt(x, mask)\n",
        "        attn_output = self.attn_dropout(attn_output)\n",
        "        attn_output = self.layer_norm1(attn_output)\n",
        "        ffn_out = self.ffn(attn_output)\n",
        "        return self.layer_norm2(ffn_out)"
      ],
      "metadata": {
        "id": "mPW7tpI1HP37"
      },
      "id": "mPW7tpI1HP37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c9737f17-95b2-44fd-8ad7-8c67844653df",
      "metadata": {
        "id": "c9737f17-95b2-44fd-8ad7-8c67844653df"
      },
      "source": [
        "Nuevamente, probamos rapidamente para asegurarnos que las capas operan correctamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a86a7a6e-a676-4ae3-b894-b541e7db51d5",
      "metadata": {
        "id": "a86a7a6e-a676-4ae3-b894-b541e7db51d5"
      },
      "outputs": [],
      "source": [
        "tb = TransformerBlock(emb_dim)\n",
        "tb(embedding, mask).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f26309b-50b4-4b07-8a0a-43ff831c1266",
      "metadata": {
        "id": "4f26309b-50b4-4b07-8a0a-43ff831c1266"
      },
      "outputs": [],
      "source": [
        "num_heads = 4\n",
        "vocab_size = english_tokenizer.vocab_size\n",
        "\n",
        "token_embeddings = TokenAndPosEmbedding(max_len, emb_dim, vocab_size)\n",
        "transformer = TransformerBlock(emb_dim, num_heads)\n",
        "ff = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(max_len * emb_dim, english_cat_dataset.num_classes)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da72a44-230f-4cea-9f38-5a5c0289261c",
      "metadata": {
        "id": "3da72a44-230f-4cea-9f38-5a5c0289261c"
      },
      "outputs": [],
      "source": [
        "it = iter(train_loader)\n",
        "batch = next(it)\n",
        "x, mask, y = batch['input_ids'], batch['attention_mask'], batch['y']\n",
        "\n",
        "embeddings = token_embeddings(x)\n",
        "assert embeddings.shape == (train_loader.batch_size, max_len, emb_dim)\n",
        "\n",
        "attention = transformer(embeddings, mask)\n",
        "attention.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "105ce285-9124-40a8-88ab-7a9921279baa",
      "metadata": {
        "id": "105ce285-9124-40a8-88ab-7a9921279baa"
      },
      "outputs": [],
      "source": [
        "pred = ff(attention)\n",
        "pred.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39",
      "metadata": {
        "id": "39"
      },
      "source": [
        "### 8. Definición del clasificador"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40",
      "metadata": {
        "id": "40"
      },
      "source": [
        "Finalmente, definimos el modelo en si. Este modelo constará de 3 capas:\n",
        "\n",
        "- La tokenización, tal como la definimos anteriormente.\n",
        "- El transformer, que acabamos de definir.\n",
        "- Una capa densa adicional que servirá como clasificador de aquello que nos entregue la capa del transformer.\n",
        "\n",
        "Como este es un LightningModule, aquí definiremos el resto de funciones utilitarias para el entrenamiento de la tarea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41",
      "metadata": {
        "id": "41"
      },
      "outputs": [],
      "source": [
        "class EnglishClassifier(LightningModule):\n",
        "\n",
        "    def __init__(self, max_len: int, vocab_size: int, num_classes: int, emb_dim: int, num_heads: int = 8):\n",
        "        super(EnglishClassifier, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.token_embeddings = TokenAndPosEmbedding(max_len, emb_dim, vocab_size)\n",
        "        self.transformer = TransformerBlock(emb_dim, num_heads)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(max_len * emb_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "        self.train_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "        self.val_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "        self.test_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        embeddings = self.token_embeddings(x)\n",
        "        attention = self.transformer(embeddings, mask)\n",
        "        return self.classifier(attention)\n",
        "\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        x, mask, y = batch['input_ids'], batch['attention_mask'], batch['y']\n",
        "        y_hat = self(x, mask)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.train_acc(y_hat, y)\n",
        "        self.log('train-loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log('train-acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        x, mask, y = batch['input_ids'], batch['attention_mask'], batch['y']\n",
        "        y_hat = self(x, mask)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.val_acc(y_hat, y)\n",
        "        self.log('val-loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log('val-acc', self.val_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        x, mask, y = batch['input_ids'], batch['attention_mask'], batch['y']\n",
        "        y_hat = self(x, mask)\n",
        "        self.test_acc(y_hat, y)\n",
        "        self.log('test-acc', self.test_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
        "\n",
        "\n",
        "    def predict_step(self, batch):\n",
        "        x, mask = batch['input_ids'], batch['attention_mask']\n",
        "        return self(x, mask)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer =  torch.optim.AdamW(self.parameters(), lr=2e-5, weight_decay=1e-5)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70df10db-fd19-4718-b8b0-0021c34c800d",
      "metadata": {
        "id": "70df10db-fd19-4718-b8b0-0021c34c800d"
      },
      "outputs": [],
      "source": [
        "model = EnglishClassifier(max_len=english_cat_dataset.seq_length, vocab_size=english_tokenizer.vocab_size, num_classes=english_cat_dataset.num_classes, emb_dim=emb_dim)\n",
        "\n",
        "tb_logger = TensorBoardLogger('tb_logs', name='TransformersClassifier')\n",
        "callbacks=[EarlyStopping(monitor='train-loss', patience=3, mode='min')]\n",
        "trainer = Trainer(max_epochs=10, devices=1, logger=tb_logger, callbacks=callbacks, precision=\"16-mixed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7807b825-a3f7-495e-b179-2e7be68db4ac",
      "metadata": {
        "id": "7807b825-a3f7-495e-b179-2e7be68db4ac"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42",
      "metadata": {
        "id": "42"
      },
      "source": [
        "Observemos el proceso de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43",
      "metadata": {
        "id": "43"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44",
      "metadata": {
        "id": "44"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir tb_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pIK7TEh8Dyz9",
      "metadata": {
        "id": "pIK7TEh8Dyz9"
      },
      "source": [
        "Inicialmente alcanzamos una exactitud del 0.62 en el conjunto de validación utilizando hiperparámetros fijos. A continuación, buscaremos mejorar este resultado empleando el optimizador Optuna."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m5aj9IDQEuR0",
      "metadata": {
        "id": "m5aj9IDQEuR0"
      },
      "source": [
        "### 9. Optimización del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear una clase para poder tener los hiperparametros más importantes configurables."
      ],
      "metadata": {
        "id": "hJxXAmlLMi5p"
      },
      "id": "hJxXAmlLMi5p"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tokenizer(trial, dataset):\n",
        "    # Hiperparámetro que controla cuántos textos usamos para entrenar el tokenizer\n",
        "    length = trial.suggest_int(\"tokenizer_length\", 50, 500, step=50)\n",
        "\n",
        "    iter_dataset = iter(dataset)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "    byte_to_unicode_map = bytes_to_unicode()\n",
        "    unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
        "    base_vocab = list(unicode_to_byte_map.keys())\n",
        "\n",
        "    def batch_iterator(batch_size: int = 10):\n",
        "        for _ in range(0, length, batch_size):\n",
        "            yield [next(iter_dataset)[\"text\"] for _ in range(batch_size)]\n",
        "\n",
        "    english_tokenizer = tokenizer.train_new_from_iterator(\n",
        "        batch_iterator(),\n",
        "        vocab_size=50000,\n",
        "        initial_alphabet=base_vocab\n",
        "    )\n",
        "    return english_tokenizer"
      ],
      "metadata": {
        "id": "PZXy0ngRRCmC"
      },
      "id": "PZXy0ngRRCmC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnglishOptunaObjective:\n",
        "    def __init__(self, dataset, train_loader, val_loader, num_classes, max_len, device=\"gpu\"):\n",
        "        self.dataset = dataset\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.num_classes = num_classes\n",
        "        self.max_len = max_len\n",
        "        self.device = device\n",
        "\n",
        "    def __call__(self, trial):\n",
        "\n",
        "        try:\n",
        "            # Hiperparámetro: longitud usada en el tokenizer\n",
        "            length = trial.suggest_int(\"tokenizer_length\", 50, 500, step=50)\n",
        "\n",
        "            # Crear tokenizer dinámico\n",
        "            english_tokenizer = make_tokenizer(trial, self.dataset)\n",
        "            vocab_size = english_tokenizer.vocab_size\n",
        "\n",
        "            # Otros hiperparámetros\n",
        "            emb_dim = trial.suggest_categorical(\"emb_dim\", [64, 128, 256])\n",
        "            num_heads = trial.suggest_categorical(\"num_heads\", [2, 4, 8])\n",
        "            lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
        "            weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
        "\n",
        "            # Definir modelo\n",
        "            model = EnglishClassifier(\n",
        "                max_len=self.max_len,\n",
        "                vocab_size=vocab_size,\n",
        "                num_classes=self.num_classes,\n",
        "                emb_dim=emb_dim,\n",
        "                num_heads=num_heads,\n",
        "            )\n",
        "\n",
        "            # Sobrescribir optimizador con los hiperparámetros sugeridos\n",
        "            def configure_optimizers_override():\n",
        "                return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "            model.configure_optimizers = configure_optimizers_override\n",
        "\n",
        "            # Callbacks para early stopping y checkpointing\n",
        "            checkpoint_callback = ModelCheckpoint(\n",
        "                monitor=\"val-acc\",\n",
        "                mode=\"max\",\n",
        "                save_top_k=1,\n",
        "                dirpath=f\"optuna_checkpoints/trial_{trial.number}\",\n",
        "                filename=\"best_model\"\n",
        "            )\n",
        "\n",
        "            trainer = Trainer(\n",
        "                max_epochs=5,  # menos para explorar rápido\n",
        "                devices=1,\n",
        "                accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                logger=TensorBoardLogger(\"tb_logs\", name=f\"optuna_trial_{trial.number}\"),\n",
        "                callbacks=[EarlyStopping(monitor=\"val-loss\", patience=2, mode=\"min\"),\n",
        "                          checkpoint_callback],\n",
        "                enable_checkpointing=True,\n",
        "                precision=\"16-mixed\"\n",
        "            )\n",
        "\n",
        "            trainer.fit(model, train_dataloaders=self.train_loader, val_dataloaders=self.val_loader)\n",
        "\n",
        "            # Recuperamos mejor checkpoint\n",
        "            best_ckpt_path = checkpoint_callback.best_model_path\n",
        "            trial.set_user_attr(\"best_ckpt\", best_ckpt_path)\n",
        "\n",
        "            # Métrica a optimizar\n",
        "            val_acc = trainer.callback_metrics[\"val-acc\"].item()\n",
        "            return val_acc\n",
        "        except RuntimeError as e:\n",
        "            if \"CUDA error\" in str(e):\n",
        "                print(f\"Trial {trial.number} falló por error CUDA, devolviendo 0.\")\n",
        "                return 0.0  # penalizamos este trial\n",
        "            else:\n",
        "                raise  # otros errores sí los lanzamos"
      ],
      "metadata": {
        "id": "-4v_TZSeGfrC"
      },
      "id": "-4v_TZSeGfrC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora corremos el estudio para optimizar el modelo."
      ],
      "metadata": {
        "id": "MUzR9-AWNM05"
      },
      "id": "MUzR9-AWNM05"
    },
    {
      "cell_type": "code",
      "source": [
        "objective = EnglishOptunaObjective(\n",
        "    dataset=dataset,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    num_classes=english_cat_dataset.num_classes,\n",
        "    max_len=english_cat_dataset.seq_length,\n",
        ")\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20)"
      ],
      "metadata": {
        "id": "vh6ksN3PNKRs"
      },
      "id": "vh6ksN3PNKRs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mejores hiperparámetros:\", study.best_trial.params)\n",
        "print(\"Checkpoint del mejor modelo:\", study.best_trial.user_attrs[\"best_ckpt\"])"
      ],
      "metadata": {
        "id": "8JSyXngFR11n"
      },
      "id": "8JSyXngFR11n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = EnglishClassifier.load_from_checkpoint(study.best_trial.user_attrs[\"best_ckpt\"])"
      ],
      "metadata": {
        "id": "xfncef0rR5rE"
      },
      "id": "xfncef0rR5rE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = get_param_importances(study)\n",
        "print(\"Importancia de hiperparámetros:\")\n",
        "for param, score in importances.items():\n",
        "    print(f\"{param}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "z27pHp8VTmg3"
      },
      "id": "z27pHp8VTmg3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importancia en gráfico de barras\n",
        "fig = vis.plot_param_importances(study)\n",
        "fig.show()\n",
        "\n",
        "# Evolución del valor objetivo a lo largo de los trials\n",
        "fig = vis.plot_optimization_history(study)\n",
        "fig.show()\n",
        "\n",
        "# Relación entre parámetros y objetivo\n",
        "fig = vis.plot_parallel_coordinate(study)\n",
        "fig.show()\n",
        "\n",
        "# Valores probados vs métrica\n",
        "fig = vis.plot_slice(study)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "dTA_gDU-T1Jf"
      },
      "id": "dTA_gDU-T1Jf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "45",
      "metadata": {
        "id": "45"
      },
      "source": [
        "### 10. Evaluación de desempeño del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46",
      "metadata": {
        "id": "46"
      },
      "source": [
        "Realizamos la validación contra el conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47",
      "metadata": {
        "id": "47"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "trainer.test(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48",
      "metadata": {
        "id": "48"
      },
      "source": [
        "### 11. Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49",
      "metadata": {
        "id": "49"
      },
      "source": [
        "Finalmente, vamos a hacer uso del modelo y ver que tan bueno es para la clasificación de titulares de noticias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50",
      "metadata": {
        "id": "50"
      },
      "outputs": [],
      "source": [
        "predictions = trainer.predict(model, test_loader)\n",
        "predictions = torch.cat(predictions, dim=0)\n",
        "predictions = torch.argmax(predictions, dim=-1)\n",
        "predictions = [english_cat_dataset.id_2_class_map[pred] for pred in predictions.tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_indices = test_dataset.indices\n",
        "df = pd.DataFrame(data={\n",
        "    \"texto\": dataset[test_indices]['text'],\n",
        "    \"tokens\": [english_tokenizer(v)['input_ids'] for v in dataset[test_indices]['text']],\n",
        "    \"categoría\": dataset[test_indices]['category'],\n",
        "    'predicción': predictions\n",
        "}, index=test_indices)\n",
        "\n",
        "df['tokens_string'] = df.tokens.apply(lambda t: english_tokenizer.convert_ids_to_tokens(t))\n",
        "df = df[[\"texto\", \"tokens\", \"tokens_string\", \"categoría\", \"predicción\"]]\n",
        "df.style.set_table_styles(\n",
        "    [\n",
        "        {'selector': 'td', 'props': [('word-wrap', 'break-word')]}\n",
        "    ]\n",
        ")\n",
        "df.head(15)"
      ],
      "metadata": {
        "id": "NJ6ZogCKIZwD"
      },
      "id": "NJ6ZogCKIZwD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52",
      "metadata": {
        "id": "52"
      },
      "outputs": [],
      "source": [
        "errors = df[df['categoría'] != df['predicción']]\n",
        "errors.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GC6LqPWli-z5",
      "metadata": {
        "id": "GC6LqPWli-z5"
      },
      "source": [
        "Se observa que, cuando los títulos no contienen palabras clave que diferencien claramente la categoría, el modelo tiende a mostrar mayor incertidumbre en su clasificación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53",
      "metadata": {
        "id": "53"
      },
      "outputs": [],
      "source": [
        "df['predicción'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54",
      "metadata": {
        "id": "54"
      },
      "source": [
        "### 12. Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55",
      "metadata": {
        "id": "55"
      },
      "source": [
        "#### Eficacia del flujo de análisis\n",
        "\n",
        "- Se implementó un pipeline de clasificación multiclase sobre títulos de noticias en español utilizando un modelo basado en LSTM.\n",
        "- El bloque LSTM actuó como featurizer, extrayendo representaciones de las secuencias de entrada a partir de las cuales se realizaron las predicciones.\n",
        "\n",
        "#### Rendimiento del modelo\n",
        "\n",
        "- El modelo alcanzó un accuracy de 0.9, lo que indica un desempeño bueno para la tarea de clasificación.\n",
        "- Este resultado refleja que, en su configuración actual y sin optimización de hiperparámetros, el modelo aún no logra capturar de manera suficiente las características del texto para diferenciar entre las clases.\n",
        "\n",
        "#### Limitaciones observadas\n",
        "\n",
        "- El tiempo de entrenamiento fue elevado, lo cual es consistente con la naturaleza secuencial de las LSTM, donde en cada paso temporal se deben calcular gradientes.\n",
        "- El modelo se entrenó sin realizar ajustes de hiperparámetros (tasa de aprendizaje, tamaño de batch, número de capas, etc.), lo cual limita su potencial de rendimiento.\n",
        "- La representación basada únicamente en tokens y padding puede ser insuficiente para capturar matices semánticos complejos.\n",
        "\n",
        "#### Áreas de mejora\n",
        "\n",
        "- Explorar técnicas de optimización de hiperparámetros y regularización para mejorar la capacidad predictiva.\n",
        "- Incorporar embeddings preentrenados en español (por ejemplo, FastText o Word2Vec) para enriquecer la representación semántica.\n",
        "- Evaluar arquitecturas más modernas como GRU, Transformers o modelos preentrenados que podrían ofrecer un mejor trade-off entre rendimiento y tiempo de cómputo.\n",
        "\n",
        "#### Valor práctico\n",
        "\n",
        "- Aunque el rendimiento inicial es intermedio, el experimento permite validar el pipeline y sentar bases para futuros ajustes.\n",
        "- Este método ofrece un punto de partida funcional para experimentar con arquitecturas más sofisticadas y con un mejor ajuste de hiperparámetros, lo cual podría elevar significativamente la precisión del clasificador."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1089b9-ae81-4ae0-91ec-d323c7c84352",
      "metadata": {
        "id": "4b1089b9-ae81-4ae0-91ec-d323c7c84352"
      },
      "source": [
        "### 13. Apendice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636a28e4-5609-4f01-b6f9-60343d6cb413",
      "metadata": {
        "id": "636a28e4-5609-4f01-b6f9-60343d6cb413"
      },
      "outputs": [],
      "source": [
        "import pkg_resources\n",
        "\n",
        "libs = [\n",
        "    \"numpy\",\n",
        "    \"pandas\",\n",
        "    \"datasets\",\n",
        "    \"torch\",\n",
        "    \"pytorch-lightning\",\n",
        "    \"torchmetrics\",\n",
        "    \"tqdm\",\n",
        "    \"transformers\",\n",
        "    \"scikit-learn\"\n",
        "]\n",
        "\n",
        "for lib in libs:\n",
        "    try:\n",
        "        version = pkg_resources.get_distribution(lib).version\n",
        "        print(f\"{lib}=={version}\")\n",
        "    except Exception:\n",
        "        print(f\"{lib}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646eaeca-8b47-46c4-9995-16c1f2ceb0e0",
      "metadata": {
        "id": "646eaeca-8b47-46c4-9995-16c1f2ceb0e0"
      },
      "outputs": [],
      "source": [
        "import nbformat\n",
        "\n",
        "# Cargar notebook\n",
        "with open(\"nlp_with_transformers.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "# Eliminar widgets corruptos si existen\n",
        "if \"widgets\" in nb[\"metadata\"]:\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "\n",
        "# Guardar reparado\n",
        "with open(\"nlp_with_transformers.ipynb\", \"w\", encoding=\"utf-8\") as f:\n",
        "    nbformat.write(nb, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}