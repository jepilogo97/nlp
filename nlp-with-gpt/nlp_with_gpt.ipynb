{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jepilogo97/nlp/blob/main/nlp-with-gpt/nlp_with_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# NLP con GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "##### Jean Pierre Londoño González\n",
        "##### Mini-Proyecto de clasificación de texto usando GPT\n",
        "##### 21SEP2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "source": [
        "En este notebook harémos uso de un modelo GPT neo de 125 M que utilizaremos para generar texto a partir de un contexto inicial que proveerémos. Luego, harémos fine tuning a este modelo con un dataset de podcast en inglés del investigador de IA Lex Fridman y observar como cambia la generación de texto en función del dataset que utilicemos.\n",
        "\n",
        "#### Referencias\n",
        "- Dataset: https://huggingface.co/datasets/RamAnanth1/lex-fridman-podcasts\n",
        "- https://huggingface.co/EleutherAI/gpt-neo-125m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "source": [
        "### 1. Importación de librerias y carga de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "source": [
        "Inicio importando las librerías necesarias para el procesamiento de lenguaje natural, la manipulación de datos y la construcción del modelo. Esto incluye NumPy y pandas para el manejo y análisis de datos; Hugging Face Datasets y Transformers para la carga de corpus y la tokenización; y PyTorch junto con PyTorch Lightning para definir, entrenar y evaluar el modelo de manera estructurada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "232f9b07-b6e9-4e9b-96fa-697d4f04b78a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "232f9b07-b6e9-4e9b-96fa-697d4f04b78a",
        "outputId": "e5dea922-a470-44f7-da49-d21f7739a7bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2396000874.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "import pkg_resources\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "installed_packages = [package.key for package in pkg_resources.working_set]\n",
        "IN_COLAB = 'google-colab' in installed_packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5",
        "outputId": "bf1bc792-e5f5-420c-f40a-136e125bb44d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-22 01:07:22--  https://raw.githubusercontent.com/jepilogo97/nlp/main/nlp-with-gpt/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "\rrequirements.txt      0%[                    ]       0  --.-KB/s               \rrequirements.txt    100%[===================>]     228  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-22 01:07:22 (6.72 MB/s) - ‘requirements.txt’ saved [228/228]\n",
            "\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pandas==2.3.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.3.1)\n",
            "Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.0.0)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.2.0)\n",
            "Requirement already satisfied: torchvision==0.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.17.0)\n",
            "Requirement already satisfied: lightning==2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.2.0)\n",
            "Requirement already satisfied: torchmetrics==1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: torchinfo==1.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (1.8.0)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (1.6.1)\n",
            "Requirement already satisfied: optuna==4.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (4.5.0)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (4.67.1)\n",
            "Requirement already satisfied: transformers==4.41.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (4.41.2)\n",
            "Requirement already satisfied: evaluate==0.4.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (0.4.3)\n",
            "Requirement already satisfied: peft==0.11.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (0.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.3.1->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.3.1->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.3.1->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 3)) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0->-r requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 4)) (2.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0->-r requirements.txt (line 5)) (11.3.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from lightning==2.2.0->-r requirements.txt (line 6)) (0.15.2)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.12/dist-packages (from lightning==2.2.0->-r requirements.txt (line 6)) (2.5.5)\n",
            "Requirement already satisfied: pretty-errors==1.2.25 in /usr/local/lib/python3.12/dist-packages (from torchmetrics==1.4.0->-r requirements.txt (line 7)) (1.2.25)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1->-r requirements.txt (line 9)) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1->-r requirements.txt (line 9)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1->-r requirements.txt (line 9)) (3.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna==4.5.0->-r requirements.txt (line 10)) (1.16.5)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna==4.5.0->-r requirements.txt (line 10)) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna==4.5.0->-r requirements.txt (line 10)) (2.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->-r requirements.txt (line 12)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->-r requirements.txt (line 12)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->-r requirements.txt (line 12)) (0.6.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1->-r requirements.txt (line 14)) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1->-r requirements.txt (line 14)) (1.10.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->-r requirements.txt (line 4)) (12.6.85)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from pretty-errors==1.2.25->torchmetrics==1.4.0->-r requirements.txt (line 7)) (0.4.6)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna==4.5.0->-r requirements.txt (line 10)) (1.3.10)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 3)) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==4.0.0->-r requirements.txt (line 3)) (1.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning==2.2.0->-r requirements.txt (line 6)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.3.1->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0->-r requirements.txt (line 3)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0->-r requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0->-r requirements.txt (line 3)) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna==4.5.0->-r requirements.txt (line 10)) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.0->-r requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 3)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 3)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 3)) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 3)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r requirements.txt (line 3)) (1.20.1)\n"
          ]
        }
      ],
      "source": [
        "!wget -O requirements.txt https://raw.githubusercontent.com/jepilogo97/nlp/main/nlp-with-gpt/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "outputs": [],
      "source": [
        "# Procesamiento de lenguaje natural y utilidades\n",
        "import numpy as np  # Cálculo numérico y manejo de arreglos multidimensionales\n",
        "import pandas as pd  # Manipulación y análisis de datos en estructuras tipo DataFrame\n",
        "\n",
        "pd.set_option(\"display.max_rows\", None)     # Todas las filas\n",
        "pd.set_option(\"display.max_columns\", None)  # Todas las columnas\n",
        "pd.set_option(\"display.width\", None)        # No cortar líneas\n",
        "\n",
        "from datasets import Dataset, load_dataset, concatenate_datasets  # Carga y combinación de datasets de Hugging Face\n",
        "from datasets.dataset_dict import DatasetDict\n",
        "from collections import Counter  # Conteo de frecuencias de elementos (tokens, palabras, etc.)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Deep Learning con PyTorch\n",
        "import torch  # Librería principal de tensores y operaciones en GPU/CPU\n",
        "import torch.nn as nn  # Definición de capas y módulos de redes neuronales\n",
        "import torch.nn.functional as F  # Funciones de activación y operaciones matemáticas de redes\n",
        "from torch.utils.data import random_split, DataLoader, Subset  # Utilidades para crear y dividir datasets, cargar lotes y trabajar con subconjuntos\n",
        "from torchinfo import summary\n",
        "\n",
        "# Entrenamiento estructurado con PyTorch Lightning\n",
        "from pytorch_lightning import LightningModule, Trainer  # Clase base y manejador de entrenamiento de modelos\n",
        "from pytorch_lightning.loggers import TensorBoardLogger  # Registro de métricas e historial en TensorBoard\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint  # Detener entrenamiento si no mejora la métrica\n",
        "from torchmetrics import Accuracy  # Métrica de precisión para clasificación supervisada\n",
        "\n",
        "# Tipado para mayor legibilidad y validación de funciones\n",
        "from typing import Optional, Tuple # Definición de tipos de datos para funciones y estructuras\n",
        "\n",
        "from tqdm.auto import tqdm  # Barra de progreso adaptable para bucles\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments  # Tokenizador automático de modelos preentrenados de Hugging Face\n",
        "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode  # Conversión de bytes a caracteres Unicode (usado en tokenización tipo GPT-2)\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "\n",
        "# Métricas de evaluación con Scikit-learn\n",
        "from sklearn.model_selection import train_test_split  # División de datos en conjuntos de entrenamiento y prueba\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # Métricas de evaluación de modelos de clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "source": [
        "### 2. Generative pre-training Transformer - GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "source": [
        "Los modelos tipo GPT, introducidos por Radfor, et.al., de OpenAI, al igual que los modelos BERT, hacen uso extensivo de la arquitectura de transformers como hemos estado viendo. Las diferencias claves se podrían resumir en:\n",
        "\n",
        "1. GPT utiliza bloques de **Transformer Decoder** encadenados, mientras que el modelo BERT utiliza bloques de *Transformer Encoder*\n",
        "2. GPT se centra en la generación de texto basado en un contexto, la tarea principal es la predicción del siguiente token en la secuencia, mientras que BERT se centra en el completado de partes de una secuencia, en función de un contexto anterior y posterior a la secuencia de entrada. Entonces BERT se centra en la construicción de representación de lenguage, mientras que GPT se centra en la generación de texto en función de un contexto.\n",
        "\n",
        "Sin embargo, ambos se basan en la misma premisa de pre-entrenar el modelo en tareas no-supervisadas o semi-supervisadas para que el modelo aprenda las representaciones semánticas del lenguage y luego al modelo se le pueda hacer fine tuning a tareas posteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos el gpt neo en este caso de 125m, dada las limitaciones de memoria (Se intento con 2.7B y 1.3 B pero no se tenia la suficiente memoria en Ram)."
      ],
      "metadata": {
        "id": "nOj-nFAe210s"
      },
      "id": "nOj-nFAe210s"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9",
        "outputId": "7e1fa604-c357-4f57-f696-23944f144717",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoForCausalLM(\n",
              "  (transformer): GPTNeoModel(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(2048, 768)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPTNeoBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTNeoAttention(\n",
              "          (attention): GPTNeoSelfAttention(\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPTNeoMLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"EleutherAI/gpt-neo-125m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f92fe6ed-e49e-475b-b883-228d3745744e",
      "metadata": {
        "id": "f92fe6ed-e49e-475b-b883-228d3745744e",
        "outputId": "0a2851a3-64f0-4c46-80e5-888eaba5773c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'transformer',\n",
              " 'transformer.wte',\n",
              " 'transformer.wpe',\n",
              " 'transformer.drop',\n",
              " 'transformer.h',\n",
              " 'transformer.h.0',\n",
              " 'transformer.h.0.ln_1',\n",
              " 'transformer.h.0.attn',\n",
              " 'transformer.h.0.attn.attention',\n",
              " 'transformer.h.0.attn.attention.attn_dropout',\n",
              " 'transformer.h.0.attn.attention.resid_dropout',\n",
              " 'transformer.h.0.attn.attention.k_proj',\n",
              " 'transformer.h.0.attn.attention.v_proj',\n",
              " 'transformer.h.0.attn.attention.q_proj',\n",
              " 'transformer.h.0.attn.attention.out_proj',\n",
              " 'transformer.h.0.ln_2',\n",
              " 'transformer.h.0.mlp',\n",
              " 'transformer.h.0.mlp.c_fc',\n",
              " 'transformer.h.0.mlp.c_proj',\n",
              " 'transformer.h.0.mlp.act',\n",
              " 'transformer.h.0.mlp.dropout',\n",
              " 'transformer.h.1',\n",
              " 'transformer.h.1.ln_1',\n",
              " 'transformer.h.1.attn',\n",
              " 'transformer.h.1.attn.attention',\n",
              " 'transformer.h.1.attn.attention.attn_dropout',\n",
              " 'transformer.h.1.attn.attention.resid_dropout',\n",
              " 'transformer.h.1.attn.attention.k_proj',\n",
              " 'transformer.h.1.attn.attention.v_proj',\n",
              " 'transformer.h.1.attn.attention.q_proj',\n",
              " 'transformer.h.1.attn.attention.out_proj',\n",
              " 'transformer.h.1.ln_2',\n",
              " 'transformer.h.1.mlp',\n",
              " 'transformer.h.1.mlp.c_fc',\n",
              " 'transformer.h.1.mlp.c_proj',\n",
              " 'transformer.h.1.mlp.act',\n",
              " 'transformer.h.1.mlp.dropout',\n",
              " 'transformer.h.2',\n",
              " 'transformer.h.2.ln_1',\n",
              " 'transformer.h.2.attn',\n",
              " 'transformer.h.2.attn.attention',\n",
              " 'transformer.h.2.attn.attention.attn_dropout',\n",
              " 'transformer.h.2.attn.attention.resid_dropout',\n",
              " 'transformer.h.2.attn.attention.k_proj',\n",
              " 'transformer.h.2.attn.attention.v_proj',\n",
              " 'transformer.h.2.attn.attention.q_proj',\n",
              " 'transformer.h.2.attn.attention.out_proj',\n",
              " 'transformer.h.2.ln_2',\n",
              " 'transformer.h.2.mlp',\n",
              " 'transformer.h.2.mlp.c_fc',\n",
              " 'transformer.h.2.mlp.c_proj',\n",
              " 'transformer.h.2.mlp.act',\n",
              " 'transformer.h.2.mlp.dropout',\n",
              " 'transformer.h.3',\n",
              " 'transformer.h.3.ln_1',\n",
              " 'transformer.h.3.attn',\n",
              " 'transformer.h.3.attn.attention',\n",
              " 'transformer.h.3.attn.attention.attn_dropout',\n",
              " 'transformer.h.3.attn.attention.resid_dropout',\n",
              " 'transformer.h.3.attn.attention.k_proj',\n",
              " 'transformer.h.3.attn.attention.v_proj',\n",
              " 'transformer.h.3.attn.attention.q_proj',\n",
              " 'transformer.h.3.attn.attention.out_proj',\n",
              " 'transformer.h.3.ln_2',\n",
              " 'transformer.h.3.mlp',\n",
              " 'transformer.h.3.mlp.c_fc',\n",
              " 'transformer.h.3.mlp.c_proj',\n",
              " 'transformer.h.3.mlp.act',\n",
              " 'transformer.h.3.mlp.dropout',\n",
              " 'transformer.h.4',\n",
              " 'transformer.h.4.ln_1',\n",
              " 'transformer.h.4.attn',\n",
              " 'transformer.h.4.attn.attention',\n",
              " 'transformer.h.4.attn.attention.attn_dropout',\n",
              " 'transformer.h.4.attn.attention.resid_dropout',\n",
              " 'transformer.h.4.attn.attention.k_proj',\n",
              " 'transformer.h.4.attn.attention.v_proj',\n",
              " 'transformer.h.4.attn.attention.q_proj',\n",
              " 'transformer.h.4.attn.attention.out_proj',\n",
              " 'transformer.h.4.ln_2',\n",
              " 'transformer.h.4.mlp',\n",
              " 'transformer.h.4.mlp.c_fc',\n",
              " 'transformer.h.4.mlp.c_proj',\n",
              " 'transformer.h.4.mlp.act',\n",
              " 'transformer.h.4.mlp.dropout',\n",
              " 'transformer.h.5',\n",
              " 'transformer.h.5.ln_1',\n",
              " 'transformer.h.5.attn',\n",
              " 'transformer.h.5.attn.attention',\n",
              " 'transformer.h.5.attn.attention.attn_dropout',\n",
              " 'transformer.h.5.attn.attention.resid_dropout',\n",
              " 'transformer.h.5.attn.attention.k_proj',\n",
              " 'transformer.h.5.attn.attention.v_proj',\n",
              " 'transformer.h.5.attn.attention.q_proj',\n",
              " 'transformer.h.5.attn.attention.out_proj',\n",
              " 'transformer.h.5.ln_2',\n",
              " 'transformer.h.5.mlp',\n",
              " 'transformer.h.5.mlp.c_fc',\n",
              " 'transformer.h.5.mlp.c_proj',\n",
              " 'transformer.h.5.mlp.act',\n",
              " 'transformer.h.5.mlp.dropout',\n",
              " 'transformer.h.6',\n",
              " 'transformer.h.6.ln_1',\n",
              " 'transformer.h.6.attn',\n",
              " 'transformer.h.6.attn.attention',\n",
              " 'transformer.h.6.attn.attention.attn_dropout',\n",
              " 'transformer.h.6.attn.attention.resid_dropout',\n",
              " 'transformer.h.6.attn.attention.k_proj',\n",
              " 'transformer.h.6.attn.attention.v_proj',\n",
              " 'transformer.h.6.attn.attention.q_proj',\n",
              " 'transformer.h.6.attn.attention.out_proj',\n",
              " 'transformer.h.6.ln_2',\n",
              " 'transformer.h.6.mlp',\n",
              " 'transformer.h.6.mlp.c_fc',\n",
              " 'transformer.h.6.mlp.c_proj',\n",
              " 'transformer.h.6.mlp.act',\n",
              " 'transformer.h.6.mlp.dropout',\n",
              " 'transformer.h.7',\n",
              " 'transformer.h.7.ln_1',\n",
              " 'transformer.h.7.attn',\n",
              " 'transformer.h.7.attn.attention',\n",
              " 'transformer.h.7.attn.attention.attn_dropout',\n",
              " 'transformer.h.7.attn.attention.resid_dropout',\n",
              " 'transformer.h.7.attn.attention.k_proj',\n",
              " 'transformer.h.7.attn.attention.v_proj',\n",
              " 'transformer.h.7.attn.attention.q_proj',\n",
              " 'transformer.h.7.attn.attention.out_proj',\n",
              " 'transformer.h.7.ln_2',\n",
              " 'transformer.h.7.mlp',\n",
              " 'transformer.h.7.mlp.c_fc',\n",
              " 'transformer.h.7.mlp.c_proj',\n",
              " 'transformer.h.7.mlp.act',\n",
              " 'transformer.h.7.mlp.dropout',\n",
              " 'transformer.h.8',\n",
              " 'transformer.h.8.ln_1',\n",
              " 'transformer.h.8.attn',\n",
              " 'transformer.h.8.attn.attention',\n",
              " 'transformer.h.8.attn.attention.attn_dropout',\n",
              " 'transformer.h.8.attn.attention.resid_dropout',\n",
              " 'transformer.h.8.attn.attention.k_proj',\n",
              " 'transformer.h.8.attn.attention.v_proj',\n",
              " 'transformer.h.8.attn.attention.q_proj',\n",
              " 'transformer.h.8.attn.attention.out_proj',\n",
              " 'transformer.h.8.ln_2',\n",
              " 'transformer.h.8.mlp',\n",
              " 'transformer.h.8.mlp.c_fc',\n",
              " 'transformer.h.8.mlp.c_proj',\n",
              " 'transformer.h.8.mlp.act',\n",
              " 'transformer.h.8.mlp.dropout',\n",
              " 'transformer.h.9',\n",
              " 'transformer.h.9.ln_1',\n",
              " 'transformer.h.9.attn',\n",
              " 'transformer.h.9.attn.attention',\n",
              " 'transformer.h.9.attn.attention.attn_dropout',\n",
              " 'transformer.h.9.attn.attention.resid_dropout',\n",
              " 'transformer.h.9.attn.attention.k_proj',\n",
              " 'transformer.h.9.attn.attention.v_proj',\n",
              " 'transformer.h.9.attn.attention.q_proj',\n",
              " 'transformer.h.9.attn.attention.out_proj',\n",
              " 'transformer.h.9.ln_2',\n",
              " 'transformer.h.9.mlp',\n",
              " 'transformer.h.9.mlp.c_fc',\n",
              " 'transformer.h.9.mlp.c_proj',\n",
              " 'transformer.h.9.mlp.act',\n",
              " 'transformer.h.9.mlp.dropout',\n",
              " 'transformer.h.10',\n",
              " 'transformer.h.10.ln_1',\n",
              " 'transformer.h.10.attn',\n",
              " 'transformer.h.10.attn.attention',\n",
              " 'transformer.h.10.attn.attention.attn_dropout',\n",
              " 'transformer.h.10.attn.attention.resid_dropout',\n",
              " 'transformer.h.10.attn.attention.k_proj',\n",
              " 'transformer.h.10.attn.attention.v_proj',\n",
              " 'transformer.h.10.attn.attention.q_proj',\n",
              " 'transformer.h.10.attn.attention.out_proj',\n",
              " 'transformer.h.10.ln_2',\n",
              " 'transformer.h.10.mlp',\n",
              " 'transformer.h.10.mlp.c_fc',\n",
              " 'transformer.h.10.mlp.c_proj',\n",
              " 'transformer.h.10.mlp.act',\n",
              " 'transformer.h.10.mlp.dropout',\n",
              " 'transformer.h.11',\n",
              " 'transformer.h.11.ln_1',\n",
              " 'transformer.h.11.attn',\n",
              " 'transformer.h.11.attn.attention',\n",
              " 'transformer.h.11.attn.attention.attn_dropout',\n",
              " 'transformer.h.11.attn.attention.resid_dropout',\n",
              " 'transformer.h.11.attn.attention.k_proj',\n",
              " 'transformer.h.11.attn.attention.v_proj',\n",
              " 'transformer.h.11.attn.attention.q_proj',\n",
              " 'transformer.h.11.attn.attention.out_proj',\n",
              " 'transformer.h.11.ln_2',\n",
              " 'transformer.h.11.mlp',\n",
              " 'transformer.h.11.mlp.c_fc',\n",
              " 'transformer.h.11.mlp.c_proj',\n",
              " 'transformer.h.11.mlp.act',\n",
              " 'transformer.h.11.mlp.dropout',\n",
              " 'transformer.ln_f',\n",
              " 'lm_head']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "modules = [m for m, _ in model.named_modules()]\n",
        "modules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f02a8c59-a7cb-458e-be52-19116d37b264",
      "metadata": {
        "id": "f02a8c59-a7cb-458e-be52-19116d37b264"
      },
      "source": [
        "Observermos un ejemplo de generación simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1f6d06a4-d24b-440d-a35c-8d7ab4ce6477",
      "metadata": {
        "id": "1f6d06a4-d24b-440d-a35c-8d7ab4ce6477",
        "outputId": "ec882483-3208-4b21-de08-f07f10dbb8e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones de la entrada: torch.Size([1, 14])\n",
            "Dimensiones de la salida: torch.Size([1, 14, 50257])\n",
            "Dimensiones del último token de la secuencia: torch.Size([50257])\n",
            "Dimensiones de la probabilidad de los tokens: torch.Size([50257])\n",
            "{' we': '7.82%', ' the': '6.88%', ' I': '4.46%', ' is': '3.97%', ' a': '3.79%', ' MIT': '3.69%', ' you': '3.64%', ' and': '3.04%', ' Artificial': '2.95%', ' it': '1.95%'}\n"
          ]
        }
      ],
      "source": [
        "text = \"As part of MIT course 6S099, Artificial General Intelligence,\"\n",
        "best = 10\n",
        "\n",
        "with torch.no_grad():\n",
        "    tokens = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\n",
        "    print(\"Dimensiones de la entrada:\", tokens.shape)\n",
        "    output = model(input_ids=tokens)\n",
        "    print(\"Dimensiones de la salida:\", output.logits.shape)\n",
        "    output = output.logits[0, -1, :]\n",
        "    print(\"Dimensiones del último token de la secuencia:\", output.shape)\n",
        "    probs = torch.softmax(output, dim=-1)\n",
        "    print(\"Dimensiones de la probabilidad de los tokens:\", probs.shape)\n",
        "    sorted_probs = torch.argsort(probs, dim=-1, descending=True)\n",
        "    print({tokenizer.decode(token): f\"{prob.cpu().numpy() * 100:.2f}%\" for token, prob in zip(sorted_probs[:best], probs[sorted_probs[:best]])})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "source": [
        "### 2. Implementando una función de generación"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21",
      "metadata": {
        "id": "21"
      },
      "source": [
        "Ahora, la idea es que este modelo nos sirva para generar texto de forma recurrente e incremental. En la última capa de los modelos tipo GPT encontrarémos un tensor con forma $(b, s, v)$, donde:\n",
        "\n",
        "- $b$: Es el tamaño del bache, o la cantidad de secuencias a procesar.\n",
        "- $s$: Es la longitud de la secuencia de entrada.\n",
        "- $v$: Es el tamaño del vocabulario del modelo, cuantos tokens soporta.\n",
        "\n",
        "Pero este es el tensor de salida, por qué tiene la forma de la secuencia de entrada?, porque cada posición en la salida corresponde a la la predicción del siguiente token de esa posición en la secuencia de entrada. En otras palabras, lo que obtenemos como predicción, es una secuencia de igual tamaño a la de entrada, movida un token hacia adelante, lo que efectivamente nos predice un solo token a la vez y por ende, el token que nos insteresa, es el último.\n",
        "\n",
        "Lo que obtenemos en este tensor es además los logits de TODO el vocabulario del modelo, con los cuales podemos calcular las probabilidades de que cada uno sea el que continue en la secuencia. Hay varias formas de decodificar el siguiente token, la más fácil de implementar sería una decodificación codiciosa (greedy) del siguiente token, que consiste simplemente en seleccionar el token con la probabilidad más alta. Este es un enfoque simple y efectivo para algunos casos, pero al mismo tiempo sufre de poca variabilidad e incluso puede caer en generación repetitiva.\n",
        "\n",
        "Otra opción es el muestreo, ya que justamente podemos obtener probabilidades del siguiente token, lo más lógico sería muestrear con esas opciones de probabilidad, de este modo podemos obtener mayor diversidad a la hora de generar el texto, al costo eso si de que haya mayor aleatoridad ya que se le daría la oportunidad a incluso tokens con baja probabilidad, de ser seleccionados.\n",
        "\n",
        "Otra opción podría ser un balanceo entre una decodificación greedy y una por muestreo, en función de otro hiperparámetro que podemos definir. Esta sería una técnica muy común en el contexto de Reinforcement Learning llamade e-greedy. Se hace la aclaración de que en este ejemplo no harémos nada de RL, solamente se hace mención de esta técnica para balancear entre explotación y exploración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1e473a39-4f5c-4ee4-80b5-c84a5eaa6094",
      "metadata": {
        "id": "1e473a39-4f5c-4ee4-80b5-c84a5eaa6094"
      },
      "outputs": [],
      "source": [
        "def generate(\n",
        "        model: nn.Module,\n",
        "        tokenizer: PreTrainedTokenizerBase,\n",
        "        start: str,\n",
        "        max_length: int = 1000,\n",
        "        eps: float = 0.5,\n",
        "        top_n: int = 5,\n",
        "        return_iterations: bool = False,\n",
        "        device: str = \"cpu\") -> Tuple[str, Optional[pd.DataFrame]]:\n",
        "\n",
        "    output = [start]\n",
        "    iterations = []\n",
        "    with torch.no_grad():\n",
        "        input_ids = tokenizer(output[-1], return_tensors='pt')['input_ids'].to(device)\n",
        "        for _ in range(max_length):\n",
        "            # Tomamos los logits producidos por la última capa del modelo\n",
        "            # Estos corresponden al siguiente token por cada posición de la cadena\n",
        "            logits = model(input_ids=input_ids).logits\n",
        "            # Por lo tanto, el que nos interesa es el último, que correspondería a la\n",
        "            # predicción del siguiente token después del final de la cadena original\n",
        "            # A este aplicamos un softmax para obtener las probabilidades por cada\n",
        "            # token del vocabulario para estar presente en la cadena.\n",
        "            probs = torch.softmax(logits[0, -1, :], dim=-1)\n",
        "            # Simplemente ordenamos por probabilidad de forma descendente\n",
        "            sorted_tokens = torch.argsort(probs, dim=-1, descending=True)\n",
        "\n",
        "            # Utilizamos una politica tipo e-greedy para obtener el siguiente token de la secuencia\n",
        "            # Un eps>=1 quiere decir que siempre se va seleccionar el token de forma 'greedy', es decir\n",
        "            # siempre se toma el token con probabilidad más alta.\n",
        "\n",
        "            # Un eps=0 quiere decir que siempre se va a muestrear el siguiente token en función\n",
        "            # de las probabilidades de cada token\n",
        "\n",
        "            # Un 0<eps<1 va a balancear de forma binomial entre tomar el token con la\n",
        "            # probabilidad más alta y muestrear el token en función de sus probabilidades.\n",
        "            if np.random.random_sample(1)[0] < eps:\n",
        "                # Se toma el mejor token\n",
        "                next_token = sorted_tokens[0].unsqueeze(dim=0)\n",
        "            else:\n",
        "                # Se muetrea el token de la probabilidad de distribución\n",
        "                next_token = torch.multinomial(probs, 1)\n",
        "\n",
        "            if return_iterations:\n",
        "                # Mantenemos pista de todas las iteraciones para análisis\n",
        "                iteration = {'input': ''.join(output)}\n",
        "                best_n = sorted_tokens[:top_n].cpu().tolist()\n",
        "                choices = {f'Choice #{choice+1}': f'{tokenizer.decode(token)} ({prob:.4f})' for choice, (token, prob) in enumerate(zip(best_n, probs[best_n].cpu().tolist()))}\n",
        "                iteration.update(choices)\n",
        "                iterations.append(iteration)\n",
        "\n",
        "            output.append(tokenizer.decode(next_token))\n",
        "            input_ids = torch.cat([input_ids, next_token.unsqueeze(dim=0)], dim=-1)\n",
        "\n",
        "        output_text = ''.join(output)\n",
        "        if not return_iterations:\n",
        "            return output_text, None\n",
        "        else:\n",
        "            df = pd.DataFrame(iterations)\n",
        "            return output_text, df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cba98f34-26fc-4f8d-9340-99950a1cdba1",
      "metadata": {
        "id": "cba98f34-26fc-4f8d-9340-99950a1cdba1"
      },
      "source": [
        "Ahora observemos que pasa cuando generamos texto con nuestra función y algunos parámetros.\n",
        "\n",
        "Primero, observemos que pasa cuando pasamos un `eps=1` que quiere decir que la generación va a ser de tipo greedy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a476e165-88bd-4612-8a85-38187028cae6",
      "metadata": {
        "id": "a476e165-88bd-4612-8a85-38187028cae6",
        "outputId": "e05161f9-933a-4a07-b0d7-0a60d9d7f4d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As part of MIT course 6S099, Artificial General Intelligence, we will be using the MIT Lab to develop a new framework for AI.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                input            Choice #1  \\\n",
              "0   As part of MIT course 6S099, Artificial Genera...          we (0.0782)   \n",
              "1   As part of MIT course 6S099, Artificial Genera...        will (0.0905)   \n",
              "2   As part of MIT course 6S099, Artificial Genera...          be (0.1654)   \n",
              "3   As part of MIT course 6S099, Artificial Genera...       using (0.1106)   \n",
              "4   As part of MIT course 6S099, Artificial Genera...         the (0.1655)   \n",
              "5   As part of MIT course 6S099, Artificial Genera...         MIT (0.0519)   \n",
              "6   As part of MIT course 6S099, Artificial Genera...         Lab (0.0481)   \n",
              "7   As part of MIT course 6S099, Artificial Genera...          to (0.2280)   \n",
              "8   As part of MIT course 6S099, Artificial Genera...     develop (0.0766)   \n",
              "9   As part of MIT course 6S099, Artificial Genera...           a (0.2434)   \n",
              "10  As part of MIT course 6S099, Artificial Genera...         new (0.1133)   \n",
              "11  As part of MIT course 6S099, Artificial Genera...   framework (0.0418)   \n",
              "12  As part of MIT course 6S099, Artificial Genera...         for (0.5426)   \n",
              "13  As part of MIT course 6S099, Artificial Genera...          AI (0.0760)   \n",
              "14  As part of MIT course 6S099, Artificial Genera...           . (0.2004)   \n",
              "\n",
              "                Choice #2             Choice #3           Choice #4  \\\n",
              "0            the (0.0688)            I (0.0446)         is (0.0397)   \n",
              "1            are (0.0776)            � (0.0718)       have (0.0715)   \n",
              "2            use (0.0468)        learn (0.0461)    discuss (0.0449)   \n",
              "3    introducing (0.0486)   discussing (0.0434)    working (0.0357)   \n",
              "4              a (0.1058)   Artificial (0.0570)        our (0.0470)   \n",
              "5         latest (0.0446)         same (0.0252)         AI (0.0236)   \n",
              "6           Open (0.0433)   Artificial (0.0347)          - (0.0320)   \n",
              "7              � (0.1425)           's (0.0936)         as (0.0927)   \n",
              "8          study (0.0623)       create (0.0622)      build (0.0507)   \n",
              "9            and (0.0764)          the (0.0556)         an (0.0439)   \n",
              "10     framework (0.0242)      machine (0.0216)    variety (0.0188)   \n",
              "11           way (0.0364)       method (0.0350)   approach (0.0299)   \n",
              "12        called (0.1098)           to (0.0865)       that (0.0578)   \n",
              "13           the (0.0556)   artificial (0.0432)   learning (0.0292)   \n",
              "14             , (0.0836)          and (0.0834)       that (0.0682)   \n",
              "\n",
              "              Choice #5             Choice #6             Choice #7  \\\n",
              "0            a (0.0379)          MIT (0.0369)          you (0.0364)   \n",
              "1      present (0.0243)      provide (0.0195)          'll (0.0177)   \n",
              "2    introduce (0.0385)      explore (0.0371)        cover (0.0353)   \n",
              "3    exploring (0.0322)   presenting (0.0295)       taking (0.0252)   \n",
              "4           AI (0.0445)         this (0.0442)           an (0.0224)   \n",
              "5          new (0.0234)    following (0.0183)         code (0.0121)   \n",
              "6           AI (0.0148)   Technology (0.0144)       course (0.0107)   \n",
              "7          for (0.0872)           of (0.0261)           at (0.0220)   \n",
              "8      explore (0.0462)        learn (0.0422)      provide (0.0220)   \n",
              "9           AI (0.0410)          our (0.0329)          new (0.0290)   \n",
              "10       novel (0.0170)       number (0.0160)         deep (0.0155)   \n",
              "11        tool (0.0289)           AI (0.0279)        class (0.0255)   \n",
              "12           , (0.0388)           of (0.0176)            . (0.0155)   \n",
              "13        deep (0.0285)      machine (0.0272)   Artificial (0.0207)   \n",
              "14           - (0.0553)     research (0.0440)     learning (0.0238)   \n",
              "\n",
              "                  Choice #8             Choice #9          Choice #10  \n",
              "0              and (0.0304)   Artificial (0.0295)         it (0.0195)  \n",
              "1              've (0.0176)          're (0.0158)        use (0.0155)  \n",
              "2             show (0.0212)        teach (0.0198)    present (0.0170)  \n",
              "3         teaching (0.0231)      looking (0.0224)   focusing (0.0215)  \n",
              "4             Deep (0.0147)   artificial (0.0144)      these (0.0127)  \n",
              "5       Artificial (0.0120)         Deep (0.0120)       most (0.0080)  \n",
              "6            Cloud (0.0104)    framework (0.0099)   Learning (0.0094)  \n",
              "7                , (0.0206)           in (0.0166)        and (0.0157)  \n",
              "8            teach (0.0192)      analyze (0.0182)       help (0.0175)  \n",
              "9       artificial (0.0237)   Artificial (0.0232)    machine (0.0165)  \n",
              "10            tool (0.0146)       system (0.0128)    program (0.0117)  \n",
              "11            type (0.0184)         kind (0.0161)   language (0.0150)  \n",
              "12             and (0.0141)           in (0.0116)      which (0.0107)  \n",
              "13   understanding (0.0191)     building (0.0187)    general (0.0150)  \n",
              "14              in (0.0224)           to (0.0174)   training (0.0146)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a4dfdc3-632a-4002-be12-2bb9a891782c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>Choice #1</th>\n",
              "      <th>Choice #2</th>\n",
              "      <th>Choice #3</th>\n",
              "      <th>Choice #4</th>\n",
              "      <th>Choice #5</th>\n",
              "      <th>Choice #6</th>\n",
              "      <th>Choice #7</th>\n",
              "      <th>Choice #8</th>\n",
              "      <th>Choice #9</th>\n",
              "      <th>Choice #10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>we (0.0782)</td>\n",
              "      <td>the (0.0688)</td>\n",
              "      <td>I (0.0446)</td>\n",
              "      <td>is (0.0397)</td>\n",
              "      <td>a (0.0379)</td>\n",
              "      <td>MIT (0.0369)</td>\n",
              "      <td>you (0.0364)</td>\n",
              "      <td>and (0.0304)</td>\n",
              "      <td>Artificial (0.0295)</td>\n",
              "      <td>it (0.0195)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>will (0.0905)</td>\n",
              "      <td>are (0.0776)</td>\n",
              "      <td>� (0.0718)</td>\n",
              "      <td>have (0.0715)</td>\n",
              "      <td>present (0.0243)</td>\n",
              "      <td>provide (0.0195)</td>\n",
              "      <td>'ll (0.0177)</td>\n",
              "      <td>'ve (0.0176)</td>\n",
              "      <td>'re (0.0158)</td>\n",
              "      <td>use (0.0155)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>be (0.1654)</td>\n",
              "      <td>use (0.0468)</td>\n",
              "      <td>learn (0.0461)</td>\n",
              "      <td>discuss (0.0449)</td>\n",
              "      <td>introduce (0.0385)</td>\n",
              "      <td>explore (0.0371)</td>\n",
              "      <td>cover (0.0353)</td>\n",
              "      <td>show (0.0212)</td>\n",
              "      <td>teach (0.0198)</td>\n",
              "      <td>present (0.0170)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>using (0.1106)</td>\n",
              "      <td>introducing (0.0486)</td>\n",
              "      <td>discussing (0.0434)</td>\n",
              "      <td>working (0.0357)</td>\n",
              "      <td>exploring (0.0322)</td>\n",
              "      <td>presenting (0.0295)</td>\n",
              "      <td>taking (0.0252)</td>\n",
              "      <td>teaching (0.0231)</td>\n",
              "      <td>looking (0.0224)</td>\n",
              "      <td>focusing (0.0215)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>the (0.1655)</td>\n",
              "      <td>a (0.1058)</td>\n",
              "      <td>Artificial (0.0570)</td>\n",
              "      <td>our (0.0470)</td>\n",
              "      <td>AI (0.0445)</td>\n",
              "      <td>this (0.0442)</td>\n",
              "      <td>an (0.0224)</td>\n",
              "      <td>Deep (0.0147)</td>\n",
              "      <td>artificial (0.0144)</td>\n",
              "      <td>these (0.0127)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>MIT (0.0519)</td>\n",
              "      <td>latest (0.0446)</td>\n",
              "      <td>same (0.0252)</td>\n",
              "      <td>AI (0.0236)</td>\n",
              "      <td>new (0.0234)</td>\n",
              "      <td>following (0.0183)</td>\n",
              "      <td>code (0.0121)</td>\n",
              "      <td>Artificial (0.0120)</td>\n",
              "      <td>Deep (0.0120)</td>\n",
              "      <td>most (0.0080)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>Lab (0.0481)</td>\n",
              "      <td>Open (0.0433)</td>\n",
              "      <td>Artificial (0.0347)</td>\n",
              "      <td>- (0.0320)</td>\n",
              "      <td>AI (0.0148)</td>\n",
              "      <td>Technology (0.0144)</td>\n",
              "      <td>course (0.0107)</td>\n",
              "      <td>Cloud (0.0104)</td>\n",
              "      <td>framework (0.0099)</td>\n",
              "      <td>Learning (0.0094)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>to (0.2280)</td>\n",
              "      <td>� (0.1425)</td>\n",
              "      <td>'s (0.0936)</td>\n",
              "      <td>as (0.0927)</td>\n",
              "      <td>for (0.0872)</td>\n",
              "      <td>of (0.0261)</td>\n",
              "      <td>at (0.0220)</td>\n",
              "      <td>, (0.0206)</td>\n",
              "      <td>in (0.0166)</td>\n",
              "      <td>and (0.0157)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>develop (0.0766)</td>\n",
              "      <td>study (0.0623)</td>\n",
              "      <td>create (0.0622)</td>\n",
              "      <td>build (0.0507)</td>\n",
              "      <td>explore (0.0462)</td>\n",
              "      <td>learn (0.0422)</td>\n",
              "      <td>provide (0.0220)</td>\n",
              "      <td>teach (0.0192)</td>\n",
              "      <td>analyze (0.0182)</td>\n",
              "      <td>help (0.0175)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>a (0.2434)</td>\n",
              "      <td>and (0.0764)</td>\n",
              "      <td>the (0.0556)</td>\n",
              "      <td>an (0.0439)</td>\n",
              "      <td>AI (0.0410)</td>\n",
              "      <td>our (0.0329)</td>\n",
              "      <td>new (0.0290)</td>\n",
              "      <td>artificial (0.0237)</td>\n",
              "      <td>Artificial (0.0232)</td>\n",
              "      <td>machine (0.0165)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>new (0.1133)</td>\n",
              "      <td>framework (0.0242)</td>\n",
              "      <td>machine (0.0216)</td>\n",
              "      <td>variety (0.0188)</td>\n",
              "      <td>novel (0.0170)</td>\n",
              "      <td>number (0.0160)</td>\n",
              "      <td>deep (0.0155)</td>\n",
              "      <td>tool (0.0146)</td>\n",
              "      <td>system (0.0128)</td>\n",
              "      <td>program (0.0117)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>framework (0.0418)</td>\n",
              "      <td>way (0.0364)</td>\n",
              "      <td>method (0.0350)</td>\n",
              "      <td>approach (0.0299)</td>\n",
              "      <td>tool (0.0289)</td>\n",
              "      <td>AI (0.0279)</td>\n",
              "      <td>class (0.0255)</td>\n",
              "      <td>type (0.0184)</td>\n",
              "      <td>kind (0.0161)</td>\n",
              "      <td>language (0.0150)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>for (0.5426)</td>\n",
              "      <td>called (0.1098)</td>\n",
              "      <td>to (0.0865)</td>\n",
              "      <td>that (0.0578)</td>\n",
              "      <td>, (0.0388)</td>\n",
              "      <td>of (0.0176)</td>\n",
              "      <td>. (0.0155)</td>\n",
              "      <td>and (0.0141)</td>\n",
              "      <td>in (0.0116)</td>\n",
              "      <td>which (0.0107)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>AI (0.0760)</td>\n",
              "      <td>the (0.0556)</td>\n",
              "      <td>artificial (0.0432)</td>\n",
              "      <td>learning (0.0292)</td>\n",
              "      <td>deep (0.0285)</td>\n",
              "      <td>machine (0.0272)</td>\n",
              "      <td>Artificial (0.0207)</td>\n",
              "      <td>understanding (0.0191)</td>\n",
              "      <td>building (0.0187)</td>\n",
              "      <td>general (0.0150)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Genera...</td>\n",
              "      <td>. (0.2004)</td>\n",
              "      <td>, (0.0836)</td>\n",
              "      <td>and (0.0834)</td>\n",
              "      <td>that (0.0682)</td>\n",
              "      <td>- (0.0553)</td>\n",
              "      <td>research (0.0440)</td>\n",
              "      <td>learning (0.0238)</td>\n",
              "      <td>in (0.0224)</td>\n",
              "      <td>to (0.0174)</td>\n",
              "      <td>training (0.0146)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a4dfdc3-632a-4002-be12-2bb9a891782c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a4dfdc3-632a-4002-be12-2bb9a891782c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a4dfdc3-632a-4002-be12-2bb9a891782c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bfa1cf26-9c1b-4232-9d6c-e32d1d10fa90\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bfa1cf26-9c1b-4232-9d6c-e32d1d10fa90')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bfa1cf26-9c1b-4232-9d6c-e32d1d10fa90 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "iterations_df",
              "summary": "{\n  \"name\": \"iterations_df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"As part of MIT course 6S099, Artificial General Intelligence, we will be using the MIT Lab to develop\",\n          \"As part of MIT course 6S099, Artificial General Intelligence, we will be using the MIT Lab to develop a new\",\n          \"As part of MIT course 6S099, Artificial General Intelligence,\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice #1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \" a (0.2434)\",\n          \" framework (0.0418)\",\n          \" we (0.0782)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice #2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \" and (0.0764)\",\n          \" way (0.0364)\",\n          \" the (0.0688)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice #3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \" the (0.0556)\",\n          \" method (0.0350)\",\n          \" I (0.0446)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice #4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \" an (0.0439)\",\n          \" approach (0.0299)\",\n          \" is (0.0397)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice #5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \" AI (0.0410)\",\n          \" tool (0.0289)\",\n          \" a (0.0379)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice #6\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \" our (0.0329)\",\n          \" AI (0.0279)\",\n          \" MIT (0.0369)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice #7\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \" new (0.0290)\",\n          \" class (0.0255)\",\n          \" you (0.0364)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice #8\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \" artificial (0.0237)\",\n          \" type (0.0184)\",\n          \" and (0.0304)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice #9\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \" Artificial (0.0232)\",\n          \" kind (0.0161)\",\n          \" Artificial (0.0295)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice #10\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \" machine (0.0165)\",\n          \" language (0.0150)\",\n          \" it (0.0195)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "output_text, iterations_df = generate(model, tokenizer, text, max_length=15, eps=1.0, top_n=10, return_iterations=True, device=device)\n",
        "print(output_text)\n",
        "iterations_df.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7fb81ac-2dec-44ca-a289-40abc66f20fc",
      "metadata": {
        "id": "d7fb81ac-2dec-44ca-a289-40abc66f20fc"
      },
      "source": [
        "Observamos como el input progresa a la vez que las opciones de tokens que hay. Sin importar cuantas veces invoquemos a la función con los mismos parámetros, siempre vamos a obtener los mismos resultados.\n",
        "\n",
        "Ahora, observemos que pasa si introducimos exploración al reducir el `eps=0.5`, lo cual nos dice que aproximadamente la mitad de las veces va a elegir el siguiente token muestreando y la otra mitad explotando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "77b01b10-52a5-4133-a552-1df11138a136",
      "metadata": {
        "id": "77b01b10-52a5-4133-a552-1df11138a136",
        "outputId": "2eea83a8-0751-4ac5-85d6-b73f6fa5b2bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As part of MIT course 6S099, Artificial General Intelligence, in the course of the MIT course Linguistics, will be elaborated. Artificial General Intelligence (AGI) is an artificial intelligence component which will be developed to solve an artificial intelligence problem and for which the AGI technology will be proven to be a superimplemented intelligent machine.\n",
            "\n",
            "Professor Louise Garrett, Associate Professor of Hebrew and Hebrew Literature, will be responsible for answering the questions we have been asked about AI and AI-controller.\n",
            "\n",
            "Professor Louise Garrett was appointed Academy Technical Advisor of\n"
          ]
        }
      ],
      "source": [
        "output_text, _ = generate(model, tokenizer, text, max_length=100, eps=0.5, device=device)\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3rX7rhp3qwbc",
      "metadata": {
        "id": "3rX7rhp3qwbc"
      },
      "source": [
        "Lo primero observado es que aparecen extrañamente muchos signos de interrogación dentro del texto generado.\n",
        "Al intentar aumentar el max length se evidencia que es demasiado costoso, pasar de 100 a 1000 toma un tiempo bastante considerable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28",
      "metadata": {
        "id": "28"
      },
      "source": [
        "### 3. Generando texto con las utilidades del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "source": [
        "Ahora, la clase de Huggingface implementa la función `generate` que hace la labor de generación por nosotros, incluyendo las opciones de muestreo y explotación como hemos observado. Solo que además permite otra serie de parámetros y opciones para controlar la generación de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4717a0ac-3717-4d5b-a858-d6763cd859cb",
      "metadata": {
        "id": "4717a0ac-3717-4d5b-a858-d6763cd859cb",
        "outputId": "9827c2f4-d5fa-4aa6-fc0b-5264d6f81001",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As part of MIT course 6S099, Artificial General Intelligence, the MIT Technology Review, and the MIT Technology Review, I’ve been a part of MIT’s AI and Machine Learning courses for a couple of years. I’ve also been a part of MIT’s AI and Machine Learning courses for a couple of years and have done so at MIT since 1998.\n",
            "\n",
            "I’ve been a part of MIT’s AI and Machine Learning courses for\n"
          ]
        }
      ],
      "source": [
        "output = model.generate(tokens, pad_token_id=tokenizer.eos_token_id, max_length=100, do_sample=True, temperature=0.5, top_k=0)\n",
        "print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R05T73HcrHcj",
      "metadata": {
        "id": "R05T73HcrHcj"
      },
      "source": [
        "La exploración de la temperatura promueve creatividad y diversidad, pero con casos que evidencian incoherencia o “alucinaciones”."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e0bf6ea-a2bb-4276-b355-9e2083eda65f",
      "metadata": {
        "id": "6e0bf6ea-a2bb-4276-b355-9e2083eda65f"
      },
      "source": [
        "### 4. Carga de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56251dff-d7d1-42dc-82d8-4cafa039d2dd",
      "metadata": {
        "id": "56251dff-d7d1-42dc-82d8-4cafa039d2dd"
      },
      "source": [
        "Ahora, intentemos hacer fine tuning a nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a7b6c2ae-c471-467f-a22d-08363a8e36b6",
      "metadata": {
        "id": "a7b6c2ae-c471-467f-a22d-08363a8e36b6",
        "outputId": "cfb43e98-03a3-49a7-fd91-9ad24671312e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['captions', 'title'],\n",
              "        num_rows: 319\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset = load_dataset(\"RamAnanth1/lex-fridman-podcasts\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5cb3f821-996a-4ada-9cb4-b1c50765c020",
      "metadata": {
        "id": "5cb3f821-996a-4ada-9cb4-b1c50765c020",
        "outputId": "2f5615b5-3af9-4bbd-9ef7-8e75936c626e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'captions': \" As part of MIT course 6S099, Artificial General Intelligence, I've gotten the chance to sit down with Max Tegmark. He is a professor here at MIT. He's a physicist, spent a large part of his career studying the mysteries of our cosmological universe. But he's also studied and delved into the beneficial possibilities and the existential risks of artificial intelligence. Amongst many other things, he is the cofounder of the Future of Life Institute, author of two books, both of which I highly recommend. First, Our Mathematical Universe. Second is Life 3.0. He's truly an out of the box thinker and a fun personality, so I really enjoy talking to him. If you'd like to see more of these videos in the future, please subscribe and also click the little bell icon to make sure you don't miss any videos. Also, Twitter, LinkedIn, agi.mit.edu if you wanna watch other lectures or conversations like this one. Better yet, go read Max's book, Life 3.0. Chapter seven on goals is my favorite. It's really where philosophy and engineering come together and it opens with a quote by Dostoevsky. The mystery of human existence lies not in just staying alive but in finding something to live for. Lastly, I believe that every failure rewards us with an opportunity to learn and in that sense, I've been very fortunate to fail in so many new and exciting ways and this conversation was no different. I've learned about something called radio frequency interference, RFI, look it up. Apparently, music and conversations from local radio stations can bleed into the audio that you're recording in such a way that it almost completely ruins that audio. It's an exceptionally difficult sound source to remove. So, I've gotten the opportunity to learn how to avoid RFI in the future during recording sessions. I've also gotten the opportunity to learn how to use Adobe Audition and iZotope RX 6 to do some noise, some audio repair. Of course, this is an exceptionally difficult noise to remove. I am an engineer. I'm not an audio engineer. Neither is anybody else in our group but we did our best. Nevertheless, I thank you for your patience and I hope you're still able to enjoy this conversation. Do you think there's intelligent life out there in the universe? Let's open up with an easy question. I have a minority view here actually. When I give public lectures, I often ask for a show of hands who thinks there's intelligent life out there somewhere else and almost everyone put their hands up and when I ask why, they'll be like, oh, there's so many galaxies out there, there's gotta be. But I'm a numbers nerd, right? So when you look more carefully at it, it's not so clear at all. When we talk about our universe, first of all, we don't mean all of space. We actually mean, I don't know, you can throw me the universe if you want, it's behind you there. It's, we simply mean the spherical region of space from which light has a time to reach us so far during the 14.8 billion year, 13.8 billion years since our Big Bang. There's more space here but this is what we call a universe because that's all we have access to. So is there intelligent life here that's gotten to the point of building telescopes and computers? My guess is no, actually. The probability of it happening on any given planet is some number we don't know what it is. And what we do know is that the number can't be super high because there's over a billion Earth like planets in the Milky Way galaxy alone, many of which are billions of years older than Earth. And aside from some UFO believers, there isn't much evidence that any superduran civilization has come here at all. And so that's the famous Fermi paradox, right? And then if you work the numbers, what you find is that if you have no clue what the probability is of getting life on a given planet, so it could be 10 to the minus 10, 10 to the minus 20, or 10 to the minus two, or any power of 10 is sort of equally likely if you wanna be really open minded, that translates into it being equally likely that our nearest neighbor is 10 to the 16 meters away, 10 to the 17 meters away, 10 to the 18. By the time you get much less than 10 to the 16 already, we pretty much know there is nothing else that close. And when you get beyond 10. Because they would have discovered us. Yeah, they would have been discovered as long ago, or if they're really close, we would have probably noted some engineering projects that they're doing. And if it's beyond 10 to the 26 meters, that's already outside of here. So my guess is actually that we are the only life in here that's gotten the point of building advanced tech, which I think is very, puts a lot of responsibility on our shoulders, not screw up. I think people who take for granted that it's okay for us to screw up, have an accidental nuclear war or go extinct somehow because there's a sort of Star Trek like situation out there where some other life forms are gonna come and bail us out and it doesn't matter as much. I think they're leveling us into a false sense of security. I think it's much more prudent to say, let's be really grateful for this amazing opportunity we've had and make the best of it just in case it is down to us. So from a physics perspective, do you think intelligent life, so it's unique from a sort of statistical view of the size of the universe, but from the basic matter of the universe, how difficult is it for intelligent life to come about? The kind of advanced tech building life is implied in your statement that it's really difficult to create something like a human species. Well, I think what we know is that going from no life to having life that can do a level of tech, there's some sort of two going beyond that than actually settling our whole universe with life. There's some major roadblock there, which is some great filter as it's sometimes called, which is tough to get through. It's either that roadblock is either behind us or in front of us. I'm hoping very much that it's behind us. I'm super excited every time we get a new report from NASA saying they failed to find any life on Mars. I'm like, yes, awesome. Because that suggests that the hard part, maybe it was getting the first ribosome or some very low level kind of stepping stone so that we're home free. Because if that's true, then the future is really only limited by our own imagination. It would be much suckier if it turns out that this level of life is kind of a dime a dozen, but maybe there's some other problem. Like as soon as a civilization gets advanced technology, within a hundred years, they get into some stupid fight with themselves and poof. That would be a bummer. Yeah, so you've explored the mysteries of the universe, the cosmological universe, the one that's sitting between us today. I think you've also begun to explore the other universe, which is sort of the mystery, the mysterious universe of the mind of intelligence, of intelligent life. So is there a common thread between your interest or the way you think about space and intelligence? Oh yeah, when I was a teenager, I was already very fascinated by the biggest questions. And I felt that the two biggest mysteries of all in science were our universe out there and our universe in here. So it's quite natural after having spent a quarter of a century on my career, thinking a lot about this one, that I'm now indulging in the luxury of doing research on this one. It's just so cool. I feel the time is ripe now for you trans greatly deepening our understanding of this. Just start exploring this one. Yeah, because I think a lot of people view intelligence as something mysterious that can only exist in biological organisms like us, and therefore dismiss all talk about artificial general intelligence as science fiction. But from my perspective as a physicist, I am a blob of quarks and electrons moving around in a certain pattern and processing information in certain ways. And this is also a blob of quarks and electrons. I'm not smarter than the water bottle because I'm made of different kinds of quarks. I'm made of up quarks and down quarks, exact same kind as this. There's no secret sauce, I think, in me. It's all about the pattern of the information processing. And this means that there's no law of physics saying that we can't create technology, which can help us by being incredibly intelligent and help us crack mysteries that we couldn't. In other words, I think we've really only seen the tip of the intelligence iceberg so far. Yeah, so the perceptronium. Yeah. So you coined this amazing term. It's a hypothetical state of matter, sort of thinking from a physics perspective, what is the kind of matter that can help, as you're saying, subjective experience emerge, consciousness emerge. So how do you think about consciousness from this physics perspective? Very good question. So again, I think many people have underestimated our ability to make progress on this by convincing themselves it's hopeless because somehow we're missing some ingredient that we need. There's some new consciousness particle or whatever. I happen to think that we're not missing anything and that it's not the interesting thing about consciousness that gives us this amazing subjective experience of colors and sounds and emotions. It's rather something at the higher level about the patterns of information processing. And that's why I like to think about this idea of perceptronium. What does it mean for an arbitrary physical system to be conscious in terms of what its particles are doing or its information is doing? I don't think, I hate carbon chauvinism, this attitude you have to be made of carbon atoms to be smart or conscious. There's something about the information processing that this kind of matter performs. Yeah, and you can see I have my favorite equations here describing various fundamental aspects of the world. I feel that I think one day, maybe someone who's watching this will come up with the equations that information processing has to satisfy to be conscious. I'm quite convinced there is big discovery to be made there because let's face it, we know that so many things are made up of information. We know that some information processing is conscious because we are conscious. But we also know that a lot of information processing is not conscious. Like most of the information processing happening in your brain right now is not conscious. There are like 10 megabytes per second coming in even just through your visual system. You're not conscious about your heartbeat regulation or most things. Even if I just ask you to like read what it says here, you look at it and then, oh, now you know what it said. But you're not aware of how the computation actually happened. Your consciousness is like the CEO that got an email at the end with the final answer. So what is it that makes a difference? I think that's both a great science mystery. We're actually studying it a little bit in my lab here at MIT, but I also think it's just a really urgent question to answer. For starters, I mean, if you're an emergency room doctor and you have an unresponsive patient coming in, wouldn't it be great if in addition to having a CT scanner, you had a consciousness scanner that could figure out whether this person is actually having locked in syndrome or is actually comatose. And in the future, imagine if we build robots or the machine that we can have really good conversations with, which I think is very likely to happen. Wouldn't you want to know if your home helper robot is actually experiencing anything or just like a zombie, I mean, would you prefer it? What would you prefer? Would you prefer that it's actually unconscious so that you don't have to feel guilty about switching it off or giving boring chores or what would you prefer? Well, certainly we would prefer, I would prefer the appearance of consciousness. But the question is whether the appearance of consciousness is different than consciousness itself. And sort of to ask that as a question, do you think we need to understand what consciousness is, solve the hard problem of consciousness in order to build something like an AGI system? No, I don't think that. And I think we will probably be able to build things even if we don't answer that question. But if we want to make sure that what happens is a good thing, we better solve it first. So it's a wonderful controversy you're raising there where you have basically three points of view about the hard problem. So there are two different points of view. They both conclude that the hard problem of consciousness is BS. On one hand, you have some people like Daniel Dennett who say that consciousness is just BS because consciousness is the same thing as intelligence. There's no difference. So anything which acts conscious is conscious, just like we are. And then there are also a lot of people, including many top AI researchers I know, who say, oh, consciousness is just bullshit because, of course, machines can never be conscious. They're always going to be zombies. You never have to feel guilty about how you treat them. And then there's a third group of people, including Giulio Tononi, for example, and Krzysztof Koch and a number of others. I would put myself also in this middle camp who say that actually some information processing is conscious and some is not. So let's find the equation which can be used to determine which it is. And I think we've just been a little bit lazy, kind of running away from this problem for a long time. It's been almost taboo to even mention the C word in a lot of circles because, but we should stop making excuses. This is a science question and there are ways we can even test any theory that makes predictions for this. And coming back to this helper robot, I mean, so you said you'd want your helper robot to certainly act conscious and treat you, like have conversations with you and stuff. I think so. But wouldn't you, would you feel, would you feel a little bit creeped out if you realized that it was just a glossed up tape recorder, you know, that was just zombie and was a faking emotion? Would you prefer that it actually had an experience or would you prefer that it's actually not experiencing anything so you feel, you don't have to feel guilty about what you do to it? It's such a difficult question because, you know, it's like when you're in a relationship and you say, well, I love you. And the other person said, I love you back. It's like asking, well, do they really love you back or are they just saying they love you back? Don't you really want them to actually love you? It's hard to, it's hard to really know the difference between everything seeming like there's consciousness present, there's intelligence present, there's affection, passion, love, and it actually being there. I'm not sure, do you have? But like, can I ask you a question about this? Like to make it a bit more pointed. So Mass General Hospital is right across the river, right? Yes. Suppose you're going in for a medical procedure and they're like, you know, for anesthesia, what we're going to do is we're going to give you muscle relaxants so you won't be able to move and you're going to feel excruciating pain during the whole surgery, but you won't be able to do anything about it. But then we're going to give you this drug that erases your memory of it. Would you be cool about that? What's the difference that you're conscious about it or not if there's no behavioral change, right? Right, that's a really, that's a really clear way to put it. That's, yeah, it feels like in that sense, experiencing it is a valuable quality. So actually being able to have subjective experiences, at least in that case, is valuable. And I think we humans have a little bit of a bad track record also of making these self serving arguments that other entities aren't conscious. You know, people often say, oh, these animals can't feel pain. It's okay to boil lobsters because we ask them if it hurt and they didn't say anything. And now there was just a paper out saying, lobsters do feel pain when you boil them and they're banning it in Switzerland. And we did this with slaves too often and said, oh, they don't mind. They don't maybe aren't conscious or women don't have souls or whatever. So I'm a little bit nervous when I hear people just take as an axiom that machines can't have experience ever. I think this is just a really fascinating science question is what it is. Let's research it and try to figure out what it is that makes the difference between unconscious intelligent behavior and conscious intelligent behavior. So in terms of, so if you think of a Boston Dynamics human or robot being sort of with a broom being pushed around, it starts pushing on a consciousness question. So let me ask, do you think an AGI system like a few neuroscientists believe needs to have a physical embodiment? Needs to have a body or something like a body? No, I don't think so. You mean to have a conscious experience? To have consciousness. I do think it helps a lot to have a physical embodiment to learn the kind of things about the world that are important to us humans, for sure. But I don't think the physical embodiment is necessary after you've learned it to just have the experience. Think about when you're dreaming, right? Your eyes are closed. You're not getting any sensory input. You're not behaving or moving in any way but there's still an experience there, right? And so clearly the experience that you have when you see something cool in your dreams isn't coming from your eyes. It's just the information processing itself in your brain which is that experience, right? But if I put it another way, I'll say because it comes from neuroscience is the reason you want to have a body and a physical something like a physical, you know, a physical system is because you want to be able to preserve something. In order to have a self, you could argue, would you need to have some kind of embodiment of self to want to preserve? Well, now we're getting a little bit anthropomorphic into anthropomorphizing things. Maybe talking about self preservation instincts. I mean, we are evolved organisms, right? So Darwinian evolution endowed us and other evolved organism with a self preservation instinct because those that didn't have those self preservation genes got cleaned out of the gene pool, right? But if you build an artificial general intelligence the mind space that you can design is much, much larger than just a specific subset of minds that can evolve. So an AGI mind doesn't necessarily have to have any self preservation instinct. It also doesn't necessarily have to be so individualistic as us. Like, imagine if you could just, first of all, or we are also very afraid of death. You know, I suppose you could back yourself up every five minutes and then your airplane is about to crash. You're like, shucks, I'm gonna lose the last five minutes of experiences since my last cloud backup, dang. You know, it's not as big a deal. Or if we could just copy experiences between our minds easily like we, which we could easily do if we were silicon based, right? Then maybe we would feel a little bit more like a hive mind actually, that maybe it's the, so I don't think we should take for granted at all that AGI will have to have any of those sort of competitive as alpha male instincts. On the other hand, you know, this is really interesting because I think some people go too far and say, of course we don't have to have any concerns either that advanced AI will have those instincts because we can build anything we want. That there's a very nice set of arguments going back to Steve Omohundro and Nick Bostrom and others just pointing out that when we build machines, we normally build them with some kind of goal, you know, win this chess game, drive this car safely or whatever. And as soon as you put in a goal into machine, especially if it's kind of open ended goal and the machine is very intelligent, it'll break that down into a bunch of sub goals. And one of those goals will almost always be self preservation because if it breaks or dies in the process, it's not gonna accomplish the goal, right? Like suppose you just build a little, you have a little robot and you tell it to go down the store market here and get you some food, make you cook an Italian dinner, you know, and then someone mugs it and tries to break it on the way. That robot has an incentive to not get destroyed and defend itself or run away, because otherwise it's gonna fail in cooking your dinner. It's not afraid of death, but it really wants to complete the dinner cooking goal. So it will have a self preservation instinct. Continue being a functional agent somehow. And similarly, if you give any kind of more ambitious goal to an AGI, it's very likely they wanna acquire more resources so it can do that better. And it's exactly from those sort of sub goals that we might not have intended that some of the concerns about AGI safety come. You give it some goal that seems completely harmless. And then before you realize it, it's also trying to do these other things which you didn't want it to do. And it's maybe smarter than us. So it's fascinating. And let me pause just because I am in a very kind of human centric way, see fear of death as a valuable motivator. So you don't think, you think that's an artifact of evolution, so that's the kind of mind space evolution created that we're sort of almost obsessed about self preservation, some kind of genetic flow. You don't think that's necessary to be afraid of death. So not just a kind of sub goal of self preservation just so you can keep doing the thing, but more fundamentally sort of have the finite thing like this ends for you at some point. Interesting. Do I think it's necessary for what precisely? For intelligence, but also for consciousness. So for those, for both, do you think really like a finite death and the fear of it is important? So before I can answer, before we can agree on whether it's necessary for intelligence or for consciousness, we should be clear on how we define those two words. Cause a lot of really smart people define them in very different ways. I was on this panel with AI experts and they couldn't agree on how to define intelligence even. So I define intelligence simply as the ability to accomplish complex goals. I like your broad definition, because again I don't want to be a carbon chauvinist. Right. And in that case, no, certainly it doesn't require fear of death. I would say alpha go, alpha zero is quite intelligent. I don't think alpha zero has any fear of being turned off because it doesn't understand the concept of it even. And similarly consciousness. I mean, you could certainly imagine very simple kind of experience. If certain plants have any kind of experience I don't think they're very afraid of dying or there's nothing they can do about it anyway much. So there wasn't that much value in, but more seriously I think if you ask, not just about being conscious but maybe having what you would, we might call an exciting life where you feel passion and really appreciate the things. Maybe there somehow, maybe there perhaps it does help having a backdrop that, Hey, it's finite. No, let's make the most of this, let's live to the fullest. So if you knew you were going to live forever do you think you would change your? Yeah, I mean, in some perspective it would be an incredibly boring life living forever. So in the sort of loose subjective terms that you said of something exciting and something in this that other humans would understand, I think is, yeah it seems that the finiteness of it is important. Well, the good news I have for you then is based on what we understand about cosmology everything is in our universe is probably ultimately probably finite, although. Big crunch or big, what's the, the infinite expansion. Yeah, we could have a big chill or a big crunch or a big rip or that's the big snap or death bubbles. All of them are more than a billion years away. So we should, we certainly have vastly more time than our ancestors thought, but there is still it's still pretty hard to squeeze in an infinite number of compute cycles, even though there are some loopholes that just might be possible. But I think, you know, some people like to say that you should live as if you're about to you're going to die in five years or so. And that's sort of optimal. Maybe it's a good assumption. We should build our civilization as if it's all finite to be on the safe side. Right, exactly. So you mentioned defining intelligence as the ability to solve complex goals. Where would you draw a line or how would you try to define human level intelligence and superhuman level intelligence? Where is consciousness part of that definition? No, consciousness does not come into this definition. So, so I think of intelligence as it's a spectrum but there are very many different kinds of goals you can have. You can have a goal to be a good chess player a good goal player, a good car driver, a good investor good poet, et cetera. So intelligence that by its very nature isn't something you can measure by this one number or some overall goodness. No, no. There are some people who are more better at this. Some people are better than that. Right now we have machines that are much better than us at some very narrow tasks like multiplying large numbers fast, memorizing large databases, playing chess playing go and soon driving cars. But there's still no machine that can match a human child in general intelligence but artificial general intelligence, AGI the name of your course, of course that is by its very definition, the quest to build a machine that can do everything as well as we can. So the old Holy grail of AI from back to its inception in the sixties, if that ever happens, of course I think it's going to be the biggest transition in the history of life on earth but it doesn't necessarily have to wait the big impact until machines are better than us at knitting that the really big change doesn't come exactly at the moment they're better than us at everything. The really big change comes first there are big changes when they start becoming better at us at doing most of the jobs that we do because that takes away much of the demand for human labor. And then the really whopping change comes when they become better than us at AI research, right? Because right now the timescale of AI research is limited by the human research and development cycle of years typically, you know how long does it take from one release of some software or iPhone or whatever to the next? But once Google can replace 40,000 engineers by 40,000 equivalent pieces of software or whatever but then there's no reason that has to be years it can be in principle much faster and the timescale of future progress in AI and all of science and technology will be driven by machines, not humans. So it's this simple point which gives right this incredibly fun controversy about whether there can be intelligence explosion so called singularity as Werner Vinge called it. Now the idea is articulated by I.J. Good is obviously way back fifties but you can see Alan Turing and others thought about it even earlier. So you asked me what exactly would I define human level intelligence, yeah. So the glib answer is to say something which is better than us at all cognitive tasks with a better than any human at all cognitive tasks but the really interesting bar I think goes a little bit lower than that actually. It's when they can, when they're better than us at AI programming and general learning so that they can if they want to get better than us at anything by just studying. So they're better is a key word and better is towards this kind of spectrum of the complexity of goals it's able to accomplish. So another way to, and that's certainly a very clear definition of human love. So there's, it's almost like a sea that's rising you can do more and more and more things it's a geographic that you show it's really nice way to put it. So there's some peaks that and there's an ocean level elevating and you solve more and more problems but just kind of to take a pause and we took a bunch of questions and a lot of social networks and a bunch of people asked a sort of a slightly different direction on creativity and things that perhaps aren't a peak. Human beings are flawed and perhaps better means having contradiction being flawed in some way. So let me sort of start easy, first of all. So you have a lot of cool equations. Let me ask, what's your favorite equation, first of all? I know they're all like your children, but like which one is that? This is the shirt in your equation. It's the master key of quantum mechanics of the micro world. So this equation will protect everything to do with atoms, molecules and all the way up. Right? Yeah, so, okay. So quantum mechanics is certainly a beautiful mysterious formulation of our world. So I'd like to sort of ask you, just as an example it perhaps doesn't have the same beauty as physics does but in mathematics abstract, the Andrew Wiles who proved the Fermat's last theorem. So he just saw this recently and it kind of caught my eye a little bit. This is 358 years after it was conjectured. So this is very simple formulation. Everybody tried to prove it, everybody failed. And so here's this guy comes along and eventually proves it and then fails to prove it and then proves it again in 94. And he said like the moment when everything connected into place in an interview said it was so indescribably beautiful. That moment when you finally realize the connecting piece of two conjectures. He said, it was so indescribably beautiful. It was so simple and so elegant. I couldn't understand how I'd missed it. And I just stared at it in disbelief for 20 minutes. Then during the day, I walked around the department and I keep coming back to my desk looking to see if it was still there. It was still there. I couldn't contain myself. I was so excited. It was the most important moment on my working life. Nothing I ever do again will mean as much. So that particular moment. And it kind of made me think of what would it take? And I think we have all been there at small levels. Maybe let me ask, have you had a moment like that in your life where you just had an idea? It's like, wow, yes. I wouldn't mention myself in the same breath as Andrew Wiles, but I've certainly had a number of aha moments when I realized something very cool about physics, which has completely made my head explode. In fact, some of my favorite discoveries I made later, I later realized that they had been discovered earlier by someone who sometimes got quite famous for it. So it's too late for me to even publish it, but that doesn't diminish in any way. The emotional experience you have when you realize it, like, wow. Yeah, so what would it take in that moment, that wow, that was yours in that moment? So what do you think it takes for an intelligence system, an AGI system, an AI system to have a moment like that? That's a tricky question because there are actually two parts to it, right? One of them is, can it accomplish that proof? Can it prove that you can never write A to the N plus B to the N equals three to that equal Z to the N for all integers, et cetera, et cetera, when N is bigger than two? That's simply a question about intelligence. Can you build machines that are that intelligent? And I think by the time we get a machine that can independently come up with that level of proofs, probably quite close to AGI. The second question is a question about consciousness. When will we, how likely is it that such a machine will actually have any experience at all, as opposed to just being like a zombie? And would we expect it to have some sort of emotional response to this or anything at all akin to human emotion where when it accomplishes its machine goal, it views it as somehow something very positive and sublime and deeply meaningful? I would certainly hope that if in the future we do create machines that are our peers or even our descendants, that I would certainly hope that they do have this sublime appreciation of life. In a way, my absolutely worst nightmare would be that at some point in the future, the distant future, maybe our cosmos is teeming with all this post biological life doing all the seemingly cool stuff. And maybe the last humans, by the time our species eventually fizzles out, will be like, well, that's OK because we're so proud of our descendants here. And look what all the, my worst nightmare is that we haven't solved the consciousness problem. And we haven't realized that these are all the zombies. They're not aware of anything any more than a tape recorder has any kind of experience. So the whole thing has just become a play for empty benches. That would be the ultimate zombie apocalypse. So I would much rather, in that case, that we have these beings which can really appreciate how amazing it is. And in that picture, what would be the role of creativity? A few people ask about creativity. When you think about intelligence, certainly the story you told at the beginning of your book involved creating movies and so on, making money. You can make a lot of money in our modern world with music and movies. So if you are an intelligent system, you may want to get good at that. But that's not necessarily what I mean by creativity. Is it important on that complex goals where the sea is rising for there to be something creative? Or am I being very human centric and thinking creativity somehow special relative to intelligence? My hunch is that we should think of creativity simply as an aspect of intelligence. And we have to be very careful with human vanity. We have this tendency to very often want to say, as soon as machines can do something, we try to diminish it and say, oh, but that's not real intelligence. Isn't it creative or this or that? The other thing, if we ask ourselves to write down a definition of what we actually mean by being creative, what we mean by Andrew Wiles, what he did there, for example, don't we often mean that someone takes a very unexpected leap? It's not like taking 573 and multiplying it by 224 by just a step of straightforward cookbook like rules, right? You can maybe make a connection between two things that people had never thought was connected or something like that. I think this is an aspect of intelligence. And this is actually one of the most important aspects of it. Maybe the reason we humans tend to be better at it than traditional computers is because it's something that comes more naturally if you're a neural network than if you're a traditional logic gate based computer machine. We physically have all these connections. And you activate here, activate here, activate here. Bing. My hunch is that if we ever build a machine where you could just give it the task, hey, you say, hey, I just realized I want to travel around the world instead this month. Can you teach my AGI course for me? And it's like, OK, I'll do it. And it does everything that you would have done and improvises and stuff. That would, in my mind, involve a lot of creativity. Yeah, so it's actually a beautiful way to put it. I think we do try to grasp at the definition of intelligence is everything we don't understand how to build. So we as humans try to find things that we have and machines don't have. And maybe creativity is just one of the things, one of the words we use to describe that. That's a really interesting way to put it. I don't think we need to be that defensive. I don't think anything good comes out of saying, well, we're somehow special, you know? Contrary wise, there are many examples in history of where trying to pretend that we're somehow superior to all other intelligent beings has led to pretty bad results, right? Nazi Germany, they said that they were somehow superior to other people. Today, we still do a lot of cruelty to animals by saying that we're so superior somehow, and they can't feel pain. Slavery was justified by the same kind of just really weak arguments. And I don't think if we actually go ahead and build artificial general intelligence, it can do things better than us, I don't think we should try to found our self worth on some sort of bogus claims of superiority in terms of our intelligence. I think we should instead find our calling and the meaning of life from the experiences that we have. I can have very meaningful experiences even if there are other people who are smarter than me. When I go to a faculty meeting here, and we talk about something, and then I certainly realize, oh, boy, he has an old prize, he has an old prize, he has an old prize, I don't have one. Does that make me enjoy life any less or enjoy talking to those people less? Of course not. And the contrary, I feel very honored and privileged to get to interact with other very intelligent beings that are better than me at a lot of stuff. So I don't think there's any reason why we can't have the same approach with intelligent machines. That's a really interesting. So people don't often think about that. They think about when there's going, if there's machines that are more intelligent, you naturally think that that's not going to be a beneficial type of intelligence. You don't realize it could be like peers with Nobel prizes that would be just fun to talk with, and they might be clever about certain topics, and you can have fun having a few drinks with them. Well, also, another example we can all relate to of why it doesn't have to be a terrible thing to be in the presence of people who are even smarter than us all around is when you and I were both two years old, I mean, our parents were much more intelligent than us, right? Worked out OK, because their goals were aligned with our goals. And that, I think, is really the number one key issue we have to solve if we value align the value alignment problem, exactly. Because people who see too many Hollywood movies with lousy science fiction plot lines, they worry about the wrong thing, right? They worry about some machine suddenly turning evil. It's not malice that is the concern. It's competence. By definition, intelligent makes you very competent. If you have a more intelligent goal playing, computer playing is a less intelligent one. And when we define intelligence as the ability to accomplish goal winning, it's going to be the more intelligent one that wins. And if you have a human and then you have an AGI that's more intelligent in all ways and they have different goals, guess who's going to get their way, right? So I was just reading about this particular rhinoceros species that was driven extinct just a few years ago. Ellen Bummer is looking at this cute picture of a mommy rhinoceros with its child. And why did we humans drive it to extinction? It wasn't because we were evil rhino haters as a whole. It was just because our goals weren't aligned with those of the rhinoceros. And it didn't work out so well for the rhinoceros because we were more intelligent, right? So I think it's just so important that if we ever do build AGI, before we unleash anything, we have to make sure that it learns to understand our goals, that it adopts our goals, and that it retains those goals. So the cool, interesting problem there is us as human beings trying to formulate our values. So you could think of the United States Constitution as a way that people sat down, at the time a bunch of white men, which is a good example, I should say. They formulated the goals for this country. And a lot of people agree that those goals actually held up pretty well. That's an interesting formulation of values and failed miserably in other ways. So for the value alignment problem and the solution to it, we have to be able to put on paper or in a program human values. How difficult do you think that is? Very. But it's so important. We really have to give it our best. And it's difficult for two separate reasons. There's the technical value alignment problem of figuring out just how to make machines understand our goals, adopt them, and retain them. And then there's the separate part of it, the philosophical part. Whose values anyway? And since it's not like we have any great consensus on this planet on values, what mechanism should we create then to aggregate and decide, OK, what's a good compromise? That second discussion can't just be left to tech nerds like myself. And if we refuse to talk about it and then AGI gets built, who's going to be actually making the decision about whose values? It's going to be a bunch of dudes in some tech company. And are they necessarily so representative of all of humankind that we want to just entrust it to them? Are they even uniquely qualified to speak to future human happiness just because they're good at programming AI? I'd much rather have this be a really inclusive conversation. But do you think it's possible? So you create a beautiful vision that includes the diversity, cultural diversity, and various perspectives on discussing rights, freedoms, human dignity. But how hard is it to come to that consensus? Do you think it's certainly a really important thing that we should all try to do? But do you think it's feasible? I think there's no better way to guarantee failure than to refuse to talk about it or refuse to try. And I also think it's a really bad strategy to say, OK, let's first have a discussion for a long time. And then once we reach complete consensus, then we'll try to load it into some machine. No, we shouldn't let perfect be the enemy of good. Instead, we should start with the kindergarten ethics that pretty much everybody agrees on and put that into machines now. We're not doing that even. Look at anyone who builds this passenger aircraft, wants it to never under any circumstances fly into a building or a mountain. Yet the September 11 hijackers were able to do that. And even more embarrassingly, Andreas Lubitz, this depressed Germanwings pilot, when he flew his passenger jet into the Alps killing over 100 people, he just told the autopilot to do it. He told the freaking computer to change the altitude to 100 meters. And even though it had the GPS maps, everything, the computer was like, OK. So we should take those very basic values, where the problem is not that we don't agree. The problem is just we've been too lazy to try to put it into our machines and make sure that from now on, airplanes will just, which all have computers in them, but will just refuse to do something like that. Go into safe mode, maybe lock the cockpit door, go over to the nearest airport. And there's so much other technology in our world as well now, where it's really becoming quite timely to put in some sort of very basic values like this. Even in cars, we've had enough vehicle terrorism attacks by now, where people have driven trucks and vans into pedestrians, that it's not at all a crazy idea to just have that hardwired into the car. Because yeah, there are a lot of, there's always going to be people who for some reason want to harm others, but most of those people don't have the technical expertise to figure out how to work around something like that. So if the car just won't do it, it helps. So let's start there. So there's a lot of, that's a great point. So not chasing perfect. There's a lot of things that most of the world agrees on. Yeah, let's start there. Let's start there. And then once we start there, we'll also get into the habit of having these kind of conversations about, okay, what else should we put in here and have these discussions? This should be a gradual process then. Great, so, but that also means describing these things and describing it to a machine. So one thing, we had a few conversations with Stephen Wolfram. I'm not sure if you're familiar with Stephen. Oh yeah, I know him quite well. So he is, he works with a bunch of things, but cellular automata, these simple computable things, these computation systems. And he kind of mentioned that, we probably have already within these systems already something that's AGI, meaning like we just don't know it because we can't talk to it. So if you give me this chance to try to at least form a question out of this is, I think it's an interesting idea to think that we can have intelligent systems, but we don't know how to describe something to them and they can't communicate with us. I know you're doing a little bit of work in explainable AI, trying to get AI to explain itself. So what are your thoughts of natural language processing or some kind of other communication? How does the AI explain something to us? How do we explain something to it, to machines? Or you think of it differently? So there are two separate parts to your question there. One of them has to do with communication, which is super interesting, I'll get to that in a sec. The other is whether we already have AGI but we just haven't noticed it there. Right. There I beg to differ. I don't think there's anything in any cellular automaton or anything or the internet itself or whatever that has artificial general intelligence and that it can really do exactly everything we humans can do better. I think the day that happens, when that happens, we will very soon notice, we'll probably notice even before because in a very, very big way. But for the second part, though. Wait, can I ask, sorry. So, because you have this beautiful way to formulating consciousness as information processing, and you can think of intelligence as information processing, and you can think of the entire universe as these particles and these systems roaming around that have this information processing power. You don't think there is something with the power to process information in the way that we human beings do that's out there that needs to be sort of connected to. It seems a little bit philosophical, perhaps, but there's something compelling to the idea that the power is already there, which the focus should be more on being able to communicate with it. Well, I agree that in a certain sense, the hardware processing power is already out there because our universe itself can think of it as being a computer already, right? It's constantly computing what water waves, how it devolved the water waves in the River Charles and how to move the air molecules around. Seth Lloyd has pointed out, my colleague here, that you can even in a very rigorous way think of our entire universe as being a quantum computer. It's pretty clear that our universe supports this amazing processing power because you can even, within this physics computer that we live in, right? We can even build actual laptops and stuff, so clearly the power is there. It's just that most of the compute power that nature has, it's, in my opinion, kind of wasting on boring stuff like simulating yet another ocean wave somewhere where no one is even looking, right? So in a sense, what life does, what we are doing when we build computers is we're rechanneling all this compute that nature is doing anyway into doing things that are more interesting than just yet another ocean wave, and let's do something cool here. So the raw hardware power is there, for sure, but then even just computing what's going to happen for the next five seconds in this water bottle, takes a ridiculous amount of compute if you do it on a human computer. This water bottle just did it. But that does not mean that this water bottle has AGI because AGI means it should also be able to, like I've written my book, done this interview. And I don't think it's just communication problems. I don't really think it can do it. Although Buddhists say when they watch the water and that there is some beauty, that there's some depth and beauty in nature that they can communicate with. Communication is also very important though because I mean, look, part of my job is being a teacher. And I know some very intelligent professors even who just have a bit of hard time communicating. They come up with all these brilliant ideas, but to communicate with somebody else, you have to also be able to simulate their own mind. Yes, empathy. Build well enough and understand model of their mind that you can say things that they will understand. And that's quite difficult. And that's why today it's so frustrating if you have a computer that makes some cancer diagnosis and you ask it, well, why are you saying I should have this surgery? And if it can only reply, I was trained on five terabytes of data and this is my diagnosis, boop, boop, beep, beep. It doesn't really instill a lot of confidence, right? So I think we have a lot of work to do on communication there. So what kind of, I think you're doing a little bit of work in explainable AI. What do you think are the most promising avenues? Is it mostly about sort of the Alexa problem of natural language processing of being able to actually use human interpretable methods of communication? So being able to talk to a system and it talk back to you, or is there some more fundamental problems to be solved? I think it's all of the above. The natural language processing is obviously important, but there are also more nerdy fundamental problems. Like if you take, you play chess? Of course, I'm Russian. I have to. You speak Russian? Yes, I speak Russian. Excellent, I didn't know. When did you learn Russian? I speak very bad Russian, I'm only an autodidact, but I bought a book, Teach Yourself Russian, read a lot, but it was very difficult. Wow. That's why I speak so bad. How many languages do you know? Wow, that's really impressive. I don't know, my wife has some calculation, but my point was, if you play chess, have you looked at the AlphaZero games? The actual games, no. Check it out, some of them are just mind blowing, really beautiful. And if you ask, how did it do that? You go talk to Demis Hassabis, I know others from DeepMind, all they'll ultimately be able to give you is big tables of numbers, matrices, that define the neural network. And you can stare at these tables of numbers till your face turn blue, and you're not gonna understand much about why it made that move. And even if you have natural language processing that can tell you in human language about, oh, five, seven, points, two, eight, still not gonna really help. So I think there's a whole spectrum of fun challenges that are involved in taking a computation that does intelligent things and transforming it into something equally good, equally intelligent, but that's more understandable. And I think that's really valuable because I think as we put machines in charge of ever more infrastructure in our world, the power grid, the trading on the stock market, weapon systems and so on, it's absolutely crucial that we can trust these AIs to do all we want. And trust really comes from understanding in a very fundamental way. And that's why I'm working on this, because I think the more, if we're gonna have some hope of ensuring that machines have adopted our goals and that they're gonna retain them, that kind of trust, I think, needs to be based on things you can actually understand, preferably even improve theorems on. Even with a self driving car, right? If someone just tells you it's been trained on tons of data and it never crashed, it's less reassuring than if someone actually has a proof. Maybe it's a computer verified proof, but still it says that under no circumstances is this car just gonna swerve into oncoming traffic. And that kind of information helps to build trust and helps build the alignment of goals, at least awareness that your goals, your values are aligned. And I think even in the very short term, if you look at how, you know, today, right? This absolutely pathetic state of cybersecurity that we have, where is it? Three billion Yahoo accounts we can't pack, almost every American's credit card and so on. Why is this happening? It's ultimately happening because we have software that nobody fully understood how it worked. That's why the bugs hadn't been found, right? And I think AI can be used very effectively for offense, for hacking, but it can also be used for defense. Hopefully automating verifiability and creating systems that are built in different ways so you can actually prove things about them. And it's important. So speaking of software that nobody understands how it works, of course, a bunch of people ask about your paper, about your thoughts of why does deep and cheap learning work so well? That's the paper. But what are your thoughts on deep learning? These kind of simplified models of our own brains have been able to do some successful perception work, pattern recognition work, and now with AlphaZero and so on, do some clever things. What are your thoughts about the promise limitations of this piece? Great, I think there are a number of very important insights, very important lessons we can always draw from these kinds of successes. One of them is when you look at the human brain, you see it's very complicated, 10th of 11 neurons, and there are all these different kinds of neurons and yada, yada, and there's been this long debate about whether the fact that we have dozens of different kinds is actually necessary for intelligence. We can now, I think, quite convincingly answer that question of no, it's enough to have just one kind. If you look under the hood of AlphaZero, there's only one kind of neuron and it's ridiculously simple mathematical thing. So it's just like in physics, it's not, if you have a gas with waves in it, it's not the detailed nature of the molecule that matter, it's the collective behavior somehow. Similarly, it's this higher level structure of the network that matters, not that you have 20 kinds of neurons. I think our brain is such a complicated mess because it wasn't evolved just to be intelligent, it was involved to also be self assembling and self repairing, right? And evolutionarily attainable. And so on and so on. So I think it's pretty, my hunch is that we're going to understand how to build AGI before we fully understand how our brains work, just like we understood how to build flying machines long before we were able to build a mechanical bird. Yeah, that's right. You've given the example exactly of mechanical birds and airplanes and airplanes do a pretty good job of flying without really mimicking bird flight. And even now after 100 years later, did you see the Ted talk with this German mechanical bird? I heard you mention it. Check it out, it's amazing. But even after that, right, we still don't fly in mechanical birds because it turned out the way we came up with was simpler and it's better for our purposes. And I think it might be the same there. That's one lesson. And another lesson, it's more what our paper was about. First, as a physicist thought it was fascinating how there's a very close mathematical relationship actually between our artificial neural networks and a lot of things that we've studied for in physics go by nerdy names like the renormalization group equation and Hamiltonians and yada, yada, yada. And when you look a little more closely at this, you have, at first I was like, well, there's something crazy here that doesn't make sense. Because we know that if you even want to build a super simple neural network to tell apart cat pictures and dog pictures, right, that you can do that very, very well now. But if you think about it a little bit, you convince yourself it must be impossible because if I have one megapixel, even if each pixel is just black or white, there's two to the power of 1 million possible images, which is way more than there are atoms in our universe, right, so in order to, and then for each one of those, I have to assign a number, which is the probability that it's a dog. So an arbitrary function of images is a list of more numbers than there are atoms in our universe. So clearly I can't store that under the hood of my GPU or my computer, yet somehow it works. So what does that mean? Well, it means that out of all of the problems that you could try to solve with a neural network, almost all of them are impossible to solve with a reasonably sized one. But then what we showed in our paper was that the fraction, the kind of problems, the fraction of all the problems that you could possibly pose, that we actually care about given the laws of physics is also an infinite testimony, tiny little part. And amazingly, they're basically the same part. Yeah, it's almost like our world was created for, I mean, they kind of come together. Yeah, well, you could say maybe where the world was created for us, but I have a more modest interpretation, which is that the world was created for us, but I have a more modest interpretation, which is that instead evolution endowed us with neural networks precisely for that reason. Because this particular architecture, as opposed to the one in your laptop, is very, very well adapted to solving the kind of problems that nature kept presenting our ancestors with. So it makes sense that why do we have a brain in the first place? It's to be able to make predictions about the future and so on. So if we had a sucky system, which could never solve it, we wouldn't have a world. So this is, I think, a very beautiful fact. Yeah. We also realize that there's been earlier work on why deeper networks are good, but we were able to show an additional cool fact there, which is that even incredibly simple problems, like suppose I give you a thousand numbers and ask you to multiply them together, and you can write a few lines of code, boom, done, trivial. If you just try to do that with a neural network that has only one single hidden layer in it, you can do it, but you're going to need two to the power of a thousand neurons to multiply a thousand numbers, which is, again, more neurons than there are atoms in our universe. That's fascinating. But if you allow yourself to make it a deep network with many layers, you only need 4,000 neurons. It's perfectly feasible. That's really interesting. Yeah. So on another architecture type, I mean, you mentioned Schrodinger's equation, and what are your thoughts about quantum computing and the role of this kind of computational unit in creating an intelligence system? In some Hollywood movies that I will not mention by name because I don't want to spoil them. The way they get AGI is building a quantum computer. Because the word quantum sounds cool and so on. That's right. First of all, I think we don't need quantum computers to build AGI. I suspect your brain is not a quantum computer in any profound sense. So you don't even wrote a paper about that a lot many years ago. I calculated the so called decoherence time, how long it takes until the quantum computerness of what your neurons are doing gets erased by just random noise from the environment. And it's about 10 to the minus 21 seconds. So as cool as it would be to have a quantum computer in my head, I don't think that fast. On the other hand, there are very cool things you could do with quantum computers. Or I think we'll be able to do soon when we get bigger ones. That might actually help machine learning do even better than the brain. So for example, one, this is just a moonshot, but learning is very much same thing as search. If you're trying to train a neural network to get really learned to do something really well, you have some loss function, you have a bunch of knobs you can turn, represented by a bunch of numbers, and you're trying to tweak them so that it becomes as good as possible at this thing. So if you think of a landscape with some valley, where each dimension of the landscape corresponds to some number you can change, you're trying to find the minimum. And it's well known that if you have a very high dimensional landscape, complicated things, it's super hard to find the minimum. Quantum mechanics is amazingly good at this. Like if I want to know what's the lowest energy state this water can possibly have, incredibly hard to compute, but nature will happily figure this out for you if you just cool it down, make it very, very cold. If you put a ball somewhere, it'll roll down to its minimum. And this happens metaphorically at the energy landscape too. And quantum mechanics even uses some clever tricks, which today's machine learning systems don't. Like if you're trying to find the minimum and you get stuck in the little local minimum here, in quantum mechanics you can actually tunnel through the barrier and get unstuck again. That's really interesting. Yeah, so it may be, for example, that we'll one day use quantum computers that help train neural networks better. That's really interesting. Okay, so as a component of kind of the learning process, for example. Yeah. Let me ask sort of wrapping up here a little bit, let me return to the questions of our human nature and love, as I mentioned. So do you think, you mentioned sort of a helper robot, but you could think of also personal robots. Do you think the way we human beings fall in love and get connected to each other is possible to achieve in an AI system and human level AI intelligence system? Do you think we would ever see that kind of connection? Or, you know, in all this discussion about solving complex goals, is this kind of human social connection, do you think that's one of the goals on the peaks and valleys with the raising sea levels that we'll be able to achieve? Or do you think that's something that's ultimately, or at least in the short term, relative to the other goals is not achievable? I think it's all possible. And I mean, in recent, there's a very wide range of guesses, as you know, among AI researchers, when we're going to get AGI. Some people, you know, like our friend Rodney Brooks says it's going to be hundreds of years at least. And then there are many others who think it's going to happen much sooner. And recent polls, maybe half or so of AI researchers think we're going to get AGI within decades. So if that happens, of course, then I think these things are all possible. But in terms of whether it will happen, I think we shouldn't spend so much time asking what do we think will happen in the future? As if we are just some sort of pathetic, your passive bystanders, you know, waiting for the future to happen to us. Hey, we're the ones creating this future, right? So we should be proactive about it and ask ourselves what sort of future we would like to have happen. We're going to make it like that. Well, what I prefer is just some sort of incredibly boring, zombie like future where there's all these mechanical things happening and there's no passion, no emotion, no experience, maybe even. No, I would of course, much rather prefer it if all the things that we find that we value the most about humanity are our subjective experience, passion, inspiration, love, you know. If we can create a future where those things do happen, where those things do exist, you know, I think ultimately it's not our universe giving meaning to us, it's us giving meaning to our universe. And if we build more advanced intelligence, let's make sure we build it in such a way that meaning is part of it. A lot of people that seriously study this problem and think of it from different angles have trouble in the majority of cases, if they think through that happen, are the ones that are not beneficial to humanity. And so, yeah, so what are your thoughts? What's should people, you know, I really don't like people to be terrified. What's a way for people to think about it in a way we can solve it and we can make it better? No, I don't think panicking is going to help in any way. It's not going to increase chances of things going well either. Even if you are in a situation where there is a real threat, does it help if everybody just freaks out? No, of course, of course not. I think, yeah, there are of course ways in which things can go horribly wrong. First of all, it's important when we think about this thing, about the problems and risks, to also remember how huge the upsides can be if we get it right, right? Everything we love about society and civilization is a product of intelligence. So if we can amplify our intelligence with machine intelligence and not anymore lose our loved one to what we're told is an incurable disease and things like this, of course, we should aspire to that. So that can be a motivator, I think, reminding ourselves that the reason we try to solve problems is not just because we're trying to avoid gloom, but because we're trying to do something great. But then in terms of the risks, I think the really important question is to ask, what can we do today that will actually help make the outcome good, right? And dismissing the risk is not one of them. I find it quite funny often when I'm in discussion panels about these things, how the people who work for companies, always be like, oh, nothing to worry about, nothing to worry about, nothing to worry about. And it's only academics sometimes express concerns. That's not surprising at all if you think about it. Right. Upton Sinclair quipped, right, that it's hard to make a man believe in something when his income depends on not believing in it. And frankly, we know a lot of these people in companies that they're just as concerned as anyone else. But if you're the CEO of a company, that's not something you want to go on record saying when you have silly journalists who are gonna put a picture of a Terminator robot when they quote you. So the issues are real. And the way I think about what the issue is, is basically the real choice we have is, first of all, are we gonna just dismiss the risks and say, well, let's just go ahead and build machines that can do everything we can do better and cheaper. Let's just make ourselves obsolete as fast as possible. What could possibly go wrong? That's one attitude. The opposite attitude, I think, is to say, here's this incredible potential, let's think about what kind of future we're really, really excited about. What are the shared goals that we can really aspire towards? And then let's think really hard about how we can actually get there. So start with, don't start thinking about the risks, start thinking about the goals. And then when you do that, then you can think about the obstacles you want to avoid. I often get students coming in right here into my office for career advice. I always ask them this very question, where do you want to be in the future? If all she can say is, oh, maybe I'll have cancer, maybe I'll get run over by a truck. Yeah, focus on the obstacles instead of the goals. She's just going to end up a hypochondriac paranoid. Whereas if she comes in and fire in her eyes and is like, I want to be there. And then we can talk about the obstacles and see how we can circumvent them. That's, I think, a much, much healthier attitude. And I feel it's very challenging to come up with a vision for the future, which we are unequivocally excited about. I'm not just talking now in the vague terms, like, yeah, let's cure cancer, fine. I'm talking about what kind of society do we want to create? What do we want it to mean to be human in the age of AI, in the age of AGI? So if we can have this conversation, broad, inclusive conversation, and gradually start converging towards some, some future that with some direction, at least, that we want to steer towards, right, then we'll be much more motivated to constructively take on the obstacles. And I think if I had, if I had to, if I try to wrap this up in a more succinct way, I think we can all agree already now that we should aspire to build AGI that doesn't overpower us, but that empowers us. And think of the many various ways that can do that, whether that's from my side of the world of autonomous vehicles. I'm personally actually from the camp that believes this human level intelligence is required to achieve something like vehicles that would actually be something we would enjoy using and being part of. So that's one example, and certainly there's a lot of other types of robots and medicine and so on. So focusing on those and then coming up with the obstacles, coming up with the ways that that can go wrong and solving those one at a time. And just because you can build an autonomous vehicle, even if you could build one that would drive just fine without you, maybe there are some things in life that we would actually want to do ourselves. That's right. Right, like, for example, if you think of our society as a whole, there are some things that we find very meaningful to do. And that doesn't mean we have to stop doing them just because machines can do them better. I'm not gonna stop playing tennis just the day someone builds a tennis robot and beat me. People are still playing chess and even go. Yeah, and in the very near term even, some people are advocating basic income, replace jobs. But if the government is gonna be willing to just hand out cash to people for doing nothing, then one should also seriously consider whether the government should also hire a lot more teachers and nurses and the kind of jobs which people often find great fulfillment in doing, right? We get very tired of hearing politicians saying, oh, we can't afford hiring more teachers, but we're gonna maybe have basic income. If we can have more serious research and thought into what gives meaning to our lives, the jobs give so much more than income, right? Mm hmm. And then think about in the future, what are the roles that we wanna have people continually feeling empowered by machines? And I think sort of, I come from Russia, from the Soviet Union. And I think for a lot of people in the 20th century, going to the moon, going to space was an inspiring thing. I feel like the universe of the mind, so AI, understanding, creating intelligence is that for the 21st century. So it's really surprising. And I've heard you mention this. It's really surprising to me, both on the research funding side, that it's not funded as greatly as it could be, but most importantly, on the politician side, that it's not part of the public discourse except in the killer bots terminator kind of view, that people are not yet, I think, perhaps excited by the possible positive future that we can build together. So we should be, because politicians usually just focus on the next election cycle, right? The single most important thing I feel we humans have learned in the entire history of science is they were the masters of underestimation. We underestimated the size of our cosmos again and again, realizing that everything we thought existed was just a small part of something grander, right? Planet, solar system, the galaxy, clusters of galaxies. The universe. And we now know that the future has just so much more potential than our ancestors could ever have dreamt of. This cosmos, imagine if all of Earth was completely devoid of life, except for Cambridge, Massachusetts. Wouldn't it be kind of lame if all we ever aspired to was to stay in Cambridge, Massachusetts forever and then go extinct in one week, even though Earth was gonna continue on for longer? That sort of attitude I think we have now on the cosmic scale, life can flourish on Earth, not for four years, but for billions of years. I can even tell you about how to move it out of harm's way when the sun gets too hot. And then we have so much more resources out here, which today, maybe there are a lot of other planets with bacteria or cow like life on them, but most of this, all this opportunity seems, as far as we can tell, to be largely dead, like the Sahara Desert. And yet we have the opportunity to help life flourish around this for billions of years. So let's quit squabbling about whether some little border should be drawn one mile to the left or right, and look up into the skies and realize, hey, we can do such incredible things. Yeah, and that's, I think, why it's really exciting that you and others are connected with some of the work Elon Musk is doing, because he's literally going out into that space, really exploring our universe, and it's wonderful. That is exactly why Elon Musk is so misunderstood, right? Misconstrued him as some kind of pessimistic doomsayer. The reason he cares so much about AI safety is because he more than almost anyone else appreciates these amazing opportunities that we'll squander if we wipe out here on Earth. We're not just going to wipe out the next generation, all generations, and this incredible opportunity that's out there, and that would really be a waste. And AI, for people who think that it would be better to do without technology, let me just mention that if we don't improve our technology, the question isn't whether humanity is going to go extinct. The question is just whether we're going to get taken out by the next big asteroid or the next super volcano or something else dumb that we could easily prevent with more tech, right? And if we want life to flourish throughout the cosmos, AI is the key to it. As I mentioned in a lot of detail in my book right there, even many of the most inspired sci fi writers, I feel have totally underestimated the opportunities for space travel, especially at the other galaxies, because they weren't thinking about the possibility of AGI, which just makes it so much easier. Right, yeah. So that goes to your view of AGI that enables our progress, that enables a better life. So that's a beautiful way to put it and then something to strive for. So Max, thank you so much. Thank you for your time today. It's been awesome. Thank you so much. Thanks. Have a great day.\",\n",
              " 'title': 'Max Tegmark: Life 3.0 | Lex Fridman Podcast #1'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "58999edc-3464-4fe1-8e02-f0248ac6ad48",
      "metadata": {
        "id": "58999edc-3464-4fe1-8e02-f0248ac6ad48",
        "outputId": "54657d7d-2f6b-4bca-aa9b-7676dfa98328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            captions  \\\n",
              "0   As part of MIT course 6S099, Artificial Gener...   \n",
              "1   As part of MIT course 6S099 on artificial gen...   \n",
              "2   You've studied the human mind, cognition, lan...   \n",
              "3   What difference between biological neural net...   \n",
              "4   The following is a conversation with Vladimir...   \n",
              "5   The following is a conversation with Guido va...   \n",
              "6   The following is a conversation with Jeff Atw...   \n",
              "7   The following is a conversation with Eric Sch...   \n",
              "8   The following is a conversation with Stuart R...   \n",
              "9   The following is a conversation with Peter Ab...   \n",
              "\n",
              "                                               title  \n",
              "0     Max Tegmark: Life 3.0 | Lex Fridman Podcast #1  \n",
              "1  Christof Koch: Consciousness | Lex Fridman Pod...  \n",
              "2  Steven Pinker: AI in the Age of Reason | Lex F...  \n",
              "3  Yoshua Bengio: Deep Learning | Lex Fridman Pod...  \n",
              "4  Vladimir Vapnik: Statistical Learning | Lex Fr...  \n",
              "5  Guido van Rossum: Python | Lex Fridman Podcast #6  \n",
              "6  Jeff Atwood: Stack Overflow and Coding Horror ...  \n",
              "7      Eric Schmidt: Google | Lex Fridman Podcast #8  \n",
              "8  Stuart Russell: Long-Term Future of Artificial...  \n",
              "9  Pieter Abbeel: Deep Reinforcement Learning | L...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b4be36c-1421-405f-8957-0698477b305f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>captions</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As part of MIT course 6S099, Artificial Gener...</td>\n",
              "      <td>Max Tegmark: Life 3.0 | Lex Fridman Podcast #1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>As part of MIT course 6S099 on artificial gen...</td>\n",
              "      <td>Christof Koch: Consciousness | Lex Fridman Pod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You've studied the human mind, cognition, lan...</td>\n",
              "      <td>Steven Pinker: AI in the Age of Reason | Lex F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What difference between biological neural net...</td>\n",
              "      <td>Yoshua Bengio: Deep Learning | Lex Fridman Pod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The following is a conversation with Vladimir...</td>\n",
              "      <td>Vladimir Vapnik: Statistical Learning | Lex Fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The following is a conversation with Guido va...</td>\n",
              "      <td>Guido van Rossum: Python | Lex Fridman Podcast #6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The following is a conversation with Jeff Atw...</td>\n",
              "      <td>Jeff Atwood: Stack Overflow and Coding Horror ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The following is a conversation with Eric Sch...</td>\n",
              "      <td>Eric Schmidt: Google | Lex Fridman Podcast #8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>The following is a conversation with Stuart R...</td>\n",
              "      <td>Stuart Russell: Long-Term Future of Artificial...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>The following is a conversation with Peter Ab...</td>\n",
              "      <td>Pieter Abbeel: Deep Reinforcement Learning | L...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b4be36c-1421-405f-8957-0698477b305f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6b4be36c-1421-405f-8957-0698477b305f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6b4be36c-1421-405f-8957-0698477b305f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ea2fb9f8-f824-4ee1-8ad4-9a99515cf792\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ea2fb9f8-f824-4ee1-8ad4-9a99515cf792')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ea2fb9f8-f824-4ee1-8ad4-9a99515cf792 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 319,\n  \"fields\": [\n    {\n      \"column\": \"captions\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 319,\n        \"samples\": [\n          \" The following is a conversation with Michael I. Jordan, a professor at Berkeley and one of the most influential people in the history of machine learning, statistics, and artificial intelligence. He has been cited over 170,000 times and he has mentored many of the world class researchers defining the field of AI today, including Andrew Ng, Zubin Garamani, Ben Taskar, and Yoshua Bengio. All this, to me, is as impressive as the over 32,000 points in the six NBA championships of the Michael J. Jordan of basketball fame. There's a nonzero probability that I talked to the other Michael Jordan given my connection to and love of the Chicago Bulls of the 90s, but if I had to pick one, I'm going with the Michael Jordan of statistics and computer science, or as Yann LeCun calls him, the Miles Davis of machine learning. In his blog post titled Artificial Intelligence, the Revolution Hasn't Happened Yet, Michael argues for broadening the scope of the artificial intelligence field. In many ways, the underlying spirit of this podcast is the same, to see artificial intelligence as a deeply human endeavor, to not only engineer algorithms and robots, but to understand and empower human beings at all levels of abstraction, from the individual to our civilization as a whole. This is the Artificial Intelligence Podcast. If you enjoy it, subscribe and YouTube, give it five stars at Apple Podcast, support it on Patreon, or simply connect with me on Twitter at Lex Friedman spelled F R I D M A N. As usual, I'll do one or two minutes of ads now and never any ads in the middle that can break the flow of the conversation. I hope that works for you and doesn't hurt the listening experience. This show is presented by Cash App, the number one finance app in the App Store. When you get it, use code LEX PODCAST. Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with as little as $1. Since Cash App does fractional share trading, let me mention that the order execution algorithm that worked behind the scenes to create the abstraction of the fractional orders is to me an algorithmic marvel. Great props for the Cash App engineers for solving a hard problem that in the end provides an easy interface that takes a step up to the next layer of abstraction over the stock market, making trading more accessible for new investors and diversification much easier. So once again, if you get Cash App from the App Store or Google Play and use the code LEX PODCAST, you'll get $10 and Cash App will also donate $10 to First, one of my favorite organizations that is helping to advance robotics and STEM education for young people around the world. And now, here's my conversation with Michael I. Jordan. Given that you're one of the greats in the field of AI, machine learning, computer science, and so on, you're trivially called the Michael Jordan of machine learning, although as you know, you were born first, so technically MJ is the Michael I. Jordan of basketball. But anyway, my favorite is Yann LeCun calling you the Miles Davis of machine learning because as he says, you reinvent yourself periodically and sometimes leave fans scratching their heads after you change direction. So can you put at first your historian hat on and give a history of computer science and AI as you saw it, as you experienced it, including the four generations of AI successes that I've seen you talk about? Sure. Yeah, first of all, I much prefer Yann's metaphor. Miles Davis was a real explorer in jazz and he had a coherent story. So I think I have one, but it's not just the one you lived, it's the one you think about later. What the historian does is they look back and they revisit. I think what's happening right now is not AI, that was an intellectual aspiration that's still alive today as an aspiration. But I think this is akin to the development of chemical engineering from chemistry or electrical engineering from electromagnetism. So if you go back to the 30s or 40s, there wasn't yet chemical engineering. There was chemistry, there was fluid flow, there was mechanics and so on. But people pretty clearly viewed interesting goals to try to build factories that make chemicals products and do it viably, safely, make good ones, do it at scale. So people started to try to do that, of course, and some factories worked, some didn't, some were not viable, some exploded, but in parallel, developed a whole field called chemical engineering. Electrical engineering is a field, it's no bones about it, it has theoretical aspects to it, it has practical aspects. It's not just engineering, quote unquote, it's the real thing, real concepts are needed. Same thing with electrical engineering. There was Maxwell's equations, which in some sense were everything you know about electromagnetism, but you needed to figure out how to build circuits, how to build modules, how to put them together, how to bring electricity from one point to another safely and so on and so forth. So a whole field that developed called electrical engineering. I think that's what's happening right now, is that we have a proto field, which is statistics, more of the theoretical side of it, algorithmic side of computer science, that was enough to start to build things, but what things? Systems that bring value to human beings and use human data and mix in human decisions. The engineering side of that is all ad hoc. That's what's emerging. In fact, if you wanna call machine learning a field, I think that's what it is, that it's a proto form of engineering based on statistical and computational ideas of previous generations. But do you think there's something deeper about AI in his dreams and aspirations as compared to chemical engineering and electrical engineering? Well the dreams and aspirations maybe, but those are 500 years from now. I think that that's like the Greeks sitting there and saying, it would be neat to get to the moon someday. I think we have no clue how the brain does computation. We're just a clueless. We're even worse than the Greeks on most anything interesting scientifically of our era. Can you linger on that just for a moment because you stand not completely unique, but a little bit unique in the clarity of that. Can you elaborate your intuition of why we're, like where we stand in our understanding of the human brain? And a lot of people say, you know, scientists say we're not very far in understanding human brain, but you're like, you're saying we're in the dark here. Well, I know I'm not unique. I don't even think in the clarity, but if you talk to real neuroscientists that really study real synapses or real neurons, they agree, they agree. It's a hundreds of year task and they're building it up slowly and surely. What the signal is there is not clear. We think we have all of our metaphors. We think it's electrical, maybe it's chemical, it's a whole soup, it's ions and proteins and it's a cell. And that's even around like a single synapse. If you look at a electron micrograph of a single synapse, it's a city of its own. And that's one little thing on a dendritic tree, which is extremely complicated electrochemical thing. And it's doing these spikes and voltages are flying around and then proteins are taking that and taking it down into the DNA and who knows what. So it is the problem of the next few centuries. It is fantastic. But we have our metaphors about it. Is it an economic device? Is it like the immune system or is it like a layered set of, you know, arithmetic computations? We have all these metaphors and they're fun. But that's not real science per se. There is neuroscience. That's not neuroscience. All right. That's like the Greek speculating about how to get to the moon, fun, right? And I think that I like to say this fairly strongly because I think a lot of young people think we're on the verge because a lot of people who don't talk about it clearly let it be understood that, yes, we kind of, this is a brain inspired, we're kind of close, you know, breakthroughs are on the horizon. And that's scrupulous people sometimes who need money for their labs. That's what I'm saying, scrupulous, but people will oversell, I need money for my lab, I'm studying computational neuroscience, I'm going to oversell it. And so there's been too much of that. So I'll step into the gray area between metaphor and engineering with, I'm not sure if you're familiar with brain computer interfaces. So a company like Elon Musk has Neuralink that's working on putting electrodes into the brain and trying to be able to read, both read and send electrical signals. Just as you said, even the basic mechanism of communication in the brain is not something we understand. But do you hope without understanding the fundamental principles of how the brain works, we'll be able to do something interesting at that gray area of metaphor? It's not my area. So I hope in the sense, like anybody else hopes for some interesting things to happen from research, I would expect more something like Alzheimer's will get figured out from modern neuroscience. There's a lot of human suffering based on brain disease and we throw things like lithium at the brain, it kind of works, no one has a clue why. That's not quite true, but mostly we don't know. And that's even just about the biochemistry of the brain and how it leads to mood swings and so on. How thought emerges from that, we were really, really completely dim. So that you might want to hook up electrodes and try to do some signal processing on that and try to find patterns, fine, by all means, go for it. It's just not scientific at this point. So it's like kind of sitting in a satellite and watching the emissions from a city and trying to infer things about the microeconomy, even though you don't have microeconomic concepts. It's really that kind of thing. And so yes, can you find some signals that do something interesting or useful? Can you control a cursor or mouse with your brain? Yeah, absolutely, and then I can imagine business models based on that and even medical applications of that. But from there to understanding algorithms that allow us to really tie in deeply from the brain to computer, I just, no, I don't agree with Elon Musk. I don't think that's even, that's not for our generations, not even for the century. So just in hopes of getting you to dream, you've mentioned Kolmogorov and Turing might pop up, do you think that there might be breakthroughs that will get you to sit back in five, 10 years and say, wow? Oh, I'm sure there will be, but I don't think that there'll be demos that impress me. I don't think that having a computer call a restaurant and pretend to be a human is a breakthrough. Right. And people, you know, some people present it as such. It's imitating human intelligence. It's even putting coughs in the thing to make a bit of a PR stunt. And so fine that the world runs on those things too. And I don't want to diminish all the hard work and engineering that goes behind things like that and the ultimate value to the human race. But that's not scientific understanding. And I know the people that work on these things, they are after scientific understanding. In the meantime, they've got to kind of, you know, the trains got to run and they got mouths to feed and they got things to do and there's nothing wrong with all that. I would call that though, just engineering. And I want to distinguish that between an engineering field, like electrical engineering and chemical engineering that originally emerged, that had real principles and you really know what you're doing and you had a little scientific understanding, maybe not even complete. So it became more predictable and it really gave value to human life because it was understood. And so we don't want to muddle too much these waters of, you know, what we're able to do versus what we really can't do in a way that's going to impress the next. So I don't need to be wowed, but I think that someone comes along in 20 years, a younger person who's absorbed all the technology and for them to be wowed, I think they have to be more deeply impressed. A young Kolmogorov would not be wowed by some of the stunts that you see right now coming from the big companies. The demos, but do you think the breakthroughs from Kolmogorov would be, and give this question a chance, do you think there'll be in the scientific fundamental principles arena or do you think it's possible to have fundamental breakthroughs in engineering? Meaning, you know, I would say some of the things that Elon Musk is working with SpaceX and then others sort of trying to revolutionize the fundamentals of engineering, of manufacturing, of saying, here's a problem we know how to do a demo of and actually taking it to scale. Yeah. So there's going to be all kinds of breakthroughs. I just don't like that terminology. I'm a scientist and I work on things day in and day out and things move along and eventually you say, wow, something happened, but I don't like that language very much. Also I don't like to prize theoretical breakthroughs over practical ones. I tend to be more of a theoretician and I think there's lots to do in that arena right now. And so I wouldn't point to the Kolmogorovs, I might point to the Edisons of the era and maybe Musk is a bit more like that. But you know, Musk, God bless him, also will say things about AI that he knows very little about and he leads people astray when he talks about things he doesn't know anything about. Trying to program a computer to understand natural language, to be involved in a dialogue we're having right now, that ain't going to happen in our lifetime. You could fake it, you can mimic, sort of take old sentences that humans use and retread them, but the deep understanding of language, no, it's not going to happen. And so from that, I hope you can perceive that the deeper, yet deeper kind of aspects and intelligence are not going to happen. Now will there be breakthroughs? No, I think that Google was a breakthrough, I think Amazon is a breakthrough, you know, I think Uber is a breakthrough, you know, that bring value to human beings at scale in new, brand new ways based on data flows and so on. A lot of these things are slightly broken because there's not kind of an engineering field that takes economic value in context of data and, you know, planetary scale and worries about all the externalities, the privacy, you know, we don't have that field so we don't think these things through very well. I see that as emerging and that will be, you know, looking back from 100 years, that will be a constituted breakthrough in this era, just like electrical engineering was a breakthrough in the early part of the last century and chemical engineering was a breakthrough. So the scale, the markets that you talk about and we'll get to will be seen as sort of breakthrough and we're in the very early days of really doing interesting stuff there and we'll get to that, but just taking a quick step back, can you give, kind of throw off the historian hat. I mean, you briefly said that the history of AI kind of mimics the history of chemical engineering, but... I keep saying machine learning. You keep wanting to say AI, just to let you know, I don't, you know, I resist that. I don't think this is about AI really was John McCarthy as almost a philosopher saying, wouldn't it be cool if we could put thought in a computer? If we could mimic the human capability to think or put intelligence in, in some sense into a computer. That's an interesting philosophical question and he wanted to make it more than philosophy. He wanted to actually write down a logical formula and algorithms that would do that. And that is a perfectly valid, reasonable thing to do. That's not what's happening in this era. So the reason I keep saying AI actually, and I'd love to hear what you think about it. Machine learning has a very particular set of methods and tools. Maybe your version of it is that mine doesn't, it's very, very open. It does optimization, it does sampling, it does... So systems that learn is what machine learning is. Systems that learn and make decisions. And make decisions. So it's not just pattern recognition and, you know, finding patterns, it's all about making decisions in real worlds and having close feedback loops. So something like symbolic AI, expert systems, reasoning systems, knowledge based representation, all of those kinds of things, search, does that neighbor fit into what you think of as machine learning? So I don't even like the word machine learning, I think that what the field you're talking about is all about making large collections of decisions under uncertainty by large collections of entities. Right? And there are principles for that, at that scale. You don't have to say the principles are for a single entity that's making decisions, single agent or single human. It really immediately goes to the network of decisions. Is a good word for that or no? No, there's no good words for any of this. That's kind of part of the problem. So we can continue the conversation to use AI for all that. I just want to kind of raise the flag here that this is not about, we don't know what intelligence is and real intelligence. We don't know much about abstraction and reasoning at the level of humans. We don't have a clue. We're not trying to build that because we don't have a clue. Eventually it may emerge. They'll make, I don't know if there'll be breakthroughs, but eventually we'll start to get glimmers of that. It's not what's happening right now. Okay. We're taking data. We're trying to make good decisions based on that. We're trying to scale. We're trying to economically viably, we're trying to build markets. We're trying to keep value at that scale and aspects of this will look intelligent. Computers were so dumb before, they will seem more intelligent. We will use that buzzword of intelligence so we can use it in that sense. So machine learning, you can scope it narrowly as just learning from data and pattern recognition. But when I talk about these topics, maybe data science is another word you could throw in the mix, it really is important that the decisions are as part of it. It's consequential decisions in the real world. Am I going to have a medical operation? Am I going to drive down the street? Things where there's scarcity, things that impact other human beings or other environments and so on. How do I do that based on data? How do I do that adaptively? How do I use computers to help those kinds of things go forward? Whatever you want to call that. So let's call it AI. Let's agree to call it AI, but let's not say that the goal of that is intelligence. The goal of that is really good working systems at planetary scale that we've never seen before. So reclaim the word AI from the Dartmouth conference from many decades ago of the dream of humans. I don't want to reclaim it. I want a new word. I think it was a bad choice. I mean, if you read one of my little things, the history was basically that McCarthy needed a new name because cybernetics already existed and he didn't like, no one really liked Norbert Wiener. Norbert Wiener was kind of an island to himself and he felt that he had encompassed all this and in some sense he did. You look at the language of cybernetics, it was everything we're talking about. It was control theory and signal processing and some notions of intelligence and closed feedback loops and data. It was all there. It's just not a word that lived on partly because of the maybe the personalities. But McCarthy needed a new word to say, I'm different from you. I'm not part of your show. I got my own. Invented this word and again, thinking forward about the movies that would be made about it, it was a great choice. But thinking forward about creating a sober academic and real world discipline, it was a terrible choice because it led to promises that are not true that we understand. We understand artificial perhaps, but we don't understand intelligence. It's a small tangent because you're one of the great personalities of machine learning, whatever the heck you call the field. Do you think science progresses by personalities or by the fundamental principles and theories and research that's outside of personalities? Both. And I wouldn't say there should be one kind of personality. I have mine and I have my preferences and I have a kind of network around me that feeds me and some of them agree with me and some of them disagree, but all kinds of personalities are needed. Right now, I think the personality that it's a little too exuberant, a little bit too ready to promise the moon is a little bit too much in ascendance. And I do think that there's some good to that. It certainly attracts lots of young people to our field, but a lot of those people come in with strong misconceptions and they have to then unlearn those and then find something to do. And so I think there's just got to be some multiple voices and I wasn't hearing enough of the more sober voice. So as a continuation of a fun tangent and speaking of vibrant personalities, what would you say is the most interesting disagreement you have with Jan Lacune? So Jan's an old friend and I just say that I don't think we disagree about very much really. He and I both kind of have a let's build it kind of mentality and does it work kind of mentality and kind of concrete. We both speak French and we speak French more together and we have a lot in common. And so if one wanted to highlight a disagreement, it's not really a fundamental one. I think it's just kind of what we're emphasizing. Jan has emphasized pattern recognition and has emphasized prediction. And it's interesting to try to take that as far as you can. If you could do perfect prediction, what would that give you kind of as a thought experiment? And I think that's way too limited. We cannot do perfect prediction. We will never have the data sets that allow me to figure out what you're about ready to do, what question you're going to ask next. I have no clue. I will never know such things. Moreover, most of us find ourselves during the day in all kinds of situations we had no anticipation of that are kind of very, very novel in various ways. And in that moment, we want to think through what we want. And also there's going to be market forces acting on us. I'd like to go down that street, but now it's full because there's a crane in the street. I got it. I got to think about that. I got to think about what I might really want here. And I got to sort of think about how much it costs me to do this action versus this action. I got to think about the risks involved. A lot of our current pattern recognition and prediction systems don't do any risk evaluations. They have no error bars, right? I got to think about other people's decisions around me. I got to think about a collection of my decisions, even just thinking about like a medical treatment, you know, I'm not going to take a, the prediction of a neural net about my health, about something consequential. I'm not about ready to have a heart attack because some number is over 0.7. Even if you had all the data in the world that ever been collected about heart attacks better than any doctor ever had, I'm not going to trust the output of that neural net to predict my heart attack. I'm going to want to ask what if questions around that. I'm going to want to look at some us or other possible data I didn't have, causal things. I'm going to want to have a dialogue with a doctor about things we didn't think about when he gathered the data. You know, I could go on and on. I hope you can see. And I don't, I think that if you say predictions, everything that, that, that you're missing all of this stuff. And so prediction plus decision making is everything, but both of them are equally important. And so the field has emphasized prediction, Jan rightly so has seen how powerful that is. But at the cost of people not being aware that decision making is where the rubber really hits the road, where human lives are at stake, where risks are being taken, where you got to gather more data. You got to think about the error bars. You got to think about the consequences of your decisions on others. You got to think about the economy around your decisions, blah, blah, blah, blah. I'm not the only one working on those, but we're a smaller tribe. And right now we're not the one that people talk about the most. But you know, if you go out in the real world and industry, you know, at Amazon, I'd say half the people there are working on decision making and the other half are doing, you know, the pattern recognition. It's important. And the words of pattern recognition and prediction, I think the distinction there, not to linger on words, but the distinction there is more a constrained sort of in the lab data set versus decision making is talking about consequential decisions in the real world, under the messiness and the uncertainty of the real world. And just the whole of it, the whole mess of it that actually touches human beings and scale. And the forces, that's the distinction. It helps add those, that perspective, that broader perspective. You're right. I totally agree. On the other hand, if you're a real prediction person, of course, you want it to be in the real world. You want to predict real world events. I'm just saying that's not possible with just data sets. That it has to be in the context of, you know, strategic things that someone's doing, data they might gather, things they could have gathered, the reasoning process around data. It's not just taking data and making predictions based on the data. So one of the things that you're working on, I'm sure there's others working on it, but I don't hear often it talked about, especially in the clarity that you talk about it, and I think it's both the most exciting and the most concerning area of AI in terms of decision making. So you've talked about AI systems that help make decisions that scale in a distributed way, millions, billions decisions, sort of markets of decisions. Can you, as a starting point, sort of give an example of a system that you think about when you're thinking about these kinds of systems? Yeah, so first of all, you're absolutely getting into some territory, which I will be beyond my expertise. And there are lots of things that are going to be very not obvious to think about. Just like, again, I like to think about history a little bit, but think about put yourself back in the sixties. There was kind of a banking system that wasn't computerized really. There was database theory emerging and database people had to think about how do I actually not just move data around, but actual money and have it be, you know, valid and have transactions that ATMs happen that are actually, you know, all valid and so on and so forth. So that's the kind of issues you get into when you start to get serious about sorts of things like this. I like to think about as kind of almost a thought experiment to help me think something simpler, which is the music market. And because there is, to first order, there is no music market in the world right now and in our country, for sure. There are something called things called record companies and they make money and they prop up a few really good musicians and make them superstars and they all make huge amounts of money. But there's a long tail of huge numbers of people that make lots and lots of really good music that is actually listened to by more people than the famous people. They are not in a market. They cannot have a career. They do not make money. The creators, the creators, the creators, the so called influencers or whatever that diminishes who they are. So there are people who make extremely good music, especially in the hip hop or Latin world these days. They do it on their laptop. That's what they do on the weekend and they have another job during the week and they put it up on SoundCloud or other sites. Eventually it gets streamed. It now gets turned into bits. It's not economically valuable. The information is lost. It gets put up there. People stream it. You walk around in a big city, you see people with headphones, especially young kids listening to music all the time. If you look at the data, very little of the music they are listening to is the famous people's music and none of it's old music. It's all the latest stuff. But the people who made that latest stuff are like some 16 year old somewhere who will never make a career out of this, who will never make money. Of course there will be a few counter examples. The record companies incentivize to pick out a few and highlight them. Long story short, there's a missing market there. There is not a consumer producer relationship at the level of the actual creative acts. The pipelines and Spotify's of the world that take this stuff and stream it along, they make money off of subscriptions or advertising and those things. They're making the money. All right. And then they will offer bits and pieces of it to a few people again to highlight that they simulate a market. Anyway, a real market would be if you're a creator of music that you actually are somebody who's good enough that people want to listen to you, you should have the data available to you. There should be a dashboard showing a map of the United States. So in last week, here's all the places your songs were listened to. It should be transparent, vetable, so that if someone down in Providence sees that you're being listened to 10,000 times in Providence, that they know that's real data. You know it's real data. They will have you come give a show down there. They will broadcast to the people who've been listening to you that you're coming. If you do this right, you could go down there and make $20,000. You do that three times a year, you start to have a career. So in this sense, AI creates jobs. It's not about taking away human jobs. It's creating new jobs because it creates a new market. Once you've created a market, you've now connected up producers and consumers. The person who's making the music can say to someone who comes to their shows a lot, hey, I'll play at your daughter's wedding for $10,000. You'll say 8,000. They'll say 9,000. Then again, you can now get an income up to $100,000. You're not going to be a millionaire. And now even think about really the value of music is in these personal connections, even so much so that a young kid wants to wear a tshirt with their favorite musician's signature on it. So if they listen to the music on the internet, the internet should be able to provide them with a button that they push and the merchandise arrives the next day. We can do that. And now why should we do that? Well, because the kid who bought the shirt will be happy, but more the person who made the music will get the money. There's no advertising needed. So you can create markets between producers and consumers, take 5% cut. Your company will be perfectly sound. It'll go forward into the future and it will create new markets and that raises human happiness. Now this seems like, well, this is easy, just create this dashboard, kind of create some connections and all that. But if you think about Uber or whatever, you think about the challenges in the real world of doing things like this, and there are actually new principles going to be needed. You're trying to create a new kind of two way market at a different scale that's ever been done before. There's going to be unwanted aspects of the market. There'll be bad people. There'll be the data will get used in the wrong ways, it'll fail in some ways, it won't deliver about. You have to think that through. Just like anyone who ran a big auction or ran a big matching service in economics will think these things through. And so that maybe doesn't get at all the huge issues that can arise when you start to create markets, but it starts to, at least for me, solidify my thoughts and allow me to move forward in my own thinking. Yeah. So I talked to the head of research at Spotify actually, and I think their longterm goal, they've said, is to have at least one million creators make a comfortable living putting on Spotify. So I think you articulate a really nice vision of the world and the digital and the cyberspace of markets. What do you think companies like Spotify or YouTube or Netflix can do to create such markets? Is it an AI problem? Is it an interface problem for interface design? Is it some other kind of, is it an economics problem? Who should they hire to solve these problems? Well, part of it's not just top down. So the Silicon Valley has this attitude that they know how to do it. They will create the system just like Google did with the search box that will be so good that they'll just, everyone will adopt that. It's everything you said, but really I think missing that kind of culture. So it's literally that 16 year old who's able to create the songs. You don't create that as a Silicon Valley entity. You don't hire them per se. You have to create an ecosystem in which they are wanted and that they belong. And so you have to have some cultural credibility to do things like this. Netflix, to their credit, wanted some of that credibility and they created shows, content. They call it content. It's such a terrible word, but it's culture. And so with movies, you can kind of go give a large sum of money to somebody graduating from the USC film school. It's a whole thing of its own, but it's kind of like rich white people's thing to do. And American culture has not been so much about rich white people. It's been about all the immigrants, all the Africans who came and brought that culture and those rhythms to this world and created this whole new thing. American culture. And so companies can't artificially create that. They can't just say, hey, we're here. We're going to buy it up. You've got a partner. And so anyway, not to denigrate, these companies are all trying and they should, and I'm sure they're asking these questions and some of them are even making an effort. But it is partly a respect the culture as a technology person. You've got to blend your technology with cultural meaning. How much of a role do you think the algorithm, so machine learning has in connecting the consumer to the creator, sort of the recommender system aspect of this? Yeah. It's a great question. I think pretty high. There's no magic in the algorithms, but a good recommender system is way better than a bad recommender system. And recommender systems is a billion dollar industry back even 10, 20 years ago. And it continues to be extremely important going forward. What's your favorite recommender system, just so we can put something, well, just historically I was one of the, when I first went to Amazon, I first didn't like Amazon because they put the book people out of business or the library, the local booksellers went out of business. I've come to accept that there probably are more books being sold now and poor people reading them than ever before. And then local book stores are coming back. So that's how economics sometimes work. You go up and you go down. But anyway, when I finally started going there and I bought a few books, I was really pleased to see another few books being recommended to me that I never would have thought of. And I bought a bunch of them. So they obviously had a good business model. But I learned things and I still to this day kind of browse using that service. And I think lots of people get a lot, that is a good aspect of a recommendation system. I'm learning from my peers in an indirect way. And their algorithms are not meant to have them impose what we learn. It really is trying to find out what's in the data. It doesn't work so well for other kinds of entities, but that's just the complexity of human life. Like shirts, I'm not going to get recommendations on shirts, but that's interesting. If you try to recommend restaurants, it's hard. It's hard to do it at scale. But a blend of recommendation systems with other economic ideas, matchings and so on is really, really still very open research wise. And there's new companies that are going to emerge that do that well. What do you think is going to the messy, difficult land of say politics and things like that, that YouTube and Twitter have to deal with in terms of recommendation systems? Being able to suggest, I think Facebook just launched Facebook news. So recommend the kind of news that are most likely for you to be interesting. Do you think this is AI solvable, again, whatever term we want to use, do you think it's a solvable problem for machines or is it a deeply human problem that's unsolvable? So I don't even think about it at that level. I think that what's broken with some of these companies, it's all monetization by advertising. They're not, at least Facebook, I want to critique them, but they didn't really try to connect a producer and a consumer in an economic way, right? No one wants to pay for anything. And so they all, you know, starting with Google and Facebook, they went back to the playbook of, you know, the television companies back in the day. No one wanted to pay for this signal. They will pay for the TV box, but not for the signal, at least back in the day. And so advertising kind of filled that gap and advertising was new and interesting and it somehow didn't take over our lives quite, right? Fast forward, Google provides a service that people don't want to pay for. And so somewhat surprisingly in the nineties, they made, they ended up making huge amounts so they cornered the advertising market. It didn't seem like that was going to happen, at least to me. These little things on the right hand side of the screen just did not seem all that economically interesting, but that companies had maybe no other choice. The TV market was going away and billboards and so on. So they've, they got it. And I think that sadly that Google just has, it was doing so well with that at making such money. They didn't think much more about how, wait a minute, is there a producer consumer relationship to be set up here? Not just between us and the advertisers market to be created. Is there an actual market between the producer consumer? They're the producers, the person who created that video clip, the person that made that website, the person who could make more such things, the person who could adjust it as a function of demand, the person on the other side who's asking for different kinds of things, you know? So you see glimmers of that now there's influencers and there's kind of a little glimmering of a market, but it should have been done 20 years ago. It should have been thought about. It should have been created in parallel with the advertising ecosystem. And then Facebook inherited that. And I think they also didn't think very much about that. So fast forward and now they are making huge amounts of money off of advertising. And the news thing and all these clicks is just feeding the advertising. It's all connected up to the advertiser. So you want more people to click on certain things because that money flows to you, Facebook. You're very much incentivized to do that. And when you start to find it's breaking, people are telling you, well, we're getting into some troubles. You try to adjust it with your smart AI algorithms, right? And figure out what are bad clicks. So maybe it shouldn't be click through rate, it should be something else. I find that pretty much hopeless. It does get into all the complexity of human life and you can try to fix it. You should, but you could also fix the whole business model. And the business model is that really, what are, are there some human producers and consumers out there? Is there some economic value to be liberated by connecting them directly? Is it such that it's so valuable that people will be able to pay for it? All right. And micro payments, like small payments. Micro, but even have to be micro. So I like the example, suppose I'm going, next week I'm going to India. Never been to India before. Right? I have a couple of days in Mumbai, I have no idea what to do there. Right? And I could go on the web right now and search. It's going to be kind of hopeless. I'm not going to find, you know, I have lots of advertisers in my face. Right? What I really want to do is broadcast to the world that I am going to Mumbai and have someone on the other side of a market look at me and, and there's a recommendation system there. So I'm not looking at all possible people coming to Mumbai. They're looking at the people who are relevant to them. So someone in my age group, someone who kind of knows me in some level, I give up a little privacy by that, but I'm happy because what I'm going to get back is this person can make a little video for me, or they're going to write a little two page paper on here's the cool things that you want to do and move by this week, especially, right? I'm going to look at that. I'm not going to pay a micro payment. I'm going to pay, you know, a hundred dollars or whatever for that. It's real value. It's like journalism. Um, and as an honest subscription, it's that I'm going to pay that person in that moment. Company's going to take 5% of that. And that person has now got it. It's a gig economy, if you will, but you know, done for it, you know, thinking about a little bit behind YouTube, there was actually people who could make more of those things. If they were connected to a market, they would make more of those things independently. You don't have to tell them what to do. You don't have to incentivize them any other way. Um, and so, yeah, these companies, I don't think have thought long and hard about that. So I do distinguish on Facebook on the one side, who just not thought about these things at all. I think, uh, thinking that AI will fix everything, uh, and Amazon thinks about them all the time because they were already out in the real world. They were delivering packages, people's doors. They were, they were worried about a market. They were worried about sellers and, you know, they worry and some things they do are great. Some things maybe not so great, but you know, they're in that business model. And then I'd say Google sort of hovers somewhere in between. I don't, I don't think for a long, long time they got it. I think they probably see that YouTube is more pregnant with possibility than, than, than they might've thought and that they're probably heading that direction. Um, but uh, you know, Silicon Valley has been dominated by the Google Facebook kind of mentality and the subscription and advertising and that is, that's the core problem, right? The fake news actually rides on top of that because it means that you're monetizing with clip through rate and that is the core problem. You got to remove that. So advertisement, if we're going to linger on that, I mean, that's an interesting thesis. I don't know if everyone really deeply thinks about that. So you're right. The thought is the advertising model is the only thing we have, the only thing we'll ever have. We have to fix, we have to build algorithms that despite that business model, you know, find the better angels of our nature and do good by society and by the individual. But you think we can slowly, you think, first of all, there's a difference between should and could. So you're saying we should slowly move away from the advertising model and have a direct connection between the consumer and the creator. The question I also have is, can we, because the advertising model is so successful now in terms of just making a huge amount of money and therefore being able to build a big company that provides, has really smart people working that create a good service. Do you think it's possible? And just to clarify, you think we should move away? Well, I think we should. Yeah. But we is the, you know, me. So society. Yeah. Well, the companies, I mean, so first of all, full disclosure, I'm doing a day a week at Amazon because I kind of want to learn more about how they do things. So, you know, I'm not speaking for Amazon in any way, but, you know, I did go there because I actually believe they get a little bit of this or trying to create these markets. And they don't really use, advertising is not a crucial part of it. Well, that's a good question. So it has become not crucial, but it's become more and more present if you go to Amazon website. And, you know, without revealing too many deep secrets about Amazon, I can tell you that, you know, a lot of people in the company question this and there's a huge questioning going on. You do not want a world where there's zero advertising. That actually is a bad world. Okay. So here's a way to think about it. You're a company that like Amazon is trying to bring products to customers, right? And the customer, at any given moment, you want to buy a vacuum cleaner, say, you want to know what's available for me. And, you know, it's not going to be that obvious. You have to do a little bit of work at it. The recommendation system will sort of help, right? But now suppose this other person over here has just made the world, you know, they spent a huge amount of energy. They had a great idea. They made a great vacuum cleaner. They know they really did it. They nailed it. It's an MIT, you know, whiz kid that made a great new vacuum cleaner, right? It's not going to be in the recommendation system. No one will know about it. The algorithms will not find it and AI will not fix that. Okay. At all. Right. How do you allow that vacuum cleaner to start to get in front of people, be sold well advertising. And here, what advertising is, it's a signal that you're, you believe in your product enough that you're willing to pay some real money for it. And to me as a consumer, I look at that signal. I say, well, first of all, I know these are not just cheap little ads cause we have now right now there. I know that, you know, these are super cheap, you know, pennies. If I see an ad where it's actually, I know the company is only doing a few of these and they're making, you know, real money is kind of flowing and I see an ad, I may pay more attention to it. And I actually might want that because I see, Hey, that guy spent money on his vacuum cleaner. Maybe there's something good there. So I will look at it. And so that's part of the overall information flow in a good market. So advertising has a role, but the problem is of course that that signal is now completely gone because it just, you know, dominant by these tiny little things that add up to big money for the company, you know? So I think it will just, I think it will change because the societies just don't, you know, stick with things that annoy a lot of people and advertising currently annoys people more than it provides information. And I think that a Google probably is smart enough to figure out that this is a dead, this is a bad model, even though it's a hard, huge amount of money and they'll have to figure out how to pull it away from it slowly. And I'm sure the CEO there will figure it out, but they need to do it. And they needed it to, so if you reduce advertising, not to zero, but you reduce it at the same time you bring up producer, consumer, actual real value being delivered. So real money is being paid and they take a 5% cut that 5% could start to get big enough to cancel out the lost revenue from the kind of the poor kind of advertising. And I think that a good company will do that, will realize that. And Facebook, you know, again, God bless them. They bring, you know, grandmothers, they bring children's pictures into grandmothers lives. It's fantastic. But they need to think of a new business model and that's the core problem there. Until they start to connect producer consumer, I think they will just continue to make money and then buy the next social network company and then buy the next one and the innovation level will not be high and the health issues will not go away. So I apologize that we kind of returned to words, I don't think the exact terms matter, but in sort of defense of advertisement, don't you think the kind of direct connection between consumer and creator producer is what advertisement strives to do, right? So that is best advertisement is literally now Facebook is listening to our conversation and heard that you're going to India and will be able to actually start automatically for you making these connections and start giving this offer. So like, I apologize if it's just a matter of terms, but just to draw a distinction, is it possible to make advertisements just better and better and better algorithmically to where it actually becomes a connection, almost a direct connection? That's a good question. So let's component on that. First of all, what we just talked about, I was defending advertising. Okay. So I was defending it as a way to get signals into a market that don't come any other way, especially algorithmically. It's a sign that someone spent money on it, it's a sign they think it's valuable. And if I think that if other things, someone else thinks it's valuable, and if I trust other people, I might be willing to listen. I don't trust that Facebook though, who's an intermediary between this. I don't think they care about me. Okay. I don't think they do. And I find it creepy that they know I'm going to India next week because of our conversation. Why do you think that is? So what, could you just put your PR hat on? Why do you think you find Facebook creepy and not trust them as do majority of the population? So they're out of the Silicon Valley companies, I saw like not approval rate, but there's ranking of how much people trust companies and Facebook is in the gutter. In the gutter, including people inside of Facebook. So what do you attribute that to? Because when I... Come on, you don't find it creepy that right now we're talking that I might walk out on the street right now that some unknown person who I don't know kind of comes up to me and says, I hear you're going to India. I mean, that's not even Facebook. That's just, I want transparency in human society. I want to have, if you know something about me, there's actually some reason you know something about me. That's something that if I look at it later and audit it kind of, I approve. You know something about me because you care in some way. There's a caring relationship even, or an economic one or something. Not just that you're someone who could exploit it in ways I don't know about or care about or I'm troubled by or whatever. We're in a world right now where that happens way too much and that Facebook knows things about a lot of people and could exploit it and does exploit it at times. I think most people do find that creepy. It's not for them. It's not that Facebook is not doing it because they care about them in a real sense. And they shouldn't. They should not be a big brother caring about us. That is not the role of a company like that. Why not? Wait, not the big brother part, but the caring, the trusting. I mean, don't those companies, just to link on it because a lot of companies have a lot of information about us. I would argue that there's companies like Microsoft that has more information about us than Facebook does and yet we trust Microsoft more. Well, Microsoft is pivoting. Microsoft, you know, under Satya Nadella has decided this is really important. We don't want to do creepy things. Really want people to trust us to actually only use information in ways that they really would approve of, that we don't decide, right? And I'm just kind of adding that the health of a market is that when I connect to someone who produces a consumer, it's not just a random producer or consumer, it's people who see each other. They don't like each other, but they sense that if they transact, some happiness will go up on both sides. If a company helps me to do that in moments that I choose of my choosing, then fine. So, and also think about the difference between, you know, browsing versus buying, right? There are moments in my life I just want to buy, you know, a gadget or something. I need something for that moment. I need some ammonia for my house or something because I got a problem with a spill. I want to just go in. I don't want to be advertised at that moment. I don't want to be led down various, you know, that's annoying. I want to just go and have it be extremely easy to do what I want. Other moments I might say, no, it's like today I'm going to the shopping mall. I want to walk around and see things and see people and be exposed to stuff. So I want control over that though. I don't want the company's algorithms to decide for me, right? I think that's the thing. There's a total loss of control if Facebook thinks they should take the control from us of deciding when we want to have certain kinds of information, when we don't, what information that is, how much it relates to what they know about us that we didn't really want them to know about us. I don't want them to be helping me in that way. I don't want them to be helping them by they decide they have control over what I want and when. I totally agree. Facebook, by the way, I have this optimistic thing where I think Facebook has the kind of personal information about us that could create a beautiful thing. So I'm really optimistic of what Facebook could do. It's not what it's doing, but what it could do. So I don't see that. I think that optimism is misplaced because there's not a bit, you have to have a business model behind these things. Create a beautiful thing is really, let's be, let's be clear. It's about something that people would value. And I don't think they have that business model and I don't think they will suddenly discover it by what, you know, a long hot shower. I disagree. I disagree in terms of, you can discover a lot of amazing things in a shower. So I didn't say that. I said, they won't come, they won't do it, but in the shower, I think a lot of other people will discover it. I think that this guy, so I should also, full disclosure, there's a company called United Masters, which I'm on their board and they've created this music market and I have a hundred thousand artists now signed on and they've done things like gone to the NBA and the NBA, the music you find behind NBA clips right now is their music, right? That's a company that had the right business model in mind from the get go, right? Executed on that. And from day one, there was value brought to, so here you have a kid who made some songs who suddenly their songs are on the NBA website, right? That's real economic value to people. And so, you know, so you and I differ on the optimism of being able to sort of change the direction of the Titanic, right? So I, yeah, I'm older than you, so I've seen some Titanic's crash, got it. But and just to elaborate, cause I totally agree with you and I just want to know how difficult you think this problem is of, so for example, I want to read some news and I would, there's a lot of times in the day where something makes me either smile or think in a way where I like consciously think this really gave me value. Like I sometimes listen to the daily podcasts in the New York times, way better than the New York times themselves, by the way, for people listening. That's like real journalism is happening for some reason in the podcast space. It doesn't make sense to me, but often I listen to it 20 minutes and I would be willing to pay for that, like $5, $10 for that experience. And how difficult, that's kind of what you're getting at is that little transaction. How difficult is it to create a frictionless system like Uber has, for example, for other things? What's your intuition there? So I, first of all, I pay little bits of money to, you know, to send, there's something called courts that does financial things. I like medium as a site, I don't pay there, but I would. You had a great post on medium. I would have loved to pay you a dollar and not others. I wouldn't have wanted it per se because there should be also sites where that's not actually the goal. The goal is to actually have a broadcast channel that I monetize in some other way if I chose to. I mean, I could now people know about it. I could, I'm not doing it, but that's fine with me. Also the musicians who are making all this music, I don't think the right model is that you pay a little subscription fee to them, right? Because people can copy the bits too easily and it's just not that somewhere the value is. The value is that a connection was made between real human beings, then you can follow up on that. All right. And create yet more value. So no, I think there's a lot of open questions here, hot open questions, but also, yeah, I do want good recommendation systems that recommend cool stuff to me. But it's pretty hard, right? I don't like them to recommend stuff just based on my browsing history. I don't like the based on stuff they know about me, quote unquote. What's unknown about me is the most interesting. So this is the, this is the really interesting question. We may disagree, maybe not. I think that I love recommender systems and I want to give them everything about me in a way that I trust. Yeah. But you, but you don't, because, so for example, this morning I clicked on a, you know, I was pretty sleepy this morning. I clicked on a story about the queen of England. Yes. Right. I do not give a damn about the queen of England. I really do not. But it was clickbait. It kind of looked funny and I had to say, what the heck are they talking about? I don't want to have my life, you know, heading that direction. Now that's in my browsing history. The system in any reasonable system will think that I care about the queen of England. That's browsing history. Right. But, but you're saying all the trace, all the digital exhaust or whatever, that's been kind of the models. If you collect all this stuff, you're going to figure all of us out. Well, if you're trying to figure out like kind of one person like Trump or something, maybe you could figure him out. But if you're trying to figure out, you know, 500 million people, you know, no way, no way. You think so? No, I do. I think so. I think we are, humans are just amazingly rich and complicated. Every one of us has our little quirks, every one of us has our little things that could intrigue us that we don't even know it will intrigue us. And there's no sign of it in our past, but by God, there it comes and you know, you fall in love with it. And I don't want a company trying to figure that out for me and anticipate that I want them to provide a forum, a market, a place that I kind of go and by hook or by crook, this happens, you know, I I'm walking down the street and I hear some Chilean music being played and I never knew I liked Chilean music, but wow. So there is that side and I want them to provide a limited, but you know, interesting place to go. Right. And so don't try to use your AI to kind of, you know, figure me out and then put me in a world where you figured me out, you know, no, create huge spaces for human beings where our creativity and our style will be enriched and come forward and it'll be a lot of more transparency. I won't have people randomly, anonymously putting comments up and I'll special based on stuff they know about me, facts that, you know, we are so broken right now. If you're, you know, especially if you're a celebrity, but you know, it's about anybody that anonymous people are hurting lots and lots of people right now. That's part of this thing that Silicon Valley is thinking that, you know, just collect all this information and use it in a great way. So no, I'm not, I'm not a pessimist, I'm very much an optimist by nature, but I think that's just been the wrong path for the whole technology to take. Be more limited, create, let humans rise up. Don't try to replace them. That's the AI mantra. Don't try to anticipate them. Don't try to predict them because you're, you're, you're not going to, you're not going to be able to do those things. You're going to make things worse. Okay. So right now, just give this a chance. Right now, the recommender systems are the creepy people in the shadow watching your every move. So they're looking at traces of you. They're not directly interacting with you, sort of the, your close friends and family, the way they know you is by having conversation, by actually having interactions back and forth. Do you think there's a place for recommender systems sort of to step, cause you, you just emphasize the value of human to human connection, but yeah, just give it a chance, AI human connection. Is there a role for an AI system to have conversations with you in terms of, to try to figure out what kind of music you like, not by just watching what you listening to, but actually having a conversation, natural language or otherwise. Yeah, no, I'm, I'm, so I'm not against it. I just wanted to push back against the, maybe you're saying you have options for Facebook. So there I think it's misplaced, but, but I think that distributing, yeah, no, so good for you. Go for it. That's a hard spot to be in. Yeah, no, good. Human interaction, like on our daily, the context around me in my own home is something that I don't want some big company to know about at all, but I would be more than happy to have technology help me with it. Which kind of technology? Well, you know, just, Alexa, Amazon, well, a good, Alexa's done right. And I think Alexa is a research platform right now more than anything else. But Alexa done right, you know, could do things like I, I leave the water running in my garden and I say, Hey, Alexa, the water's running in my garden. And even have Alexa figure out that that means when my wife comes home, that she should be told about that. That's a little bit of a reasoning. I would call that AI and by any kind of stretch, it's a little bit of reasoning and it actually kind of would make my life a little easier and better. And you know, I don't, I wouldn't call this a wow moment, but I kind of think that overall rises human happiness up to have that kind of thing. But not when you're lonely, Alexa, knowing loneliness. No, no, I don't want Alexa to be, feel intrusive. And I don't want just the designer of the system to kind of work all this out. I really want to have a lot of control and I want transparency and control. And if a company can stand up and give me that in the context of new technology, I think they're good. First of all, be way more successful than our current generation. And like I said, I was mentioning Microsoft, I really think they're, they're pivoting to kind of be the trusted old uncle, but you know, I think that they get that this is a way to go, that if you let people find technology, empowers them to have more control and have and have control, not just over privacy, but over this rich set of interactions, that that people are going to like that a lot more. And that's, that's the right business model going forward. What does control over privacy look like? Do you think you should be able to just view all the data that? No, it's much more than that. I mean, first of all, it should be an individual decision. Some people don't want privacy. They want their whole life out there. Other people's want it. Privacy is not a zero one. It's not a legal thing. It's not just about which data is available, which is not. I like to recall to people that, you know, a couple hundred years ago, everyone, there was not really big cities, everyone lived in on the countryside and villages and villages. Everybody knew everything about you. Very, you didn't have any privacy. Is that bad? Are we better off now? Well, you know, arguably no, because what did you get for that loss of certain kinds of privacy? Well, people help each other if they, because they know everything about you. They know something's bad's happening, they will help you with that. Right. And now you live in a big city, no one knows about that. You get no help. So it kind of depends the answer. I want certain people who I trust and there should be relationships. I should kind of manage all those, but who knows what about me? I should have some agency there. It shouldn't, I shouldn't be a drift in a sea of technology where I have no agency. I don't want to go reading things and checking boxes. So I don't know how to do that. And I'm not a privacy researcher per se. I just, I recognize the vast complexity of this. It's not just technology. It's not just legal scholars meeting technologists. There's gotta be kind of a whole layers around it. And so I, when I alluded to this emerging engineering field, this is a big part of it. When electrical engineering came, I'm not one around at the time, but you just didn't plug electricity into walls and all kinds of work. You don't have to have like underwriters laboratory that reassured you that that plug's not going to burn up your house and that that machine will do this and that and everything. There'll be whole people who can install things. There'll be people who can watch the installers. There'll be a whole layers, you know, an onion of these kinds of things. And for things as deep and interesting as privacy, which is as least as interesting as electricity, that's going to take decades to kind of work out, but it's going to require a lot of new structures that we don't have right now. So it's kind of hard to talk about it. And you're saying there's a lot of money to be made if you get it right. So something you should look at. A lot of money to be made in all these things that provide human services and people recognize them as useful parts of their lives. So yeah. So yeah, the dialogue sometimes goes from the exuberant technologists to the no technology is good, kind of. And that's, you know, in our public discourse, you know, and as far as you see too much of this kind of thing and the sober discussions in the middle, which are the challenge he wants to have or where we need to be having our conversations. And you know, there's just not actually, there's not many forum fora for those. You know, there's, that's, that's kind of what I would look for. Maybe I could go and I could read a comment section of something and it would actually be this kind of dialogue going back and forth. You don't see much of this, right? Which is why actually there's a resurgence of podcasts out of all, because people are really hungry for conversation, but there's technology is not helping much. So comment sections of anything, including YouTube is not hurting and not helping. Yeah. And you think technically speaking, it's possible to help. I don't know the answers, but it's a, it's a, it's a less anonymity, a little more locality, you know, worlds that you kind of enter in and you trust the people there in those worlds so that when you start having a discussion, you know, not only is that people are not going to hurt you, but it's not going to be a total waste of your time because there's a lot of wasting of time that, you know, a lot of us, I pulled out of Facebook early on cause it was clearly going to waste a lot of my time even though there was some value. And so, yeah, worlds that are somehow you enter in and you know what you're getting and it's kind of appeals to you and you might, new things might happen, but you kind of have some, some trust in that world. And there's some deep, interesting, complex psychological aspects around anonymity, how that changes human behavior that's quite dark. Quite dark. Yeah. I think a lot of us are, especially those of us who really loved the advent of technology. I love social networks when they came out. I was just, I didn't see any negatives there at all. But then I started seeing comment sections. I think it was maybe, you know, with the CNN or something. And I started to go, wow, this, this darkness I just did not know about and, and our technology is now amplifying it. So sorry for the big philosophical question, but on that topic, do you think human beings, cause you've also, out of all things, had a foot in psychology too, the, do you think human beings are fundamentally good? Like all of us have good intent that could be mind or is it depending on context and environment, everybody could be evil. So my answer is fundamentally good. But fundamentally limited. All of us have very, you know, blinkers on. We don't see the other person's pain that easily. We don't see the other person's point of view that easily. We're very much in our own head, in our own world. And on my good days, I think the technology could open us up to, you know, more perspectives and more less blinkered and more understanding, you know, a lot of wars in human history happened because of just ignorance. They didn't, they, they thought the other person was doing this while their person wasn't doing this. And we have a huge amounts of that. But in my lifetime, I've not seen technology really help in that way yet. And I do, I do, I do believe in that, but you know, no, I think fundamentally humans are good. The people suffer, people have grievances because you have grudges and those things cause them to do things they probably wouldn't want. They regret it often. So no, I, I think it's a, you know, part of the progress of technology is to indeed allow it to be a little easier to be the real good person you actually are. Well, but do you think individual human life or society could be modeled as an optimization problem? Not the way I think typically, I mean, that's, you're talking about one of the most complex phenomenon in the whole, you know, in all of which the individual human life or society as a whole. Both, both. I mean, individual human life is amazingly complex. And so you know, optimization is kind of just one branch of mathematics that talks about certain kinds of things. And it just feels way too limited for the complexity of such things. What properties of optimization problems do you think, so do you think most interesting problems that could be solved through optimization, what kind of properties does that surface have non convexity, convexity, linearity, all those kinds of things, saddle points? Well, so optimization is just one piece of mathematics. You know, there's like, you just, even in our era, we're aware that say sampling is coming up, examples of something coming up with a distribution. What's optimization? What's sampling? Well, they, you can, if you're a kind of a certain kind of mathematician, you can try to blend them and make them seem to be sort of the same thing. But optimization is roughly speaking, trying to find a point that, a single point that is the optimum of a criterion function of some kind. And sampling is trying to, from that same surface, treat that as a distribution or density and find points that have high density. So I want the entire distribution in a sampling paradigm and I want the, you know, the single point, that's the best point in the optimization paradigm. Now if you were optimizing in the space of probability measures, the output of that could be a whole probability distribution. So you can start to make these things the same. But in mathematics, if you go too high up that kind of abstraction hierarchy, you start to lose the, you know, the ability to do the interesting theorems. So you kind of don't try that. You don't try to overly over abstract. So as a small tangent, what kind of worldview do you find more appealing? One that is deterministic or stochastic? Well, that's easy. I mean, I'm a statistician. You know, the world is highly stochastic. I don't know what's going to happen in the next five minutes, right? Because what you're going to ask, what we're going to do, what I'll say. Due to the uncertainty. Due to the... Massive uncertainty. Yeah. You know, massive uncertainty. And so the best I can do is have come rough sense or probability distribution on things and somehow use that in my reasoning about what to do now. So how does the distributed at scale when you have multi agent systems look like? So optimization can optimize sort of, it makes a lot more sense, sort of at least from my from robotics perspective, for a single robot, for a single agent, trying to optimize some objective function. When you start to enter the real world, this game theoretic concept starts popping up. That's how do you see optimization in this? Because you've talked about markets in a scale. What does that look like? Do you see it as optimization? Do you see it as sampling? Do you see like, how should you mark? These all blend together. And a system designer thinking about how to build an incentivized system will have a blend of all these things. So, you know, a particle in a potential well is optimizing a functional called a Lagrangian, right? The particle doesn't know that. There's no algorithm running that does that. It just happens. And so it's a description mathematically of something that helps us understand as analysts what's happening, right? And so the same thing will happen when we talk about, you know, mixtures of humans and computers and markets and so on and so forth, there'll be certain principles that allow us to understand what's happening, whether or not the actual algorithms are being used by any sense is not clear. Now at some point, I may have set up a multi agent or market kind of system. And I'm now thinking about an individual agent in that system. And they're asked to do some task and they're incentivized in some way, they get certain signals and they have some utility. What they will do at that point is they just won't know the answer, they may have to optimize to find an answer. Okay, so an artist could be embedded inside of an overall market. You know, and game theory is very, very broad. It is often studied very narrowly for certain kinds of problems. But it's roughly speaking, this is just the, I don't know what you're going to do. So I kind of anticipate that a little bit, and you anticipate what I'm anticipating. And we kind of go back and forth in our own minds. We run kind of thought experiments. You've talked about this interesting point in terms of game theory, you know, most optimization problems really hate saddle points, maybe you can describe what saddle points are. But I've heard you kind of mentioned that there's a there's a branch of optimization that you could try to explicitly look for saddle points as a good thing. Oh, not optimization. That's just game theory that that so there's all kinds of different equilibria in game theory. And some of them are highly explanatory behavior. They're not attempting to be algorithmic. They're just trying to say, if you happen to be at this equilibrium, you would see certain kind of behavior. And we see that in real life. That's what an economist wants to do, especially behavioral economists in continuous differential game theory, you're in continuous spaces, a some of the simplest equilibria are saddle points and Nash equilibrium as a saddle point. It's a special kind of saddle point. So classically, in game theory, you were trying to find Nash equilibria and an algorithmic game theory, you're trying to find algorithms that would find them. And so you're trying to find saddle points. I mean, so that's literally what you're trying to do. But you know, any economist knows that Nash equilibria have their limitations. They are definitely not that explanatory in many situations. They're not what you really want. There's other kind of equilibria. And there's names associated with these because they came from history with certain people working on them, but there will be new ones emerging. So you know, one example is a Stackelberg equilibrium. So you know, Nash, you and I are both playing this game against each other or for each other, maybe it's cooperative, and we're both going to think it through and then we're going to decide and we're going to do our thing simultaneously. You know, in a Stackelberg, no, I'm going to be the first mover. I'm going to make a move. You're going to look at my move and then you're going to make yours. Now since I know you're going to look at my move, I anticipate what you're going to do. And so I don't do something stupid, but then I know that you are also anticipating me. So we're kind of going back and forth on why, but there is then a first mover thing. And so those are different equilibria, right? And so just mathematically, yeah, these things have certain topologies and certain shapes that are like, what's it, algorithmically or dynamically, how do you move towards them? How do you move away from things? You know, so some of these questions have answers, they've been studied, others do not. And especially if it becomes stochastic, especially if there's large numbers of decentralized things, there's just, you know, young people get in this field who kind of think it's all done because we have, you know, TensorFlow. Well, no, these are all open problems and they're really important and interesting. And it's about strategic settings. How do I collect data? Suppose I don't know what you're going to do because I don't know you very well, right? Well, I got to collect data about you. So maybe I want to push you into a part of the space where I don't know much about you so I can get data. Cause, and then later I'll realize that you'll never, you'll never go there because of the way the game is set up. You know, that's part of the overall, you know, data analysis context is that. Even the game of poker is fascinating space, whenever there's any uncertainty, a lack of information, it's a super exciting space. Just to linger on optimization for a second. So when we look at deep learning, it's essentially minimization of a complicated loss function. So is there something insightful or hopeful that you see in the kinds of function surface that loss functions, the deep learning and in the real world is trying to optimize over? Is there something interesting as it's just the usual kind of problems of optimization? I think from an optimization point of view, that surface, first of all, it's pretty smooth. And secondly, if there's over, if it's over parameterized, there's kind of lots of paths down to reasonable Optima. And so kind of the getting downhill to the, to an optimum is viewed as not as hard as you might've expected in high dimensions. The fact that some Optima tend to be really good ones and others not so good. And you tend to, it's not, sometimes you find the good ones is sort of still needs explanation. Yeah. But, but the particular surface is coming from the particular generation of neural nets. I kind of suspect those will, those will change in 10 years. It will not be exactly those surfaces. There'll be some others that are an optimization theory will help contribute to why other surfaces or why other algorithms. Years of arithmetic operations with a little bit of nonlinearity, that's not, that didn't come from neuroscience per se. I mean, maybe in the minds of some of the people working on it, they were thinking about brains, but they were arithmetic circuits in all kinds of fields, computer science control theory and so on. And that layers of these could transform things in certain ways. And that if it's smooth, maybe you could find parameter values is a sort of big discovery that it's working, it's able to work at this scale. But I don't think that we're stuck with that and we're, we're certainly not stuck with that cause we're understanding the brain. So in terms of on the algorithm side sort of gradient descent, do you think we're stuck with gradient descent as a variance of it? What variance do you find interesting or do you think there'll be something else invented that is able to walk all over these optimization spaces in more interesting ways? So there's a co design of the surface and the, or the architecture and the algorithm. So if you just ask if we stay with the kind of architectures that we have now and not just neural nets, but you know, phase retrieval architectures or matrix completion architectures and so on. You know, I think we've kind of come to a place where yeah, a stochastic gradient algorithms are dominant and there are versions that are a little better than others. They have more guarantees, they're more robust and so on. And there's ongoing research to kind of figure out which is the best arm for which situation. But I think that that'll start to co evolve, that that'll put pressure on the actual architecture. And so we shouldn't do it in this particular way, we should do it in a different way because this other algorithm is now available if you do it in a different way. So that I can't really anticipate that co evolution process, but you know, gradients are amazing mathematical objects. They have a lot of people who start to study them more deeply mathematically are kind of shocked about what they are and what they can do. Think about it this way, suppose that I tell you if you move along the x axis, you go uphill in some objective by three units, whereas if you move along the y axis, you go uphill by seven units, right? Now I'm going to only allow you to move a certain unit distance, right? What are you going to do? Well, most people will say that I'm going to go along the y axis, I'm getting the biggest bang for my buck, you know, and my buck is only one unit, so I'm going to put all of it in the y axis, right? And why should I even take any of my strength, my step size and put any of it in the x axis because I'm getting less bang for my buck. That seems like a completely clear argument and it's wrong because the gradient direction is not to go along the y axis, it's to take a little bit of the x axis. And to understand that, you have to know some math and so even a trivial so called operator like gradient is not trivial and so, you know, exploiting its properties is still very important. Now we know that just pervading descent has got all kinds of problems, it gets stuck in many ways and it had never, you know, good dimension dependence and so on. So my own line of work recently has been about what kinds of stochasticity, how can we get dimension dependence, how can we do the theory of that and we've come up pretty favorable results with certain kinds of stochasticity. We have sufficient conditions generally. We know if you do this, we will give you a good guarantee. We don't have necessary conditions that it must be done a certain way in general. So stochasticity, how much randomness to inject into the walking along the gradient? And what kind of randomness? Why is randomness good in this process? Why is stochasticity good? Yeah, so I can give you simple answers but in some sense again, it's kind of amazing. Stochasticity just, you know, particular features of a surface that could have hurt you if you were doing one thing deterministically won't hurt you because by chance, there's very little chance that you would get hurt. So here stochasticity, it just kind of saves you from some of the particular features of surfaces. In fact, if you think about surfaces that are discontinuous in our first derivative, like an absolute value function, you will go down and hit that point where there's nondifferentiability. And if you're running a deterministic algorithm at that point, you can really do something bad. Whereas stochasticity just means it's pretty unlikely that's going to happen, that you're going to hit that point. So it's again, nontrivial to analyze but especially in higher dimensions, also stochasticity, our intuition isn't very good about it but it has properties that kind of are very appealing in high dimensions for a lot of large number of reasons. So it's all part of the mathematics to kind of, that's what's fun to work in the field is that you get to try to understand this mathematics. But long story short, you know, partly empirically, it was discovered stochastic gradient is very effective and theory kind of followed, I'd say, that but I don't see that we're getting clearly out of that. What's the most beautiful, mysterious, a profound idea to you in optimization? I don't know the most. But let me just say that Nesterov's work on Nesterov acceleration to me is pretty surprising and pretty deep. Can you elaborate? Well Nesterov acceleration is just that, suppose that we are going to use gradients to move around in a space. For the reasons I've alluded to, they're nice directions to move. And suppose that I tell you that you're only allowed to use gradients, you're not going to be allowed to use this local person that can only sense kind of the change in the surface. But I'm going to give you kind of a computer that's able to store all your previous gradients. And so you start to learn some something about the surface. And I'm going to restrict you to maybe move in the direction of like a linear span of all the gradients. So you can't kind of just move in some arbitrary direction, right? So now we have a well defined mathematical complexity model. There's certain classes of algorithms that can do that and others that can't. And we can ask for certain kinds of surfaces, how fast can you get down to the optimum? So there's answers to these. So for a smooth convex function, there's an answer, which is one over the number of steps squared. You will be within a ball of that size after k steps. Gradient descent in particular has a slower rate, it's one over k. So you could ask, is gradient descent actually, even though we know it's a good algorithm, is it the best algorithm? And the answer is no. Well, not clear yet, because one over k squared is a lower bound. That's probably the best you can do. Gradient is one over k, but is there something better? And so I think as a surprise to most, Nesterov discovered a new algorithm that has got two pieces to it. It's two gradients and puts those together in a certain kind of obscure way. And the thing doesn't even move downhill all the time. It sometimes goes back uphill. And if you're a physicist, that kind of makes some sense. You're building up some momentum and that is kind of the right intuition, but that intuition is not enough to understand kind of how to do it and why it works. But it does. It achieves one over k squared and it has a mathematical structure and it's still kind of to this day, a lot of us are writing papers and trying to explore that and understand it. So there are lots of cool ideas and optimization, but just kind of using gradients, I think is number one that goes back, you know, 150 years. And then Nesterov, I think has made a major contribution with this idea. So like you said, gradients themselves are in some sense, mysterious. They're not as trivial as... Not as trivial. Coordinate descent is more of a trivial one. You just pick one of the coordinates. That's how we think. That's how our human mind thinks. That's how our human minds think. And gradients are not that easy for our human mind to grapple with. An absurd question, but what is statistics? So here it's a little bit, it's somewhere between math and science and technology. It's somewhere in that convex hole. So it's a set of principles that allow you to make inferences that have got some reason to be believed and also principles that allow you to make decisions where you can have some reason to believe you're not going to make errors. So all of that requires some assumptions about what do you mean by an error? What do you mean by the probabilities? But after you start making some of those assumptions, you're led to conclusions that, yes, I can guarantee that if you do this in this way, your probability of making an error will be small. Your probability of continuing to not make errors over time will be small. And the probability that you found something that's real will be small, will be high. So decision making is a big part of that. Decision making is a big part. Yeah. So statistics, short history was that, it goes back as a formal discipline, 250 years or so. It was called inverse probability because around that era, probability was developed sort of especially to explain gambling situations. Of course, interesting. So you would say, well, given the state of nature is this, there's a certain roulette board that has a certain mechanism and what kind of outcomes do I expect to see? And especially if I do things long amounts of time, what outcomes will I see? And the physicists started to pay attention to this. And then people said, well, let's turn the problem around. What if I saw certain outcomes, could I infer what the underlying mechanism was? That's an inverse problem. And in fact, for quite a while, statistics was called inverse probability. That was the name of the field. And I believe that it was Laplace who was working in Napoleon's government who needed to do a census of France, learn about the people there. So he went and gathered data and he analyzed that data to determine policy and said, well, let's call this field that does this kind of thing statistics because the word state is in there. In French, that's etat, but it's the study of data for the state. So anyway, that caught on and it's been called statistics ever since. But by the time it got formalized, it was sort of in the 30s. And around that time, there was game theory and decision theory developed nearby. People in that era didn't think of themselves as either computer science or statistics or control or econ. They were all the above. And so Von Neumann is developing game theory, but also thinking of that as decision theory. Wald is an econometrician developing decision theory and then turning that into statistics. And so it's all about, here's not just data and you analyze it, here's a loss function. Here's what you care about. Here's the question you're trying to ask. Here is a probability model and here's the risk you will face if you make certain decisions. And to this day, in most advanced statistical curricula, you teach decision theory as the starting point and then it branches out into the two branches of Bayesian and frequentist. But that's all about decisions. In statistics, what is the most beautiful, mysterious, maybe surprising idea that you've come across? Yeah, good question. I mean, there's a bunch of surprising ones. There's something that's way too technical for this thing, but something called James Stein estimation, which is kind of surprising and really takes time to wrap your head around. Can you try to maybe... I think I don't want to even want to try. Let me just say a colleague at Steven Stigler at University of Chicago wrote a really beautiful paper on James Stein estimation, which helps to... It's views a paradox. It kind of defeats the mind's attempts to understand it, but you can and Steve has a nice perspective on that. So one of the troubles with statistics is that it's like in physics that are in quantum physics, you have multiple interpretations. There's a wave and particle duality in physics and you get used to that over time, but it still kind of haunts you that you don't really quite understand the relationship. The electron's a wave and electron's a particle. Well the same thing happens here. There's Bayesian ways of thinking and frequentist, and they are different. They sometimes become sort of the same in practice, but they are physically different. And then in some practice, they are not the same at all. They give you rather different answers. And so it is very much like wave and particle duality, and that is something that you have to kind of get used to in the field. Can you define Bayesian and frequentist? Yeah in decision theory you can make, I have a video that people could see. It's called are you a Bayesian or a frequentist and kind of help try to make it really clear. It comes from decision theory. So you know, decision theory, you're talking about loss functions, which are a function of data X and parameter theta. They're a function of two arguments. Okay. Neither one of those arguments is known. You don't know the data a priori. It's random and the parameters unknown. All right. So you have a function of two things you don't know, and you're trying to say, I want that function to be small. I want small loss, right? Well what are you going to do? So you sort of say, well, I'm going to average over these quantities or maximize over them or something so that, you know, I turn that uncertainty into something certain. So you could look at the first argument and average over it, or you could look at the second argument and average over it. That's Bayesian and frequentist. So the frequentist says, I'm going to look at the X, the data, and I'm going to take that as random and I'm going to average over the distribution. So I take the expectation loss under X. Theta is held fixed, right? That's called the risk. And so it's looking at other, all the data sets you could get, right? And say, how well will a certain procedure do under all those data sets? That's called a frequentist guarantee, right? So I think it is very appropriate when like you're building a piece of software and you're shipping it out there and people are using it on all kinds of data sets. You want to have a stamp, a guarantee on it that as people run it on many, many data sets that you never even thought about that 95% of the time it will do the right thing. Perfectly reasonable. The Bayesian perspective says, well, no, I'm going to look at the other argument of the loss function, the theta part, okay? That's unknown and I'm uncertain about it. So I could have my own personal probability for what it is, you know, how many tall people are there out there? I'm trying to infer the average height of the population while I have an idea roughly what the height is. So I'm going to average over the theta. So now that loss function as only now, again, one argument's gone, now it's a function of X and that's what a Bayesian does is they say, well, let's just focus on the particular X we got, the data set we got, we condition on that. Conditional on the X, I say something about my loss. That's a Bayesian approach to things. And the Bayesian will argue that it's not relevant to look at all the other data sets you could have gotten and average over them, the frequentist approach. It's really only the data sets you got, right? And I do agree with that, especially in situations where you're working with a scientist, you can learn a lot about the domain and you're really only focused on certain kinds of data and you gathered your data and you make inferences. I don't agree with it though, that, you know, in the sense that there are needs for frequentist guarantees, you're writing software, people are using it out there, you want to say something. So these two things have to got to fight each other a little bit, but they have to blend. So long story short, there's a set of ideas that are right in the middle that are called empirical Bayes. And empirical Bayes sort of starts with the Bayesian framework. It's kind of arguably philosophically more, you know, reasonable and kosher. Write down a bunch of the math that kind of flows from that, and then realize there's a bunch of things you don't know because it's the real world and you don't know everything. So you're uncertain about certain quantities. At that point, ask, is there a reasonable way to plug in an estimate for those things? Okay. And in some cases, there's quite a reasonable thing to do, to plug in, there's a natural thing you can observe in the world that you can plug in and then do a little bit more mathematics and assure yourself it's really good. So based on math or based on human expertise, what's, what, what are good? Oh, they're both going in. The Bayesian framework allows you to put a lot of human expertise in, but the math kind of guides you along that path and then kind of reassures you the end, you could put that stamp of approval under certain assumptions, this thing will work. So you asked the question, what's my favorite, you know, or what's the most surprising, nice idea. So one that is more accessible is something called false discovery rate, which is, you know, you're making not just one hypothesis test or making one decision, you're making a whole bag of them. And in that bag of decisions, you look at the ones where you made a discovery, you announced that something interesting had happened. All right. That's going to be some subset of your big bag. In the ones you made a discovery, which subset of those are bad? Or false, false discoveries. You'd like the fraction of your false discoveries among your discoveries to be small. That's a different criterion than accuracy or precision or recall or sensitivity and specificity. It's a different quantity. Those latter ones are almost all of them have more of a frequentist flavor. They say, given the truth is that the null hypothesis is true. Here's what accuracy I would get, or given that the alternative is true, here's what I would get. So it's kind of going forward from the state of nature to the data. The Bayesian goes the other direction from the data back to the state of nature. And that's actually what false discovery rate is. It says, given you made a discovery, okay, that's conditioned on your data. What's the probability of the hypothesis? It's going the other direction. And so the classical frequency look at that, well, I can't know that there's some priors needed in that. And the empirical Bayesian goes ahead and plows forward and starts writing down these formulas and realizes at some point, some of those things can actually be estimated in a reasonable way. And so it's kind of, it's a beautiful set of ideas. So I, this kind of line of argument has come out. It's not certainly mine, but it sort of came out from Robbins around 1960. Brad Efron has written beautifully about this in various papers and books. And the FDR is, you know, Benjamin in Israel, John Story did this Bayesian interpretation and so on. And he used to absorb these things over the years and find it a very healthy way to think about statistics. Let me ask you about intelligence to jump slightly back out into philosophy, perhaps. You said that maybe you can elaborate, but you said that defining just even the question of what is intelligence is a very difficult question. Is it a useful question? Do you think we'll one day understand the fundamentals of human intelligence and what it means, you know, have good benchmarks for general intelligence that we put before our machines? So I don't work on these topics so much that you're really asking the question for a psychologist really. And I studied some, but I don't consider myself at least an expert at this point. You know, a psychologist aims to understand human intelligence, right? And I think many psychologists I know are fairly humble about this. They might try to understand how a baby understands, you know, whether something's a solid or liquid or whether something's hidden or not. And maybe how a child starts to learn the meaning of certain words, what's a verb, what's a noun and also, you know, slowly but surely trying to figure out things. But humans ability to take a really complicated environment, reason about it, abstract about it, find the right abstractions, communicate about it, interact and so on is just, you know, really staggeringly rich and complicated. And so, you know, I think in all humility, we don't think we're kind of aiming for that in the near future. A certain psychologist doing experiments with babies in the lab or with people talking has a much more limited aspiration. And you know, Kahneman and Tversky would look at our reasoning patterns and they're not deeply understanding all the how we do our reasoning, but they're sort of saying, hey, here's some oddities about the reasoning and some things you should think about it. But also, as I emphasize in some things I've been writing about, you know, AI, the revolution hasn't happened yet. Yeah. Great blog post. I've been emphasizing that, you know, if you step back and look at intelligent systems of any kind and whatever you mean by intelligence, it's not just the humans or the animals or, you know, the plants or whatever, you know, so a market that brings goods into a city, you know, food to restaurants or something every day is a system. It's a decentralized set of decisions. Looking at it from far enough away, it's just like a collection of neurons. Every neuron is making its own little decisions, presumably in some way. And if you step back enough, every little part of an economic system is making all of its decisions. And just like with the brain, who knows what an individual neuron does and what the overall goal is, right? But something happens at some aggregate level, same thing with the economy. People eat in a city and it's robust. It works at all scales, small villages to big cities. It's been working for thousands of years. It works rain or shine, so it's adaptive. So all the kind of, you know, those are adjectives one tends to apply to intelligent systems. Robust, adaptive, you know, you don't need to keep adjusting it, self healing, whatever. Plus not perfect. You know, intelligences are never perfect and markets are not perfect. But I do not believe in this era that you cannot, that you can say, well, our computers are, our humans are smart, but you know, no markets are not, more markets are. So they are intelligent. Now we humans didn't evolve to be markets. We've been participating in them, right? But we are not ourselves a market per se. The neurons could be viewed as the market. There's economic, you know, neuroscience kind of perspective. That's interesting to pursue all that. The point though is, is that if you were to study humans and really be the world's best psychologist studied for thousands of years and come up with the theory of human intelligence, you might have never discovered principles of markets, you know, supply demand curves and you know, matching and auctions and all that. Those are real principles and they lead to a form of intelligence that's not maybe human intelligence. It's arguably another kind of intelligence. There probably are third kinds of intelligence or fourth that none of us are really thinking too much about right now. So if you really, and then all of those are relevant to computer systems in the future. Certainly the market one is relevant right now. Whereas the understanding of human intelligence is not so clear that it's relevant right now. Probably not. So if you want general intelligence, whatever one means by that, or, you know, understanding intelligence in a deep sense and all that, it is definitely has to be not just human intelligence. It's gotta be this broader thing. And that's not a mystery. Markets are intelligent. So, you know, it's definitely not just a philosophical stance to say we've got to move beyond intelligence. That sounds ridiculous. Yeah. But it's not. And in that blog post, you define different kinds of like intelligent infrastructure, AI, which I really like is some of the concepts you've just been describing. Do you see ourselves, if we see earth, human civilization as a single organism, do you think the intelligence of that organism, when you think from the perspective of markets and intelligence infrastructure is increasing, is it increasing linearly? Is it increasing exponentially? What do you think the future of that intelligence? Yeah, I don't know. I don't tend to think, I don't tend to answer questions like that because you know, that's science fiction. I'm hoping to catch you off guard. Well again, because you said it's so far in the future, it's fun to ask and you'll probably, you know, like you said, predicting the future is really nearly impossible. But say as an axiom, one day we create a human level, a superhuman level intelligent, not the scale of markets, but the scale of an individual. What do you think it is, what do you think it would take to do that? Or maybe to ask another question is how would that system be different than the biological human beings that we see around us today? Is it possible to say anything interesting to that question or is it just a stupid question? It's not a stupid question, but it's science fiction. Science fiction. And so I'm totally happy to read science fiction and think about it from time in my own life. I loved, there was this like brain in a vat kind of, you know, little thing that people were talking about when I was a student, I remember, you know, imagine that, you know, between your brain and your body, there's a, you know, there's a bunch of wires, right? And suppose that every one of them was replaced with a literal wire. And then suppose that wire was turned in actually a little wireless, you know, there's a receiver and sender. So the brain has got all the senders and receiver, you know, on all of its exiting, you know, axons and all the dendrites down to the body have replaced with senders and receivers. Now you could move the body off somewhere and put the brain in a vat, right? And then you could do things like start killing off those senders and receivers one by one. And after you've killed off all of them, where is that person? You know, they thought they were out in the body walking around the world and they moved on. So those are science fiction things. Those are fun to think about. It's just intriguing about where is, what is thought, where is it and all that. And I think every 18 year old should take philosophy classes and think about these things. And I think that everyone should think about what could happen in society that's kind of bad and all that. But I really don't think that's the right thing for most of us that are my age group to be doing and thinking about. I really think that we have so many more present, you know, first challenges and dangers and real things to build and all that such that, you know, spending too much time on science fiction, at least in public for like this, I think is not what we should be doing. Maybe over beers in private. That's right. Well, I'm not going to broadcast where I have beers because this is going to go on Facebook and I don't want a lot of people showing up there. But yeah, I'll, I love Facebook, Twitter, Amazon, YouTube. I have I'm optimistic and hopeful, but maybe, maybe I don't have grounds for such optimism and hope. But let me ask, you've mentored some of the brightest sort of some of the seminal figures in the field. Can you give advice to people who are undergraduates today? What does it take to take, you know, advice on their journey if they're interested in machine learning and in the ideas of markets from economics and psychology and all the kinds of things that you've exploring? What steps should they take on that journey? Well, yeah, first of all, the door is open and second, it's a journey. I like your language there. It is not that you're so brilliant and you have great, brilliant ideas and therefore that's just, you know, that's how you have success or that's how you enter into the field. It's that you apprentice yourself, you spend a lot of time, you work on hard things, you try and pull back and you be as broad as you can, you talk to lots of people. And it's like entering in any kind of a creative community. There's years that are needed and human connections are critical to it. So, you know, I think about, you know, being a musician or being an artist or something, you don't just, you know, immediately from day one, you know, you're a genius and therefore you do it. No, you, you know, practice really, really hard on basics and you be humble about where you are and then, and you realize you'll never be an expert on everything. So you kind of pick and there's a lot of randomness and a lot of kind of luck, but luck just kind of picks out which branch of the tree you go down, but you'll go down some branch. So yeah, it's a community. So the graduate school is, I still think is one of the wonderful phenomena that we have in our, in our world. It's very much about apprenticeship with an advisor. It's very much about a group of people you belong to. It's a four or five year process. So it's plenty of time to start from kind of nothing to come up to something, you know, more, more expertise, and then to start to have your own creativity start to flower, even surprising your own self. And it's a very cooperative endeavor. I think a lot of people think of science as highly competitive and I think in some other fields it might be more so. Here it's way more cooperative than you might imagine. And people are always teaching each other something and people are always more than happy to be clear that, so I feel I'm an expert on certain kinds of things, but I'm very much not expert on lots of other things and a lot of them are relevant and a lot of them are, I should know, but should in some society, you know, you don't. So I'm always willing to reveal my ignorance to people around me so they can teach me things. And I think a lot of us feel that way about our field. So it's very cooperative. I might add it's also very international because it's so cooperative. We see no barriers. And so that the nationalism that you see, especially in the current era and everything is just at odds with the way that most of us think about what we're doing here, where this is a human endeavor and we cooperate and are very much trying to do it together for the, you know, the benefit of everybody. So last question, where and how and why did you learn French and which language is more beautiful English or French? Great question. So first of all, I think Italian is actually more beautiful than French and English. And I also speak that. So I'm married to an Italian and I have kids and we speak Italian. Anyway, all kidding aside, every language allows you to express things a bit differently. And it is one of the great fun things to do in life is to explore those things. So in fact, when I kids or teens or college students ask me what they study, I say, well, do what your heart, where your heart is, certainly do a lot of math. Math is good for everybody, but do some poetry and do some history and do some language too. You know, throughout your life, you'll want to be a thinking person. You'll want to have done that. For me, French I learned when I was, I'd say a late teen, I was living in the middle of the country in Kansas and not much was going on in Kansas with all due respect to Kansas. And so my parents happened to have some French books on the shelf and just in my boredom, I pulled them down and I found this is fun. And I kind of learned the language by reading. And when I first heard it spoken, I had no idea what was being spoken, but I realized I had somehow knew it from some previous life and so I made the connection. But then I traveled and just I love to go beyond my own barriers and my own comfort or whatever. And I found myself on trains in France next to say older people who had lived a whole life of their own. And the ability to communicate with them was special and the ability to also see myself in other people's shoes and have empathy and kind of work on that language as part of that. So after that kind of experience and also embedding myself in French culture, which is quite amazing, languages are rich, not just because there's something inherently beautiful about it, but it's all the creativity that went into it. So I learned a lot of songs, read poems, read books. And then I was here actually at MIT where we're doing the podcast today and a young professor not yet married and not having a lot of friends in the area. So I just didn't have, I was kind of a bored person. I said, I heard a lot of Italians around. There's happened to be a lot of Italians at MIT, an Italian professor for some reason. And so I was kind of vaguely understanding what they were talking about. I said, well, I should learn this language too. So I did. And then later met my spouse and Italian became a part of my life. But I go to China a lot these days. I go to Asia, I go to Europe and every time I go, I kind of am amazed by the richness of human experience and the people don't have any idea if you haven't traveled, kind of how amazingly rich and I love the diversity. It's not just a buzzword to me. It really means something. I love to embed myself with other people's experiences. And so yeah, learning language is a big part of that. I think I've said in some interview at some point that if I had millions of dollars and infinite time or whatever, what would you really work on if you really wanted to do AI? And for me, that is natural language and really done right. Deep understanding of language. That's to me, an amazingly interesting scientific challenge. One we're very far away on. One we're very far away, but good natural language. People are kind of really invested then. I think a lot of them see that's where the core of AI is that if you understand that you really help human communication, you understand something about the human mind, the semantics that come out of the human mind and I agree, I think that will be such a long time. So I didn't do that in my career just cause I kind of, I was behind in the early days. I didn't kind of know enough of that stuff. I was at MIT, I didn't learn much language and it was too late at some point to kind of spend a whole career doing that, but I admire that field and so in my little way by learning language, you know, kind of that part of my brain has been trained up. Jan was right. You truly are the Miles Davis of machine learning. I don't think there's a better place than it. Mike it was a huge honor talking to you today. Merci beaucoup. All right. It's been my pleasure. Thanks for listening to this conversation with Michael I. Jordan and thank you to our presenting sponsor, Cash App. Download it, use code LEXPodcast, you'll get $10 and $10 will go to FIRST, an organization that inspires and educates young minds to become science and technology innovators of tomorrow. If you enjoy this podcast, subscribe on YouTube, give it five stars on Apple Podcast, support on Patreon, or simply connect with me on Twitter at Lex Friedman. And now let me leave you with some words of wisdom from Michael I. Jordan from his blog post titled Artificial Intelligence, the revolution hasn't happened yet, calling for broadening the scope of the AI field. We should embrace the fact that what we are witnessing is the creation of a new branch of engineering. The term engineering is often invoked in a narrow sense in academia and beyond with overtones of cold, effectless machinery and negative connotations of loss of control by humans. But an engineering discipline can be what we want it to be. In the current era, we have a real opportunity to conceive of something historically new, a human centric engineering discipline. I will resist giving this emerging discipline a name, but if the acronym AI continues to be used, let's be aware of the very real limitations of this placeholder. Let's broaden our scope, tone down the hype, and recognize the serious challenges ahead. Thank you for listening and hope to see you next time.\",\n          \" The following is a conversation with Niels Jorgensen, a New York firefighter for over 21 years who was there at Ground Zero on September 11th, 2001. He was forced to retire because of the leukemia he contracted from cleaning up Ground Zero. This podcast tells his story, and the story of other great men and women who were there that day. Some of the stories we talk about are part of a new limited podcast series that Niels hosts called 20 for 20, with 20 episodes for the 20 years since 9 11. To support this podcast, please check out our sponsors in the description. As a side note, please allow me to say a few words about the terrorist attacks on September 11th, 2001. I was in downtown Chicago on that day, lost in the mundane busyness of an early Tuesday morning. At that time, I was already fascinated by human nature, the best and the worst of it, exploring it through the study of history and literature. In the years before, as a young boy growing up in Russia, I saw chaos, uncertainty, and desperation in the Soviet Union of the 1990s, wrapping up a century of war and suffering. But after coming to America for me, there was a sense of hope, like all of it was behind us, a bad dream to be forgotten as we enter into the new century. On 9 11, when I saw the news of the second plane hitting the towers, my sense of hope had changed. I understood that the 21st century, like the century before, would too have its tragedies, its evildoers, its wars, and its suffering. And unlike the history books, these stories will involve all of us. They will involve me in however small and insignificant a role, but one that nevertheless carries the responsibility to help. I became an American that day, a citizen of the world. I felt the common humanity in all of us. I felt the unity and the love in the days that followed. And I think most of the world shared in this feeling that we are all in this together. Evil cannot defeat the human spirit. There were many heroes sung and unsung on that day and in the years after. Often politicians fail to rightfully honor the service and sacrifice of these heroes. There's much I could say about that, but I don't want to waste my words on the failures of weak leaders. Instead, I want to say thank you to the men and women who rushed to Ground Zero to help, who put on a uniform to serve, who make me proud to be an American and a human being, and give me hope about the future of our civilization here on a small spinning rock that despite the long odds keeps kindling the fire of human consciousness and love. This is the Lex Friedman podcast and here is my conversation with Niels Jorgensen. Take me through the day of September 11th, 2001 as you experienced it, as you lived it. September 11th, 2001 was a bright, beautiful, sunny Tuesday morning. It was a late summer. There's a lot of folks who go to the beaches in New Jersey and call it the short summer. Everybody's left there for Labor Day, but it's still beautiful enough to enjoy the weather. I left my house about 6.30 in the morning and my four and a half year old daughter said to me, daddy, which truck are you driving today, the fire truck, the oil truck, or the boar's head truck? Because I had three jobs at the time. Most New York City firefighters and police officers, EMS, we don't make the most amount of money. So in order to live in that city, you have to hustle. And my wife stayed at home raising the children. So my daughter said, oh, so you should be safe because you're on the oil truck. I told her I was going on the oil truck that day. So she said, you should be safe today, daddy. So I left and worked for this great company on the North Shore, Staten Island, Quinlan Fuel. Very nice people, treated me very well. And it was my first day back actually for the winter season. Usually get laid off a couple months in the summer because things, you know, too hot to need oil. So I took the truck, started my route that day and plane to New Jersey. And plane hit the tower. So initially I'm like, oh, it's probably some silly Lear jet pilot. And he veered off track to get a better picture for a client and he hit the building. Probably hit a, you know, bad turbulence, gust of wind. It's very windy down in that area in Manhattan. So that was my first thought. Can we pause there for a second? So 6.30 a.m. you wake up, you leave, and then the plane hits at 8.45 a.m. It's just interesting how you phrase it. So how did you hear that a plane hit something? I'm a big news radio guy, news guy, bit of a buff. I've been that way since I was a kid and I had the news radio on the local New York radio station. And as I was driving the truck, I heard, you know, an emergency report. This just in, aircraft has just struck the World Trade Center. And where Quinlan's is located, it's on the north rim of Staten Island, which is right on New York Harbor. And you could see Statue of Liberty, you know, a mile or two away in your distance. And then past that is the towers. So I just literally stopped the truck and looked out and I saw the smoke. So there was smoke? Oh, it was dark, black smoke. It was just, yeah, I mean, it was burning fully at that point. Did you have fear of what the hell happened? Or is it? I was initially scared for anybody involved. I realized, I said, there's gonna be lots of fatalities, obviously, depending on the size of the aircraft. And, you know, the business day there had started probably at 8, 8.30. So those buildings should have been packed at that moment. So that was a thought that crossed my mind. But from our being responder perspective, if you're off duty, normally you do not go to a scene that they don't want you to because of accountability and safety. The on duty platoon will handle it. And if it's something very horrific, then they will have something called a recall, which is any police firefighter or EMS personnel is obligated to go to their command immediately, check in with, you know, their command to get their gear and stand by and await orders for deployment or to remain in that command for routine duties. How often throughout history have there been recalls? I believe the one prior to that was like in the 1968 riots, possibly, and then maybe in the 70s, there was another blackout and riots. And I remember my dad talking about it. And he actually always said, just remember if something bad's going down, don't just rush in, you will wait the recall. Or at the very least, if there isn't a recall, you get to your firehouse. And because if you show up somewhere, there's a good chance that no one knows you're there. And now you, in your well intended movements, you get lost or trapped or no one's looking for you. So that's the whole thing with, you know, checking in. And now you're with a squad or, you know, group of guys and everyone knows, you know, hey, there's Nels, there's Lex. Okay, they're on, you know, this team. So I said, all right, they're not gonna need us. It's probably gonna be a fifth alarm. And you know, there'll be 250 firefighters there. They'll handle it. It's gonna be a bad day for those guys, but you know, our guys take on some heavy stuff and they'll be fine. A few minutes later, the second plane hit and I knew immediately, I'm like, okay, we're under attack. So I just flew the truck back in. I told my boss, I have to go. He understood, he knew something was way wrong and I just was flying. At the time, I actually had a yellow Volkswagen Beetle, kind of a goofy car to be driving, but I loved it. So for people who are just listening, you're kind of a big guy. Well, yeah, I definitely need to lose about 50 pounds. No, I don't mean in that way, your frame, big hands. As my beloved friend, Bobby Adams would say to me, I was driving around in a clown wagon and he also says, I have a waving hairdo, waving bye bye. So thanks, Bobby. But yeah, he's a great friend. Yeah, so I took the Volkswagen and I flew in and I was heading over to Verrazano Bridge and hit the Brooklyn Queens Expressway. And my phone rang and my wife normally doesn't curse or raise a voice and she was yelling at me. And she said, don't go in there, go to your firehouse. Well, first she asked, well, she knew I was on the way, but she just wanted to know where. And I said, I'm on the curve, which is 65th Street on the Brooklyn Queens Expressway called Dead Man's Curve. We actually used to do a lot of car wrecks up there. And I was hitting that curve pretty fast. And then right around the curve is the exit to the firehouse. And I had to decide, well, am I driving right in to the battery tunnel to the city or am I going to the firehouse? And then I said, but I have no gear. I'm gonna be ineffective. How do I show up with no gear, no protection, you know? So she said, do what your dad would follow the recall, go to the firehouse. And I hung up the phone, said, I love you, gotta go. And I did, I went to the firehouse and I'm glad I listened to her. I had my father ringing in my ears. My dad, beautiful guy, he's 82. He did 34 years in the New York City Fire Department. He came down on end stage, non and he's 38 back in, going on 39, 1978. And this guy, he's my hero. He was gonna die, they sent him home. They said, there's really not much we can do. Go get your affairs in order. And he says, but doc, I have three young kids. And she called him a couple hours later. She said, I got in touch with Sloan Kettering and they have a new drug. So I'll take you to the hospital. and do the same exact reverse route and he'd get to the cancer center and my mom would meet him and he'd get his infusion and within two hours he'd be violently ill for a few days, really badly ill. And I just remember, yeah, I was 10 years old and he just had to have the room darkened out and he'd be so sick and I'd just go in and wipe the vomit on his face, just try to give him a little water but he couldn't take it down because he'd throw it up. And maybe on Saturday he'd start coming around a little bit, drink down a little bit of tea and on Sunday morning he'd put his robe on and he'd go down, mom would make him black coffee and toast and he'd sit up, watch the news, watch a game and then Monday morning he'd go back to work. He did that for four years. And he's 82 and he's still here. Yeah. You said that your dad's a man of a few words but when he talks, they're profound. So what words were ringing in your ear when you were driving? I just always remember him saying, kid, they give the recall, you go to the firehouse, you don't go where you think you should, you go to the firehouse, you follow your orders. So do the smart thing, do your job. Yes, sir. And every time we'd hang up the phone, it's fireman talk, he'd say, I love you, keep low. My dad couldn't tell me he loved me until I told him when I first got on a fire upon when I was 22 and my dad grew up in a tough household. My granddad was a good man, but a tormented man. He was sent away from home at 12 years old. He was from Denmark and I'm named after him, Grandpa Nils. And I think his demons took up a large part of his life. His anger, whatever it was, his fear. We got the sense that maybe when he was a child, he was an apprentice baker, living with strangers, working for them. And we think maybe he was abused and that's why he took it out on my dad and my grandma and my aunts. But they made it up to each other at the end of my granddad's life. My granddad turned out to be the best grandfather ever. I think he tried to heal and heal everyone by his change of behavior. So he's proof that you can change, you can improve if you work on it. But I know I'm going off track here, but. But you were man enough in your, you say in your 20s to tell your dad. To my dad, yeah. And my dad, I got on a job. He said, how'd it go, kid? That was the tour, we called it Tour of Duty. I said, oh, dad, it was great, it was great, I love it. And he goes, well, just remember, you keep low, you always keep low. And keep low means you stay down below the flames, if a room flashes over and it's burning, if you stay up high, you're gonna get burned badly. But if you get down on your belly and you crawl, you'll get out. So he'd always say that when he'd hang up the phone. And I said, well, I love you, pop. And he says, well, thanks, kid. I said, well, you can say it too. Oh, nice, you pressured him. And he did, and he said it. And now every time we talk, he says it. So, you know, they talk about masculinity and whatnot. And my dad is one of those tough, tough guys with a soft edge. And that's how he brought me up, you know, to be a protector. I hate bullies. I was bullied really badly as a kid, and I really hated it. And now I find myself sometimes throwing myself into situations to protect people that are being, you know, violated and hurt. And I just can't walk away from it. But that's my dad. My dad was that, you know, just a great guy. But anyway, yeah. You still listen to, therefore, see, you probably went to rush right to the towers, but you went. Yeah, so anyway, I got, I did, I listened to him. I listened to my wife. I went to the firehouse, and it was really strange. It was eerie because the computer dispatch system was still beeping, which meant it sent a dispatch, and the truck received it. Ladder 114, my truck company received it, and they left, they were gone. So it was this beautiful old building built in the 1880s with a spiral staircase, just a narrow old brick garage, and it was empty. And I just heard the computer chirping. And I looked down on a ticket, and it said, Ladder 114, respond, the Vessian West World Trade Center aircraft into building. And I said, oh, God, I just hope they're not on a death ride because this now was two towers, and they were burning. They were free burning, and I knew this was really, really bad. And I got on the phone, and I called command right away. I called the 40th Battalion, and Chief's aide just said, look, get 12 guys. Sign them in to the journal. There's a journal of daily events. Everything that takes place in the firehouse 24 seven has to be logged. And I logged myself as coming in, reporting for duty. And as the guys came in, I logged them in. And then one of our lieutenants took command. We grabbed up a bunch of gear, and they basically told us, get 12 guys, get a city bus, and get down to the battery tunnel they said would probably be closed. There was threats it was going to be blown up to get to the Brooklyn Bridge. And so we did. We got a city bus. We flagged it down, and the bus driver said, I'm sorry. I can't give you the bus. I will drive you. And he took us, and we stopped at Engine 201, which is just about a quarter mile down the road from us. That's our affiliated engine company, and my childhood best friend here, Johnny Schardt, he was assigned there, and he was on shift. And then they went through the tunnel. And we picked up those guys, the off duty guys from 201, and then we kept going down Fourth Avenue, and we picked up 239's crew. And then we hightailed it down the bridge, and there was a lot of traffic. There was a lot of people fleeing, coming over the bridge in waves, so it affected the inbound. What was the mood like among the crew? It was somber, because just prior to getting on the bus, the first tower went down. So we figured that I heard 114, my lieutenant, Dennis Oberg, heard him on the radio. And he said, 114, Manhattan, we're on your frequency. What do you need us? And they said, Tally Ho, which is our nickname. Tally Ho, respond to the Vessian West to the command post and receive your orders. And I heard Dennis say, Tally Ho, 10 4. And Dennis, a little while after that, they were proceeding to go into, I believe it was, I get this mixed up, and I'm sorry. I should know this by the back of my hand, but sometimes it's just such a haze. But the second tower hit was the first one to go down. And they were heading over to go in it. And all of a sudden, he looked up, and he saw what he thought to be disintegration. And he turned the guys around. He said, run. Just run. Don't look back. Don't look up. Go. They sprinted as fast as they could. And they dove under a fire truck. And the guys that were sprinting behind him 40 feet away were underneath a pile that was 10 stories deep. They were killed. And just further into that pile was his rookie son, Dennis's rookie son, who was working in Ladder 105, which was my first command under the department. I worked for it, proudly served for three years. And just aside them was my childhood best friend, John Chard, and his crew from 201. And they were all killed. And a strange irony to that is that Dennis's son, Dennis Jr., was working underneath, under the wing of a senior man, as we say. A senior man is a guy with a lot of experience. And he'll watch over you, make sure you don't veer off, like I veer off a lot in talking. And you don't veer off, and you get yourself hurt. In the morning of 1993 bombing, Henry Miller was my senior man. And I was the young guy under his wing. And he protected me. And toward the end of the day, he looked around. He said, kid, it's a bad day. He said, they didn't do it right. They blew it up in the middle. If they did it in a corner, they would have dropped this building half a mile down at Canal Street. But don't kid yourself. They'll be back, and they'll do it. And they'll do it right next time. And it's so strange and so prophetic, because he was there with them. He died with Dennis. He knew it. And like 1994, we had a training manual with a picture of the towers with a target. And this is not a matter of if, but a matter of when, be prepared. And it's haunting. It was like people knew, right? And we didn't stop it. And so we got off the bus, but just prior to that, coming over the bridge of the second tower, it's gone now. And we're just destroyed, because we're like, our guys are there. They're all in there. Now we're feeling like cowards, because we got there late. And initially, we're thinking there's 500 guys that are gone, because there was a tent alarm assignment, which means 50, 60 fire trucks, five to six guys per, you know, you're looking at. At least there was even more tent alarm, plus multiple alarms on top of it. There was a dispatch, basically equivalent of five to 600 firefighters. We figured, oh, they're all in there, all gone. All the police officers, Port Authority police, NYPD police, court officers just up the street from the courts, transit cops from the train tunnels. Like, just, you know, we knew everybody was going there, and now they're gone. So what you saw, what were we looking at? What did it look like? So you saw rubble, and then you knew that many, that 105 and 201, many of those guys are in the, they're dead. Yeah, and we thought 114 was in there, too. We didn't realize at that point. We didn't even realize that they had gotten under that truck. We thought they were all gone. But yeah, it looked like, it looked like a movie scene with just end of the earth destruction. It's just massive piles of intertwined steel, what was left of the steel. And you know, there was no cement. It was all just dust. And it was just a burning pile of dust and concrete and plastic. And it was just, everything was just pulverized. And it was truly hard to mentally compute that. Like, it was like, what? And then there was just fighter jets, a couple of fighter jets just circling. And you just heard them flying by over your head. I mean, you'd literally see the guy banking a turn around a Brooklyn bridge and just coming back. And I'm like, holy shoot, we're under attack? And we couldn't really get concrete intel as to what exactly we knew planes. But then we kept hearing there was multiple devices. There was devices in a battery tunnel. And there was devices on a George Washington bridge and in the subways. And it was just chaos. It was, I mean, we kept it together, obviously, because that's kind of, we try. That's what we do. But the just constant barrage of different reports, it was like, holy shoot. And then as we were being deployed, it was a little frustrating. But they were trying to take command and send us in groups now because they realized we have to start searching this. You could hear the alarms on the Scott Air Mask, the packs we wear to go into the building. It has a motion alarm. And if you stop moving for 30 seconds, it just sounds like this whining, just screaming bell. And it just keeps going and going. And you could hear multiple units of those going off. And you're like, wait a minute. There's guys with those. Where are they? And it's emanating from underneath the pile. And it was just surreal and truly like a war zone. I mean, I was a soldier in the reserves. And I never saw combat. And I would never claim that I did. But we trained. We trained for a lot of situations. And we trained in real life atmospheres and whatnot. And this was just beyond that by leaps and bounds. It was bizarre. Did you see the towers collapse? As we were coming over the bridge, the first one, as we were deploying from the firehouse, we had a television on. And I saw it go down. And we were so involved in getting gear together and getting teams set up and, OK, you're going to be with these two guys. And I just yelled, there's the guys. And they're looking at me. I dropped to my knees. And I started praying. They're like, what the hell's wrong? I said, I couldn't even say. I was like, 114, they're in there. And they're like, what? I said, the tower's gone. And all you saw on the TV was just this pile of dust. And I guess because they didn't see it going down, they probably thought I truly lost it. And then the realization came. It was like, wow, the tower's down. So now it was like, wow, this is really on. So we just took off and got that boss. And so if you thought many of the guys on 114 were dead, if you thought that, did you think you were going to die? I mean, if you're rushing towards the rubble? As crazy as it sounds, I never thought that the other tower would go down. I said, OK, maybe some freak chance that one went down. But no, the other one's not going to go. They're built so strong. I was in those towers so many times. I mean, I ate dinner up in the top floor restaurant windows on the world. And I'm saying, nah, there's no way. Like, how the hell did this one happen? But I was having a hard time mentally processing that the building was gone. And believe me, if you don't have fear in this industry and police, fire, military, then you're kidding yourself or you're a danger to everyone. I don't care who it is, as tough as they are, this and that. Everybody has a certain level of fear with doing this. And I don't care how long you do it, there's always that chance of something going bad. And everyone who does it has that certain amount of fear. But at that point, it was such a feeling of disbelief that fear wasn't even kicking in. It was just like, what the hell just happened? And I honestly think it was almost like a shock. And it just stayed that whole day. So the building is, before it collapses, is burning. It's just burning. I mean, upper floors, up in the 78th, up to the 80s. And then the way that the cut was from the plane, it wasn't just straight across. It was from the 78th, then on up to maybe the 86th. And then the jet fuel had come down and was burning down. And there was people on the ground who were doused with jet fuel that was already burning. And they were lit on fire on the ground. It was just insane how vast the destruction path was. As a firefighter, what are you supposed to do with that scale of fire? I think the first bosses in, the first chiefs, were just going to do their best to get, as we get hose lines, what our whole theory is, or our tactics is, to get water at the fire, at the base of the fire, and get the truck company, which is the ladder company. They're the guys who break the doors down, put ladders up, this and that, to get them to where the life is most expected and get them out of there. So I think the chiefs tactics at that point was, let me get multiple engine companies. Let me get four, five, six hose lines fighting this fire, this massive fire. And let me get 15, 20 truck companies up there just yoking people out of there. Yeah, but you got to go up the stair. Everything's not working. Yeah, guys had to walk up 80, 80, 90, 100 flights of stairs. And there's audio of officers and firefighters speaking to each other on the radio channels. And unfortunately, at that point in time, we had very, very bad communication system. We'd been fighting for years to get radios that work properly. We couldn't because it was a lot of money. We fought for years to get the full bunker firefighting suits, which is the pants and the coat. We used to have just coats and these roll up rubber boots and guys were burning to death and we had to fight. And unfortunately we lost three guys in one vicious, vicious fire in 1994. And then they finally said, enough's enough. Give these guys the gear. So it's a strange phenomenon in the first responder world and in the military world. It's really one of the most important things that takes place in society. The most pertinent organizations. And we can't get the funding we need. It's crazy. They'll throw money at every nonsensical thing. But when it comes to gear, equipment, protective equipment, trucks, this couldn't get it. Just all the ways you could take care of people. I saw since 9 11, the wars in the Middle East have cost America over six trillion dollars. And the amount of that money that was spent on the soldiers, in this case the first responders, is minimal. Compared to it, yeah. Almost nothing. They, Lex, they closed down. I believe it's either seven or eight. In May of 2002, they closed down nine firehouses in New York City for budget reasons. We hadn't even finished cleaning up the World Trade Center site and they slashed the budget. And still to this day, have not reopened those firehouses. There's a million more people now living in New York City than there were in 2001. And the fire protection is way less than it was. And it's a sin. It's really a sin. Can I ask you a difficult question? So there's this famous photograph of a falling man. So many people had to decide when they're above the fire, in the fire, whether to jump out of the building or to burn to death. What do you make of that decision? What do you make of that situation? Those people who jumped, those were acts of sheer desperation. I've been in fires and just minor burns, but minor in situation. But I've been trapped, caught somewhat. Ended up in a burn center for nothing serious at all. But for those brief seconds, half a minute was, thank God, if I didn't have my fire gear on, I would have been burned to a very, very horrible level. Those people were burning alive. And they had the choice of either to stay there and burn alive or to launch themselves. And some of them, I don't fault them, but they had a few folks, they won't show it anymore because they say, I don't know why it offends some people, but they had a couple folks that took umbrellas and they took garbage bags because they thought that it would slow down their acceleration rate to the ground and maybe, just maybe they wouldn't be killed. And that's, to me, a true sense of desperation for humanity to say, I'm going to die either way, but let me take my chance. And I don't know the exact number of those folks who did that, but our first member of the fire department killed firefighter Daniel Serf, aged 216, was struck by a jumper. And one of my dear friends was ordered to help take him and they knew he was passed away because he was hit by a flying missile. I mean, 120 miles an hour, a body lands on you. Those two bodies are now crushed. And they were ordered to take that firefighter and bring him across the street to Engine 10, Ladder 10. It was literally a firehouse, less than 100 yards from the facade of the Trade Center, from the Trade Center complex. They were literally right there. And there was plane parts that went into that firehouse, landed into the front doors onto the roof, but the building itself was not destroyed. So it was used as a mini command center for quite a while. So my friend was ordered to take Daniel's body in respect and bring it over to this firehouse and give it some semblance of dignity and lay it out on one of the bunk rooms, the bunks we have in the bunkhouse, and just cover it with a sheet and put a sign, please firefighter killed, do not disturb, and then we'll get to him later because obviously this operation is gonna go on for days. And my friend, who's such a great, wonderful guy, is so still to this day, filled with guilt because if they weren't taking his body out with the respect and dignity that they did, it took a while because it's a tough situation. His ladder company was coming over the bridge. There's a famous picture of Ladder 118. You see this tractor trailer fire truck. It's the one when the guy in the back also drives. And it's a zoomed out shot, and you see the Brooklyn Bridge, and you see only the fire truck in the middle, and you see the two burning towers in the distance. Well, his engine company was just ahead of them on the bridge, and the only reason that engine company lived is their initial duty assignment was to take that firefighter and bring his body over. It's like the military. We don't leave anyone behind. These are our guys. As some guys say, it's all about the guy right next to you, and nothing else really matters. When that guy right next to you goes down, it stops. You get that guy to safety, or if he's dead, you get him out. So in that time frame, that saved his life. But that's a heavy burden to carry now for the rest of your life, because you say, if I wasn't helping my dead friend, I'm dead. Yeah. What did it look like at Ground Zero? What did it feel like? What did it smell like? What, you said there was a sense that it was almost like a war zone, but can you paint a picture of how much dust is in the air? How hot is it? How many people are there? And again, how did it feel like? It was just, it was a scene of controlled chaos, controlled because there was a semblance of command, and we were just trying to do our jobs. But it was such a frantic pace because we're now digging frantically, knowing that there's life underneath this pile. And this is throughout the afternoon of that day, the evening. Yeah, I mean, this was nonstop, just nonstop, really, for days. But for my particular crew, we literally kept going. We initially were dispatched over towards number seven, had just gone down, and we were searching the post office that was there. There was reports of people trapped. And we painstakingly searched every single inch of that building to make sure no one was left in there. And then we were deployed to the pile, and the pile is sort of ambiguous because it was just such a vast, vast pile. I mean, it went for city blocks. And we were assisting in the retrieval of two Port Authority police officers. We're lucky enough to survive, but they were trapped. They were deep down into a crevasse, and they had to be physically dug out and extricated. So there was a couple hundred, few hundred guys involved in that process of bringing in equipment, jaws of life, airbags to lift steel, to cut pieces of steel. It was just a huge operation. And we were back toward the logistics end of it, shuttling in gear and bringing in stretchers, bringing in oxygen, whatever was needed. And you were trying to climb over this jagged pile of debris. It wasn't like you just walked 100 feet on a street with something. You were trying to climb over this I beam and then down into this hole and then back up that hole. I mean, just to run one piece of equipment took a half an hour to get 100 feet, 200 feet. You know, mind you, some of these pieces of equipment are 100 pounds, you know, generator for hearse tools, this massive motor on a frame. Unstable ground. Unstable ground, just horrible conditions. Fires were still burning aside you, beneath you. And at one point, I kind of veered off to the side and I was with this other fireman from my father's old ladder company, 172. And it was strange because we were quite a bit down, like 70 feet down into this ravine of debris. And he says, brother, what do you hear? And at the time it was like dust, it was like sand just falling down a pile and it was hissing from gas pipes and water pipes. And I said, I hear the gas lines, I hear the sand, I hear the concrete. He goes, no, no, what else do you hear? And just the side of us was a lady's pocketbook and a high heel shoe and someone's sneaker with nobody with it. And I said, I don't know, I don't hear anything. He says, me neither. He goes, no one's coming out of here. And I said, no, no, no, there's gotta be someone coming out of here. I mean, there's just thousands of people in here and they're coming out. He says, brother, we would hear them calling for help, they're gone. And I still at that point thought there was a chance. And after about the fourth day, they just said, this is a recovery now. There's no more life, there's no more chance. And then that first night we went full tilt till my crew, my specific crew of 12, 15 guys. And four in the morning, we just couldn't breathe anymore. We couldn't see, we were caked just with, it was like if you took flour and just kept dousing yourself. And the Lieutenant just said, look, guys, we're gonna go back, we're gonna get some medical aid and then we'll come back in a few hours. And we took a city bus back through the battery tunnel and unbeknownst to us that morning, this off duty firefighter, Steven Siller from Squad Company One, he raced down there with his pickup and he couldn't go any further because the traffic was stopped up because they had a report of a bomb. So everything was held up and he grabbed his fire gear and he put it on, stuff weighs about 60 pounds. And he ran through the tunnel. Two and a half miles, got to the end of the tunnel, fire truck was coming in from the other way. He hopped on the back, got him up to West Street, jumped off, tried to look for his company, where they were and he was never seen again. He just ran through the tunnel. Ran through the tunnel and he got there to help his team, right? It's all about the team, it's all about the guy right next to you. And he's the Tunnel to Towers Foundation, Steven. His brother Frank decided in his name in perpetuity, he's got a fund that now builds a home for every Gold Star family, for every seriously battle wounded warrior, for every seriously wounded first responder or killed in a line of duty first responder. If they had a home, they'll pay the mortgage. If they didn't have a home, they give them a home. And especially if it's a severely battle wounded, they give them a smart home because these poor guys come home with no limbs. And so the beauty of Steven and his selfless act was that he's now helped thousands and thousands of people. I mean, the Tunnel to Towers is incredible. That's part of our mission is to bring awareness to these great people at Tunnel to Towers, what they do. They've raised $250 million to help protect the protectors, to rescue the rescuers in a what's become, unfortunately, a somewhat ungrateful society. But they will not forget these great guys. So you tell Steven's story. He's one of the 20 people that you talk about in the new Iron Labs 20 for 20 podcast series. If you can just linger on his story a little longer, what does that tell you about the human spirit? That this guy, the Tunnel couldn't drive through, so he just puts on that heavy pack and runs. What do you make of that? That shows the depth of a man's soul. He didn't have to do that. He could have turned around and went home to his family, and nobody would have shamed him. But he's one of those beautiful, brave people that take a job that really doesn't pay a lot of money. And you become a cop or a firefighter or a nurse or an EMT or a medic or a soldier or a Marine or airman, sailor. When you take these jobs, you don't do it for fanfare. You definitely don't do it for money. I mean, those 13 brave souls we lost a week or two ago in Afghanistan, they're brand new soldiers and Marines. They make $22,000 an hour, but they don't work 40 hours a week. They work 80, they work 90 hours a week. So they make it about six bucks an hour. And you know what? They sign off. And firefighters and cops and medics and EMTs, nurses, emergency room doctors, they don't really make a lot of money. I mean, they're starting salary right now for a New York cop. I was a New York cop for two years first. I made 12.25 an hour back in 1989 to get shot at during the crack wars. If you made $11 an hour with a family of four, you were entitled to welfare back then. So I was just above the welfare level, risking my life. And these are the guys that are getting ripped up now. Right? And look, I won't get into any politics, but like that says something about someone's soul that they're willing to take a job like that and get now, get zero respect. So a guy like Steven, what that shows is the depth of that man's soul and courage and determination. It's hard to be selfless in this world anymore, but I still know a lot of selfless people that just put on equipment every day, bulletproof vests, fire bunker gear, stethoscopes, you know, flak jackets, military helmets, and they go in and they do it smiling. That young Marine that passed last week, she was photographed and quoted as saying, I have my dream job, but she was holding a little Afghani baby. And she was dead a few days later. She was so thrilled to be making $7 an hour helping people, right? Isn't that huge? Like that to me says, that's a true sign of character right there. And it's important for our society to elevate those people as heroes. Let me ask you about firefighting. What do you think it means to be a great firefighter and a great man, a great human being in a situation like you were in in 9 11? You know, that's kind of a broad term. Like some, you know, you can go to different firehouses and they might have a different definition of what they consider a great firefighter. But I think in the industry as a whole, if you're willing to put everyone else before you, especially your team, you know, as we say, there ain't no I in team, right? It's T E A M and there's no I in there. It's all about those guys and girls next to you. If you can do that, that makes you pretty great. You put everything else second and you just run in and you run in with that team for strangers. You know, I've had the honor of, I spent almost 25 years of my adult life serving humanity, my country, my former city. And the people I worked with were giants. And I don't mean that in height, I mean, but I mean that in spirit and in soul. I saw some of the most heroic, selfless acts. And then I saw some of the behind the scenes that were so impressive. You know, we'd go to the movies, so impressive, you know, we'd go to a fire around Christmas and the family would lose everything. And even when I was a cop, same thing, you'd come back either to the police precinct or the firehouse or the EMS station. And someone would put together a collection and say, hey guys, hey Lex, 50 bucks a man, you know, the Smith's down the street, just lost everything, we're gonna go get some presents for the kids and some turkeys. And not one of those guys questioned that. And they were making 12.25 an hour and they still came up with 50 bucks for that family. But see, that's the stuff the press won't show you, right? They don't wanna show that humanity, that soft edge. See, when you're a warrior, you need to have this rough shield, this rough exterior. Cause if you don't, you die. But a true great firefighter or responder or a cop or military personnel, they have that rough exterior with that soft underbelly, that heart, right? And that's, to me, the true great ones. Some of them, they just have a hard time doing that. There's no shame in showing your soft side. Well, you got your dad to say I love you back. No, that was huge, man. That took me 22 years, Lex. So you were a firefighter for 21, I was 22 years. Why did you become a firefighter? Oh, my dad, I mean, I was five years old and I went to his firehouse and there was these, at the time, they looked like giants to me with mustaches and the trucks smelled like smoke and the gear smelled like smoke and the tires and the diesel fuel and that one was like, this is what I'm gonna do. And then they bring you in the kitchen and they stuff you with ice cream and cake and everything. And then I go home to my mom, shaking with a sugar cone and she's mad at my dad, but yeah, it was just, oh, I was like, I gotta do this. It was like, they were like a baseball team in a garage with a truck and these big tools and big coats and helmets and they were just laughing and having fun and I'm like, yeah, man, I'm doing this. And I knew, I was obsessed with it. I mean, I was so pissed that the fireman's test came out when I was 14 and I couldn't take it, you had to be 18. And it was done, the test was graded and whatever. So my dad, now there's a copy circulating because it's old now. And he goes, yeah, yeah, this is what you're in for. And I took it and I did it like it was real and I got a 99 and I was so pissed. I said, I wanna get hired. He goes, you can't, you're 14. But I just wanted to do it so bad and I just wanted to help people. I just wanted to be like my dad, he'd come home smiling as tired as he was and he fought fires in the 60s and 70s when the city was burning and he's still as exhausted as he was, he'd still be smiling. I wanted to smile at work and I used to, I got paid to laugh and joke. I got paid to cry sometimes. But man, we laughed a lot. We really, it was, the chop breaking is just, it's just unending and it's great. If you don't mind, can you tell me, you were really kind enough to give me one of these shirts with 114. Can you tell me the story of 114 and Tally Ho? I wear proudly, I served eight years in that command and I didn't finish my career there. I passed the lieutenant's test and once you do, you have to leave. The story behind Tally Ho is back in World War II, there was this gentleman named Bad Jack Carroll and Jack was an airborne ranger and my father in law was also on the department and he knew Jack. And Jack came home, Jack jumped Normandy and stormed up through the Battle of the Bulls in Bastogne and he came back, greatest generation as they all did and they got jobs and they went right to work and they were treated better back then, vets, right? And he got on the New York City Fire Department and he got assigned a ladder 114 and they first got radios back then and when Jack, he would drive the truck, you're up there with the officer, either the lieutenant or captain, so if the boss is off the truck, you operate the radio for them as the driver. So when they called him and they'd say, you know, ladder 114 responding to 52nd Street, 3rd Avenue, Structure Fire, you're supposed to get back and say, ladder 114, 10 four, but he refused to do that. He'd say, ladder 114, Tally Ho, because that's what they'd yell when they'd jump out the plane. So all these years later, it stuck and it's a little bit of a bragging right, but out of 350 engine and truck companies in the whole New York City Fire Department, we're pretty much the only one that's called by their nickname on the radio, not their number. So it tweaked some guys off in other places, you know, they may F you, Tally Ho, but it's just, yeah, it's a great, great heritage and we're really proud and Shamrock was, he was Irish and a lot of the guys back then were Irish immigrants from the area, from the neighborhood, and they would actually take the fire truck to church on Sunday and park out front and one guy would stay in it to hear the radio in case they got a call. So yeah, that's the proud history. And you said that if I wear this around New York, am I getting a little bit of? You might get a guy from the Bronx, go, hey, Tally Ho, screw you, you know? But I mean, it's all that good rivalry, you know? We like to, you know, we like to kid each other back and forth, you know, guys from Manhattan, we'll say, yeah, you guys are in Brooklyn, yeah, short buildings, tall stories. And they're like, yeah, you guys are in Manhattan, tall buildings, no stories, you know? Like it's just all that jocular ball break and it's good stuff, you know? Let me ask, I guess, a difficult question. If you just step back on the events of 9 11, on the side of the people that flew into the towers, what do you take away from that day about the nature, about human nature, about good and evil? How did that change your view of the world? I witnessed evil firsthand. I remember later on, well into that night when we were trying to help get those police officers out, I remember looking up at the building, Century 21, the store runs along the east side of the towers and it was still there and the debris had come down right almost to the edge. Century 21 is this old storied department store in New York City and the sign was there and it was still lit up, like some of the neon was broken but I think some of it was actually still lit up and I just looked around and I was like, this is a war zone, like we're at war. And we knew we were attacked, we heard the fighter planes and back then it wasn't the extensive communication network and we had cell phones but they were the old school flip phones and there was no news on them and so plus we didn't have signal down there anyway. I couldn't reach my family for like 12, 13 hours and my dad had deployed down to the ferry terminal to retrieve bodies. He was retired but he still went and they deployed him to go be basically the morgue transport guys. They expected to be sending hundreds and thousands of bodies across on the ferry and they set up these tractor trailers as a mobile morgue and that never happened because there were no bodies to take, they were all buried. So I saw evil firsthand, I don't know how someone can inflict such revenge or a vengeful act in the name of anything, in the name of a religion, in the name of a cause, in the name, like what the hell? Were you ever able to make sense of that, why men are able to commit such acts of terror in the days and the years after? No, Lex, I haven't. My mom's from Ireland and I still have a lot of family there and my great uncles, one of them was dragged out and shot. He lived but just based on a rumor that he was in the IRA and I wasn't happy to see what happened to my mom's people because they were victimized and brutalized by England at that time. But blowing up bombs and killing innocents in the name of that, it doesn't make it right. I couldn't justify something like that. I can see, I was a cop, I was a soldier and you never wanna take life and those jobs but sometimes you have to. But you don't do it with a vengeance, you don't do it with a thirst, you do it because it's necessary for survival. When you do it out of a bloodlust, out of a thirst, out of a cause, that's evil, there's something wrong with you, I have no, I respect life to the highest level. I mean, I'm very, life is sacred to me, it's precious, it's beyond, it's not a commodity, it's a gift. But to take life just so randomly, so there's something way wrong with that person and maybe I'm a conflicted soul but I would have no problem seeing someone like that put to death because they do not deserve life. There's many children around me and many children around this world that are being taught to hate someone who's different than them just because the person who's allegedly teaching them says so. I don't understand it. Well, that starts with just having a basic respect and appreciation of other human beings and that starts with empathy. And one of the reasons I love this country, while joking that I'm Russian, maybe you could say the same as you being Irish, you're actually truly an American and that's why I consider myself very much an American. And one of the reasons I love this country is it serves as a beacon. I still believe it serves as a beacon of hope and that empathy and love for the rest of the world that hate is not gonna get you far, that love will get you a lot farther. And I still think sometimes it's easy to see the press, mainstream media, you could see social networks. Because you can make so much money on division, sometimes because it makes so much money, it's easy to think like we're really divided. I honestly don't think we are. That's just like the very surface level thing we see on Twitter and so on. It's that you're 100% right. There's people out there that are maximizing off this whole division, right? They want us divided, they want people angry because it sells. A lot of these people that are in charge of certain organizations, well, they all seem to have nice cars and nice houses and nice vacations and they're constantly trying to convince everybody that we hate each other. To me, I'll use a fireman analogy, right? It's like a little campfire. And if you just let the embers flutter, they'll go out. But if you take a little cup of gasoline with those embers, boom, it'll blow right up in your face. And that's what a lot of these politicians and a lot of these media folks are doing because there's something in it for them. And I think it's possible to defeat them with great leaders, with great spokespeople, with great human beings having a voice. One of the powerful things with the internet is more and more people have a voice. And I ultimately believe, certainly in America, but in the world, the good people outnumber the assholes. Oh, I agree. And there's days when I think the assholes are overrunning us, but you know what? I think what the downfall of the world is is ego and arrogance and people that think they're better than that other guy. My parents raised me to be this way. My mom is such a sweet, gentle soul. She's an immigrant. She came here at 16 years old. She helps everybody but herself, right? She's just one of those people. She's sick. She's got Parkinson's. You'd never know it. And she's still flying around her condo complex helping everybody because that's what she does. She loves to help people. But she's been in their shoes. She's been poor. She's sick. Her husband was sick. She's had all sorts of suffering and loss in her life. My granddad died when my mom was 10 and she was one of 10 children that survived out of 14. She knows hard times, but she so appreciates the good times and the goodness of this country. You know, the fire department and the police department, military, it taught me a lot about empathy and trying to really feel for someone and put yourself in their situation. I remember years back, I was a much younger fireman. I was probably five years on the job. And I was sent down to the next firehouse over to fill in. You know, we would get sent around randomly when they needed an extra guy. And someone came banging on the firehouse door and in the tenement apartment next door, they said there was an older woman that was unconscious. So we dispatched ourselves and we ran over with a medical kit and it was an elderly woman laying there on the bed. And she was obviously not breathing. She was obviously in cardiac arrest and an older gentleman that was holding her hand, just inconsolably crying. And it turned out it was her husband and they were married for 65 years. And normally we would just respectfully ask the family members to just step aside and let us do our work. And I realized that he wouldn't leave her side. So I kind of gave the crew a wink and they were doing CPR on what they had to. And I just let him keep holding her hand. And I said, sir, if you, you know, could you just come over just a little bit so we can work? And I held his hand as he held hers. And I said, sir, I said, do you have faith? And he did. And I said, would you like to pray with me for your wife? And he said, I would like to. So we said the Lord's prayer and you know, I just asked God to protect her and bless her. And I think he realized that she didn't have a chance, but we still gave her that chance. And we, you know, got her in the ambulance and maybe it was wrong to try to make it look like we could save her, but you know, you can't really not try. But the one beautiful moment was he thanked me and he was almost okay with it at that point. Like he wasn't as upset. He wasn't as distraught because I tried to just humanize that situation of what we were trying to do. We were trying to do our best, but we also tried to be compassionate to his sadness. And it just, I walked away just feeling so good, even though it was a tragic situation. And she did pass that, you know, he came by to, you know, thank us days later and just heartbreaking. But you know, there's just, it's just happens many, many times throughout the country every day. People get that opportunity as a responder to be that last bridge to the family and the loved one. And you only get that opportunity once sometimes and you really have to, to me, it's like your moment to shine. You know, you could just be very, very dismissive and very rude, or you could be compassionate and just show, hey, I have a mom, I have a grandma, I have, you know, and just in your mind, pretend that that's who you're working on and that's who you're with. So that moment of compassion, that moment of empathy, even if his brief can be the thing that saves the person from suffering, make the difference between suffering and overcoming in the face of tragedy. Yes, like I felt that even though obviously his loss was still huge, it just made it a little more bearable and, you know, tried to just take his grief down to a lower level and it made me feel, just feel really good about doing it. That's a powerful way to see the job of a first responder. Of course, you have to deal with certain aspects of the tragedy, but it's to provide somebody with that moment of compassion. Yeah, and you know, I made it a little habit because sometimes with faith, it's a little bit of a tricky subject. So every time I had someone who died, which unfortunately was many, many times, I would just touch their hand and just say a little quick prayer and just say, look, you know, I hope you're moving on to a better place. I hope if you did have faith that it's strong as you depart and if you didn't have faith, I hope maybe at your last moment that you found some and you just found some closure. So that was just my little ritual, I think. I just, you know, I felt it was important that that person, even though they were a stranger, just had someone there just sort of hoping for the best for them in their last moments. You mentioned cancer. You had a rare leukemia due to all the work that you did at Ground Zero. Can you maybe talk to the experience of just breathing through those days and what that was like, being unable to breathe, being overwhelmed by all of the dust in the air? Yes, the first day especially, we didn't have equipment. We didn't have breathing apparatus and then we were handed little 69 cent hardware store dust mask, you know, it was a little thin paint mask that would just get sweated up and sticking to your face within 30 seconds. So you would just, they were useless. And what you wound up feeling like was that you swallowed a box of razor blades because there was glass and there was cement and it was just so caustic. And I remember that night, you know, when we went back just to get some medical relief for the few hours, we were walking up the hill to the firehouse because they dropped us off like a block away down at Engine 201's quarters and one of the older firemen as we're walking up the block, we're all struggling, we're all having a hard time breathing and just, I mean, I felt like I was dying, literally, it was pretty bad. And I just remember the one guy going out, we're all dead. And I said, no, no, we made it, we made it. He goes, no, you don't get it, kid. He said, we just breathed in poison after poison for hours and then that went into days and then went into months. He says, we're all dead, man, this is gonna take us all. And I thought he was crazy and then now years later, like starting in 03 or 04, guys just started coming down with these really rare and advanced cancers and then it just stopped being a coincidence with the number of guys and they were young, one of the first guys, John McNamara, he was 33 or 34 and he came down colon cancer and it took him quickly in 2000, he was in 2005. And I kind of said to friends and family, I said, I feel like I'm running through a minefield and I wonder when I'm gonna step on my mind because everybody's gonna get sick. And I wasn't feeling well from 2008 on, I couldn't put my finger on it, but I just wasn't right. And in 2011, I failed my medical, my bloods came back horrifically wrong and they pulled me off the truck, but they strung me out for a month, the doctors in the fire department, one of them said my spleen was engorged because I was probably drinking myself to death, like as he said, most of the guys did after 9 11, which was pretty wrong of him and stereotypical, just to stereotype and to categorize and guy couldn't have cared less, he just, he was so crude and nasty. And then my one doctor who was my doctor on the outside, my blood pressure was 240 over 140, my spleen was about to rupture, she didn't even show up for my appointment and I went down, passed out, the paramedics responded, she got into an argument with a paramedic because for big ego and basically telling him there wasn't really anything wrong and he's looking at my paperwork going, this guy's got leukemia and he overrode her, he raced me out of there down to Brooklyn Methodist and the doctor, the charge physician, the ER physician, he says, you're not leaving, you're in a bad way. And I said, what is it? He said, I need a little while to figure it out, he goes, but you probably have one of a few different types of leukemia, he said, I'll drill into your hip, take your marrow and find out. And he said, but in the meantime, we'll get the swelling on the spleen down, I guess some sort of rapid medicines and whatnot because my spleen is about to rupture. I had no blood platelets left which is your clotter so I basically would have bled to death and I found out from my team of doctors that I had about 48 hours to live and that really set me off, I was infuriated because I was telling them for a long time that I was sick. The doctors failed you, the few doctors in the beginning failed you. I felt very betrayed and other guys had died and I had it out with that one doctor, I basically told her she was fired from my case and she's pretty politically in charge person and I didn't care, I jeopardized my job for it because it was my life and I got the sense that it didn't really matter to her. She didn't have any empathy, as you say. It was exact, so why for her, why for a few others, was there not a special care, a special compassion for, first of all, all humans, but human beings in your position, especially a firefighter, a first responder? You know, Alex, I think what it is in the department, their title is just to get us back to duty as quickly as possible when we are either injured or sick because what happens then is your replacement is now in overtime so you're out being paid on medical leave but then they need to replace your spot and then that costs more money. So I think it just behooves them to get as many personnel back and especially during the summertime, they look at it like, oh, maybe you want a few extra days off to go to the beach and this one doctor, he tipped his hand back as if I was drinking an alcohol beverage, he says, hey, busy summer? Because I asked him to look at my spleen which was sticking out of my abdomen like a football and I said, excuse me, sir, I said, how dare you assume that I'm abusing alcohol because alcohol abuse sometimes will present itself as the spleen is engorged and having an issue. So he automatically just assumed that that was my situation, wouldn't even give me an exam and I was horrified. I was so angry, I mean, I wanted to punch this guy out and I literally was screaming at him and an executive officer came in to diffuse it and sent me to another doctor and when I showed her my paperwork, she was horrified. She was like, what did he say? And she said, oh, okay, go to your regular doctor tomorrow who was one of the department doctors and it was just an indifference. It was like, I don't know, I was shocked at the lack of compassion but you know what, that being said, I'm past it, life moves on. The team of doctors, I ended up with a Methodist and my subsequent oncologist, Dr. Peter Menzel, world class, just incredible human being. My Dr. Pete is just, I love him. I just, I love him like a friend, like a big brother, like a father, like my primary oncology care nurse, Mike Nunez, was just an incredible human being and he knew I was frightened because I had to get two and a half years of chemo compressed into seven days or I was dead. And these massive bags of chemo that never stopped and they burned, the minute they went into your body, you felt like you were burning to death from the inside out. And Mike, when Mike came in to hook me up, he said, look, I have to wear a hazmat suit. This stuff is so caustic that if it drips, it'll burn whenever it touches. And I was like, but Mike, you're gonna put that in my body. How the hell is it not gonna kill me? He says, no, no, this is exactly what it's supposed to do. Trust me. So when he prepped the IV tube to get it flowing, it spilled onto the tube and the tube started to smoke and burn. And I said, no effing way, Mike, you're not putting that in me. No way, no way. And he goes, listen, let me get another one. Let me start it over. And here he is wearing a hazmat suit, looking at me and I'm going, this is insane. And he goes, he looked at me, he took my hand and he says, Nels, if you don't take it, you're dead. He says, you got those three kids. I'm sorry, I have no other option. You're dead. And I said, all right, Mike, okay. And he hooked me up. And you know what, it was like, you know, if you do drink alcohol and you have like a shot or want, you know, strong type spirit and you start feeling that burn. Well, the minute he hit me in the vein, it just started going up my arm, burning and then up my shoulder, across my neck, into my head, across the rest of my body, within a minute down to my feet. And I was writhing in pain for seven days and I was praying to die. I was the seventh rescuer in six months to come down with the rarest leukemia there is. There's only 500 cases in all of North America a year. And seven of us came down in six months. Two guys died during treatment. Seven responders, police, fire. Two guys died in the first couple of days of the treatment because it's so vicious, your liver, your heart, your kidneys, something will fail. And I was praying and I was praying, but I wanted to die. I was in so much pain. And I wouldn't take a painkiller because I know people with some issues and I just didn't want to go there. And finally on the last day I gave in, I said, please, I can't do this anymore. I was literally like jumping out of my skin and they gave me something. But it had burned out my mind, it burned out my body. I couldn't hear, I could barely see, it was vicious. But it worked. And my nurses especially, they just, they were so dedicated and devoted. And I was not an easy patient because I was in a lot of pain. It was bad and it was, drove my friends, my family crazy. It was just, it wasn't good. But on that first night I had a quick vision of all these people that I loved that were dead, that died. A lot of them in a trade center and I saw Johnny, I saw friends I grew up with. The last one was my mother in law who had passed six months before and she died of, she was in a coma, she had a stroke. She had a horrible, horrible last six months of life and it wasn't fair because she was so religious. She went to church every day, devout Catholic woman. And all of a sudden I see her and she's smiling and we used to talk a lot, it's the Irish thing, like the gab, the gift of gab. And she used to call me her boyfriend because we'd sit and talk for hours and talk about books and about movies and about food. I loved her, she was my friend. And she'd say, you know, my boyfriend's here. And all of a sudden she's smiling and she goes, hi, my boyfriend. And I says, Nan, Nan, what are you doing? She goes, he's not ready, he doesn't want you. You gotta go back, you got things to do. And I'm like, no, Nan, Nan, it hurts so much. Please, please take me and she left. She goes, no, no, not yet, I'll see you. And she just faded away. And one of my doctors on my team, she had a problem with religion. And that's okay, I understand that. I'm not a preacher, I have a faith, but I don't preach it, I don't push it. I just live and let live. So she sent in this shrink to see me. And I was messed up from the chemo, but I knew what I was seeing, I knew what I was saying. And he was a Jewish gentleman. He was a rabbi also in a synagogue. And I actually had responded in that district and he knew 114 would run into Borough Park. Oh yeah, I see Tyler, oh, they come down the street. And he asked me to tell him the story and I did. And he started laughing and he scared me now. I says, Doc, am I really crazy? He said, no, no. He said, I believe you, my friend. He said, we share the same God. He goes, we work in the same corporation, but in different departments. And he says, you did see your mother in law. He says, your faith is that strong. He said, I've had many patients express the same sentiments. He said, so I want you to listen to her and fight and be strong. And he said, so what else do you want to talk about? I said, well, I don't know, Doc, am I that messed up? He goes, no, no. He goes, they're paying me for an hour. It only took 20 minutes. So we watched the Yankee game together and that's less. But it was just, again, it showed the human condition. Here's these two men of two totally different faiths. And yet we shared that bond of faith. And he had empathy and he had sympathy. And he saw me in many other patients. So he just didn't assume. And he gave me a fair shake and I will always be grateful to him for that. Through any of this, the pain you had to go through with the leukemia, but also the days of 9 11 and after, did your faith get challenged? You know, Lex, it was strange. It was times I was so angry. You know, there's that range of emotions, the anger, the denial, the depression, the this, the that. And this is the weirdest thing. It was mostly, I knew my career was over and they retired me out of the job. That, I got sick in August and that October, they told me I was out. And by the time I was processed and, you know, used up my leaves and whatever you want to say it was, I was officially retired in January of 02 and it was less than six months. And I'm there walking my dog one day, my rescued Greyhound who I miss. She was such a soul. God, she lived to be almost 13, Katie. And we were walking in the snow and I got the call. I was retired and I looked at her and I'm like, Katie, what am I going to do? She just looked up and said, we're going to go on a lot more walks, you know? And I was so sad and I was so sad and I was so angry because I lost my priesthood. I loved helping people. I really, like I would have done it for free. I would never tell Mayor Bloomberg that, right? He's all about the buck, right? But like, you know, honestly, I would have been a New York City fireman. I would have paid them to do it, you know? And I wasn't allowed anymore. That's it. You have over 20 years and you have cancer. You know, back when my dad got sick, they'd let you hang around for 10, 12 years in an office, but not now. Now it's all about the bottom line. But I was more depressed about losing a job than almost losing my life. Like, as crazy as that sounds, you know? And it just... It was more than a job. I mean, it's a way of life. Oh, man, yeah. It also is your family, your father, your carrying torture, your father's... Oh, my friend. I love my friends. I love, we worked 24 hour shifts together. You cook, you clean, you break each other's jobs relentlessly. I mean, I love those guys so much. I mean, I hope that my kids and anyone that I know and care about, I hope they can experience the bond of that brotherhood that I experienced in my life. It was so... God, I would give anything to have it back. Just, yeah. Can I ask you about New York? So unfortunately, I've never lived in New York. I visit. I've always wanted to live there for a bit. Obviously, it's a very different experience to have really lived in New York for many, many years. But there's a few friends of mine that are from... They got similar accent as yours. Yeah. That are a little bit saddened. Perhaps it's temporary, but perhaps not. They don't seem to think so of what New York has become, especially with COVID. It's losing some of the spirit of New York. Do you have that sense? Do you have a hope for the city that has been so defining to what is America? My heart's broken. I had moved to New Jersey many years ago, but I still have a close attachment to New York. My parents are still there, many, many family members. And I've since now moved to Tennessee. I needed to go somewhere quiet. I wanted to heal my fractured soul. And I'm in the middle of a beautiful farming rural area in middle Tennessee. And so they probably called me a sellout back in New York for leaving, but it's not the same city and it's sad. I'll refrain from the politics and the finger pointing, but it's a mess compared to what it was. And I did Broadway theater security for many years, and I started to see it slide like with stuff that was happening, like public urination and defecation and just like tourists don't wanna see that, right? And I had an unfortunate incident two years ago. I was jumped by four teenagers coming off the subway and they were pissed off because I was wearing an American flag hat. And I don't know, I'm not really sure why, but it left me, I got out of it, okay. But I was taken back. They were literally videoing it and the kid was just throwing shadow punches at my face wanting to beat me up. And I finally looked him in the eyes and I was like, oh boy, I'm a little too old for this. Body's a little broken down for chemo. And I finally just said, all right, all right. I just had enough, I wanted to go home. Just worked a 17 hour shift as a stage hand. And I was so taken back, I was so insulted. I'm saying, I spent my life protecting this city and now I'm getting attacked like for nothing. And I just, I gave up and maybe I should have given it a little more time, but it's, I don't know, it's turned into an angry place. It's turned into, I think there's a lot of people that aren't getting the resources they need in a sense. There's a lot of mental illness. There's a lot of homelessness. There's a lot of violent people just roaming around the streets and it's not good. It's not safe. And tourists are not gonna come back. Even just leading up to the COVID, I had some tourists saying to me, I won't be back. And now I can only imagine that it's just gotten exponentially worse, but I hope there's a chance it'll swing back. Cause it is, it's the gateway to the world. I mean, my grandfather came from Denmark. He landed in Ellis Island in the twenties. American success story, 25 bucks in his pocket, didn't speak the language, had a sponsor family in Bay Ridge, Brooklyn. And he made it, you know, he ended up dying owning a bakery at one point and then an apartment building. And he did pretty well for himself for an immigrant who was poor. And my mom, my Irish mother landed in the same neighborhood, Bay Ridge, Brooklyn, 16 years old. Worked as a cashier 50, 60 hours a week in the supermarket and finished school at night. Married my father, the fireman, and, you know, lived the American dream. And it was all, it was all from New York. And my father's mom was from Irish immigrants and they all landed in Ellis Island. Well, my mom didn't cause it was closed at that point, but it's, there's people breaking down the doors to come to this country, right? There's no one breaking down the doors to leave. And this is, this is a problem I have with people that aren't grateful for being here. And this, again, it's not political, just straight down the middle fastball. If you don't like it here, I'll show you the door. I'll get you the plane ticket. I mean, would you want to live back in Russia compared to here? Would you, you might because of family ties, but I mean, if you had no ties to Russia or would you want to go to China right now and possibly end up in a labor camp or, right? There's people busting down the doors to get to this place. It's not perfect. It's got its flaws, it's got its blemishes, you know, but it's a damn great place. It's the best country in the world. Yeah, and some of it, so first of all, I have hope for New York. I think that culture is very difficult to kill. I think it will persevere. And I think ultimately the same story with New York as with the rest of the United States, it has to do with leaders. And I'm always hopeful that great leaders will emerge. I agree. And the kind of leadership we see now and the kind of conversations we have now, I think it has to do with the prosperity and comfort. And in the face of hardship, I think great leaders will emerge. And yeah, I just think ultimately in the long arc of history. Well, leaders shouldn't become rich. They shouldn't become rich in the process, right? You shouldn't go into political office as an alleged lunchbox kind of guy and then come out eating at the best steakhouse in the world. I mean, that's the problem with politics, right? My Irish grandmother, God rest her, used to say, oh, those politicians, they're all like dirty diapers. They're full of shit and they stink. And it's true. I don't give a crap what party they're in. Yeah, greed and power. We had to beg these guys, beg them for federal legislation to cover our medical bills, right? There's a gentleman, John Field from the Feel Good Foundation. This guy is a lion of a man, a general, but with a soft, big, great heart. And John is a former construction worker who came to the 9.11 site the day after. He was one of those guys cutting the steel with torches and craning it out of the air. One of those hard hats that just, that never got the credit and the praise that we did as responders. And I don't mean that as a knock to responders, right? I mean, we lost 37 Port Authority police officers, 23 NYPD officers, about a dozen emergency medical technicians and paramedics, three court officers from New York State courts and two federal agents, and I hope, and 343 New York City firefighters. We lost a ton of responders. But the recovery workers, thankfully weren't killed in that process, but there's hundreds of them now who are dead from illnesses because they came down to recover our people and the civilians and the poor lost souls that were killed at work that day. And John literally almost lost his foot in a construction accident at the site. An 8,000 pound I beam tore off half of his foot, ended up with massive sepsis, six months in the hospital, hundreds of thousand dollars in medical bills, and then no one wanted to pay him. So here's a guy, he's gonna lose his house, lose his life, lose everything. And now the never forget, it started quick, right? And he went on a mission, formed his Feel Good Foundation. His last name is Feel, F E A L, Feel Good Foundation. And this man literally went to Washington, DC with his army, as he called it. And I was honored and blessed to be with him a couple, only a couple times. I wish I had dedicated some more time to it. And what it was with John is he set out on a mission to get, and initially what he did is he got funding to take care of responders who were in that limbo, who couldn't get their medical bills paid, who couldn't make their mortgages, who couldn't make their car payments, who couldn't make their childcare payments. And John just took it upon his own to get donations and take care of you while you were suffering, right? I got a call when I got out of hospital. You okay? You need anything? I said, who is this? It's John Feel. I said, aren't you that constructor? Yeah, you need anything? I'm pretty good right now. I said, I appreciate it. Phone ring again a few weeks later. Hey, John Feel, you need anything? I'm like, this guy's incredible. But there's people who needed stuff and he was getting it done. And he, with his army, had to chase these politicians through the halls of Congress to get funding to cover the medical bills. I was getting sued for $125,000 for my month stay in the cancer ward. And I couldn't believe it. I said, well, wait a minute, I have insurance. They're like, oh, no, no, this is terrorism related. We don't cover that. So usually then workers comp will cover your on duty injury or illness. Oh, no, no, no, leukemia is not covered under that. We don't cover that. So then the ping pong game starts and I'm literally have people showing up, taking pictures of my kids in front of the house. And I went and grabbed the guy one day by the collar. So who the hell are you? Sir, I'm a private investigator. We're putting a lien on this property due to a nonpayment of a bill. I said, okay, I understand. Do your job. Let me bring my kids inside. Take all the pictures you want. Don't step on my front lawn. And I went in the house. I closed my room, my door, my door, my room, and I cried. I said, I can't believe this. I spent my entire adult life trying to help people, give of myself, and I can't even get my medical bill paid. Well, John Field got my medical bill paid. He finally got these politicians with his team, firefighter Ray Pfeiffer, who has since died, fought with terminal cancer for nine years in a wheelchair. Literally at the end, came out of hospice to go finalize getting us this coverage. Detective Luis Alvarez, who testified days before he died in front of Congress, and a bunch of other guys that were really, really sick, and we had to shame these people into signing on. And luckily we had John Stewart come on and literally just hound these guys and shame them and embarrass them. And what it all stemmed from was in 2006, the first death that was determined to be linked to 9 11, there was others, but the first one that was officially linked was a New York City police detective who initially, the city said he died of advanced lung disease. His lungs were protruding out of his body. And he was on painkillers and it was so bad at the end that the doctor said, just grind them up, snort them, drink it, whatever you need to do to get instant relief. So when they found the talcum from the pill lining in his lungs, they said, oh no, this is opiate abuse. He didn't die of lung disease. So they said, and the mayor was quoted as saying, he is not a hero. Well, shame on you, Mr. Mayor. He was a hero. And his father, who was a retired police chief, married up with the Feel Good Foundation and John Stewart and Ray Pfeiffer, Detective Alvarez. And they got us all covered. But it took so long. Like it was so heartbreaking. These people who were lining up three deep politicians, three deep to catch a picture with a responder so they can tweet, hashtag never forget and hashtag look at me and hey, how am I doing? All that bull crap. They were nowhere to be freaking found. I literally witnessed them hiding in cloak rooms, running down hallways away from us, those freaking cowards. That's cowardice. Can I just linger on the John Stewart thing, the comedian, actor, John Stewart, his testimony before Congress over the benefits for 9 11 first responders. I mean, there's a lot of important human beings in the story, but he has a big voice. And he spoke from the heart. What do you make of that testimony? Oh, it was heartfelt. I mean, he spoke. Look, I mean, John was a polarizing guy, right? There's certain things like over the years, he was cutting edge and I might not have agreed with all of his, you know, some stuff, some not, right? You know, like we all, but I tell you, I found him as funny. I enjoyed his humor. I would love the two of you to have a conversation. No, but again, I love a guy where you can have, you can have a difference in opinions. That's the beautiful thing about the firehouse kitchen. I mean, it could get raucous and now, I don't know, it's a little different situation, but I mean, back in the day, some funny stuff. But yeah, John, John literally just took his talents. You would think he was speaking from the heart of a fireman or a cop or a soldier or a Marine, you know, someone who was there. But I think he especially got to know Ray so well and Ray had this stack of mask cards from, you know, the funeral cards they give out. It looks like, you know, a larger business card that's laminated. And Ray had a stack of them he would carry around. I think it was close to a hundred cards and John saw it and he said, what's that? He says, these are my cards. He said, for what? He says, for my brother's funerals. He was like, oh my God, you've been to that many funerals? He goes, yeah, this is just the ones I made. Like, you know, and John, I think was just stunned. And John actually had that stack of cards after Ray passed and like said, look, look at these. There's gonna be more of these cards. We have one guy a week or girl, one responder or a recovery worker or someone who actually resided down there. There's more than one a week dying. It's one a day dying on average. And on average, two people are diagnosed with a 9 11 cancer or disease. Right now, the worst part is there's autoimmune diseases flying off the graph and they're not covered under the legislation. By the grace of God, my cancer is covered. If my cancer comes back, I mean, I'm in remission. It's technically incurable, but I've been blessed I'm staying ahead of this stuff going on 10 years. But if it comes back with a vengeance tomorrow and takes me, at least my wife will get my pension and be able to live her life without fear. But my friends who are suffering from these advanced autoimmunes, their wives get nothing. Their pension dies with them. And we're hoping that John and his army can shame these politicians once again to have the kindness and decency to cover these autoimmunes. You know, they're throwing a lot of money around at a lot of things lately. And this is one that they won't. And these are lives in the balance who really need it. And John had this strong line. They did their jobs, do yours, talking to the politicians. Yeah. And it's a strong wake up call that it's not about the Twitter or the social media or all that kind of stuff. You have a job to do and you have to, it's that compassion implemented in the form of money of helping people that were there for you when you needed help. Well, we had a guy, I mean, I might get audited out of this one, I hope not, but we had a Congressman from out West, I won't say where, but he prided himself on saying he was a retired cop, a busy cop, 22 years. He said no on the legislation. I witnessed a cop who was dying get out of his wheelchair and said, hey brother, I got a half a million dollars in medical bills and I'm a short timer. I got a few months to live. Who the F is gonna pay him? Do the right thing. You say you're a cop, you show me you're a cop and you sign that paper. And the guy started tearing up the Congressman and he signed it, but he had to be freaking shamed. And you know what he said? Well, this doesn't really confront me. This is pork as far as my district is concerned. He goes, oh yeah, do you know there's 10 guys from your district who came across the country to help us that are also dying? He had no idea. He had no idea. And that's the sad part about Alex. It's a failure in leadership. I think some people would vote for Mickey Mouse just because if he ran. I mean, I have no offense against Mickey Mouse. I like him, he's a good guy, right? I mean, but like, I mean. Allegedly. Allegedly, supposedly. We don't know. Yeah, yeah, yeah. But seriously, I look at some of the leaderships sometimes and go, we're in trouble. And also you lose, I think the way government is structured is people who are senators or people who are in Congress, they start playing a game between each other and they lose track of the connection to the people, to the basic humanity. So you forget, even when you think of yourself as a cop, you forget what are like the cops and the other people servicing the community actually experiencing all the troubles they're going through and how they can actually be helped because you lose touch to that because you're not actually living, you're not talking to them, you're not living among them. And I mean, that's a natural part of the system, but I think that's why character and great leadership is important is you say you leave the game of Congress and you go back to the people. I mean, that's what the country, it's like the George Washington ideal is you're not playing a game of power. You're ultimately see yourself as somebody who's servicing this country's service in the community and that requires talking to the people in their time of hardship. Well, you have some people serving in congressional districts don't even live in that district. I mean, so how are they gonna empathize? They're not even driving through there on a daily basis. And again, when anything becomes lucrative from a financial standpoint, it blurs people's vision. You have to take the potential of becoming rich out of politics. Politics is public service. Police and fire and EMS are public service, but cops and firemen and medics don't walk out of their career with gazillion dollar contracts with this company and that company on that board of directors and this board of directors. They walk out with a pension and that's it. And you have to wonder the intentions of people getting into politics. Are they truly going into to help the human condition or are they trying to help their own damn condition with their wallet and their pocketbook? And I try to lean toward the latter lately with what I'm seeing out there. Well, some of them are the good ones and that's our job as a society is to elevate the good ones. That's it and that has to do with the ideals that we elevate. There are a number of conspiracy theories around the events of 9 11. Do any of these hold true to you or do they just frustrate you, even anger you? I've been asked this by a few different people in my life. This is my take on it, right? You're a man of science and a man of education. So you... Allegedly. Allegedly, but yes, but you're a very, very intelligent man. And what I believe took place is this. Structural steel will fail at a sustained temperature of 1500 degrees Fahrenheit. And I don't know exactly how long that would have to be sustained, but that's the temp, right? Diesel fuel, kerosene fuel, kerosene based jet fuel, which was the ignition there burns at 2200 degrees Fahrenheit. So that continued burning of that diesel, that jet fuel, but kerosene based, it's all kind of similar exceeded the temperature needed for that steel in the structural members of the trade center to fail. In my heart of hearts, I would hate to ever think that somebody affiliated with our government with some sort of agenda would perpetrate that crime and that tragic just destruction of humanity and property for some other form of gain. Those planes rammed into those buildings at 450 miles an hour. They were loaded with thousands and thousands of gallons of jet fuel. Number seven trade center had the backup for the emergency management system for the city. And it was an emergency generator in that complex which had a 25,000 gallon tank of diesel fuel to continually run for weeks to keep the 911 system, the backup system going in the case of a catastrophic event. Well, that tank in seven heated up from the fire that was already going on from the aircraft debris coming into the building. So once that diesel became ignited in seven, now you had enough temperature to fail that steel in that building. So I would like to truly believe what I've learned from the minimal fire science knowledge I have from my career, that it was just a matter of, it burned too long, it burned too hot and it failed. I mean, if you look at the way it came down, it came down as it was designed to in the God forbid event that it was to collapse. It came down pancaking upon itself. If it had failed horizontally and just sprayed out side to side, those buildings would have dropped for a quarter, half a mile up to Canal Street. But you know, Lex, I can't. The fire and the destruction that could have resulted. Yeah, oh my gosh, it could have been so much worse. I mean, you would have taken out every building from that point all the way up. But in my heart, I'd like to just believe that it was just a fire that burned too long and too hot. These planes cause structural damage upon impact in both buildings and it was just a matter of time. And then you think about it, you add all the plastics, all the carpeting, all of the stuff that was burning on those floors. You add that to that fire load. I think it just had enough to collapse it. And you were in building seven for part of that day. I was just after it came down as well. We were aside it and we weren't in it or next to it when it actually did come down. But moments after we were there. And again, I would like to believe that it just, it was just that that fuel was going and it just took its physics, took its course and it failed. So physics and science aside, it's hard. It's both I would like to believe and it's hard to imagine that anybody would be so evil as to orchestrate parts of this from within the United States government. That's very difficult for me to imagine. You know what though, Lex, there's people and I won't elaborate, I won't get into it. Any controversial subjects or what have you. There's some people that don't have any problem at all perpetrating any level of evil. People like you and I who have hearts and we have depth of soul. We couldn't imagine it, but there's other people wouldn't even be a second thought. I mean, I've seen some horrific incidents in my career that I go home shaking my head at night going, human beings are just, they're not wired right. You know, I mean, I look at animals, I love animals, I love dogs especially, right. And I see this dog park when I train to fly airplanes now and something I wanted to do. And there's a dog park across from the airport and there's 60 dogs and there's bones flying up in the air and chew toys and sticks and they're running around having the time of their life, right. And they're all getting along and they're not hurting each other. They're not violating each other. They're not canceling each other. And I'm going, we really need to learn from these dogs. Like, right. And like, I just, yeah. I mean, sometimes it sounds crazy, but I think they're a better species than people. Unless they're rabid, they don't hurt on purpose. They don't, you know, they don't cut you off in traffic and throw you the middle finger. And you know, they just don't do these acts of humanity that sometimes are so vicious. Why do you think these conspiracy theories of which there's a lot take hold? Why do you think so many people believe some version of different conspiracy theories around 9 11? Well, you know, like many things in life, it leaves me a little conflicted. I have to say this, I am at the point now, I don't know who to believe anymore. So I could see that lending a hand to someone who's already a doubter going, oh yeah, look, exactly, that's what they're doing, right. I mean, you know, look at this whole virus. Like, who do you believe? Like, where'd it come from? You know, like, and you know, if you plant that seed, it's like that little campfire we were talking about earlier, right? You just toss a little gas into those embers. You got a fire now. I also think there's a lot of people with a hell of a lot of extra time on their hands, right? And they're really bored. You know? And the two are combined. Alex, yeah, man, you know, like, look, I was a three job Charlie, right? You know, one guy used to say to me, anything but home. I go, no, I got deadlines, responsibilities. You know, like, that's what it comes down to is like, I mean, look, we all have our hobbies and things we like and, you know, little nuances. And that's what makes us special. We're unique. Every person is a unique being. But I also think some people just, they want to cling to something. Like, we all want to feel accepted and belong to something. So all of a sudden you group up with these people and you all believe this fervently. Like, yeah, yeah, yeah, you know, they did it. They took it down. They took it down. And now you start going, yeah. And I think what happens is when you're in company of people and you start telling each other the same thing often, you freaking believe it. I mean, if you keep telling me I got a gray head of hair, I'm going to go, you know what? I do. But no, I don't. I mean, right? I got that waving bye bye do. But like, but you know, I think when you start hearing something often, you start believing it. But I'm not going to, I'm not going to doubt their intelligence. I'm not going to doubt their intentions, but I just don't see it as being plausible. I just, I, it would be too, too big of an operation to successfully happen. I, you know, I mean, look, there's other things that, you know, I won't say it on the interview there, but like I have my doubts with certain things, you know, that, that. I mean, conspiracy theories take hold for a reason. Cause some of them are true. No, yeah. The hard thing is just to know which ones is the problem. When you don't have facts, right? Or you don't know who to trust. Sometimes when you don't have facts, when you don't have figures and you don't have science, it's hard to take someone's word on it. You know, I had a conversation with someone a while back, right? And the guy's like, just, just dedicated atheist. And he thinks I'm an idiot for believing in God. And he's like, yo, you're one of those jerks who believes in creation. And I said, well, I do. Well, what about the big bang theory? He's going on his diatribe about the science and the gases and the chemistry. And I'm going, dude, I barely got through high school chemistry, slow down. And he went on a tangent and all of a sudden I stopped. I went, who, who created the gas and the molecules and the stuff you're talking about and the collisions? And he was furious and stoned off. And I got him. And again, I had no facts. I had no figure. He didn't either, but I stumped him. But sometimes when you can't show something, people need to see something tangible. They need to see it in their hand to believe it. And that's the real hard thing about faith. I see it in action. People restore my faith. And then I say to myself, well, there can't be that many dummies in this world if there's so many billions of us believing in this higher power, this higher, right? I mean, and you said, you said earlier, like you believe most people are good and I do too. The bad outshine the good because the bad get the press. Right? If it bleeds, it leads. That's just, you know, like, think about it. How many more damn zombie apocalypse movies can we make? Right? I didn't even know there was that many zombies. Yeah. And it just seems like every other show is just guys like, you know, bashing each other's heads in with bats with nails in it. And it's like, after a while, it's like, all right, gosh, you gotta get a new boogeyman here. You know, right? Like, but seriously, like. But meanwhile, human civilization is getting better and better. We're just like making Hollywood movies. They just. No, we're getting better and better, but we're treating each other worse and worse. You would think with all this technology and all the knowledge and all the, it's like, what the hell is going on sometimes? Like, I really want to see the good. And I think maybe, maybe the level of bad that we're seeing was always existent. It's just now everything is instantaneous news and flashes and tweets and this and this. Like, like, you know. Well, with the technology we have, it's also come to the light. So you get to see all these fights. It almost, I think that's step one of dealing with the problem is revealing it in its full beautiful light. Oh yeah. How much of a bickering species we are. 50 years ago, a guy like me who loves to talk, how the hell would I have gotten an opportunity to have someone listen to me and have, right? I love this. This is amazing. It's cool. But like, but you didn't have that arena. You didn't have all these things. My grandfather, Nels, God rest him, he died in 1979. I mean, that dude didn't even want to have a checking account. He would walk to each store, each, the phone company, the gas company, this company, and pay the bill in person. He didn't trust the bank. And it was like, now, ATMs, this, that, he would be overwhelmed. He'd be just like, I mean, I love my dad, but to watch him on his iPad is comical, right? He calls my niece's boyfriend, who's a tech guy, Matt, Matt, if you listen, he's the greatest. He'll have this poor guy on the phone for like hours. Like the second you'll walk in to see my father, my kids, hey, do me a favor, you fucking straighten out this pad. And it's comical because I'm looking at my dad and I'm going, he was born when Hitler started World War II. Yeah, wow. And I'm going, he's seen all of that. Oh, my wife's grandmother was born in 1900 in Czechoslovakia and she died in 1998. I'm going, holy, the stuff she saw in the span of her life, just, it's just incredible. But what troubles me sometimes is with all of these advances and all these devices, this is what I say to my kids, look up from the phone and look up, right? Because we don't talk anymore. I saw a girl literally, and I shouldn't say girl, guy, whatever, I saw a person literally just about walking to an open manhole cover texting. And I'm going, that's scary because your awareness is gone. And it's, I've been at restaurants, groups of people and they're texting, they're texting each other just sitting on the other side of the table. I'm like, put the freaking thing down and have a conversation. And that's the thing, we've lost the art of conversation. You know, like, my wife runs, she has this running joke. She goes, there's a lot going on up there. And I'm like, yeah, because I really, I'm inquisitive. I'm excited about life. I love to meet people. I love to learn. I love, and the only way you can do that is to have a conversation. The hilarious thing about this, so you're obviously very charismatic. You got great stories. You're a great human being. Thank you. And you're talking to a guy who spent most of his life behind a computer hiding from people. No, no, and I don't. But we're like trying to bridge this. Right, but I don't mean that as a rip, but you, I would never know that. I would never know that because you're very engaging. You're very, like, I would not know, like you don't have any impediments to your social skills, your personal, and that's, and again, I don't mean it as a knock to you and these young people. Well, no, but this is me trying to look up from a smartphone is having these conversations, talking to people. I think it's important. I mean, some of it could be, it's always hard to know. Some of it could be just you and I being old school, because you grew up before the internet. Maybe there is joy and deep human connection to be discovered inside the smartphone. We don't, it doesn't seem that way, because the smartphone's so new, maybe we just haven't figured out those things, because there's a globalizing aspect. There's a opportunity for you to connect with people from across the world in ways that. I have cousins in Ireland and England. I love it. I get a FaceTime or a WhatsApp and it's like, holy crap, they're, you know, three, 4,000 miles away and I'm having a conversation now. I used to send my grandma in Ireland a letter. I adored her. She passed when I was 10. And, no, I'm sorry, I was 11. And I sent her a letter, airmailed, and I'd wait and I'd wait, and about two weeks later, this airmail letter would come back and she'd call me Master Nils William Jorgensen. I would be so excited, open that bad letter. Handwritten, just like. Yeah, and like, and then I'd write her another one and I just couldn't wait for letters from granny. And now it's like, you know, that's kind of faded away. Yeah, I still write letters, by the way, handwritten. I do too. The way this all came about was I wrote a letter to someone to say thank you for cancer research. I'm blessed to be alive. My cancer, right? That's a good starting point for any story. I'm blessed to be alive. And my cancer was one that if I got it 15 years prior to 19, excuse me, 2011, I was a dead man, right? 15, 20 years before there was no drug to treat. I was gone, going home to see him. So there's this wonderful gentleman that donated hundreds of millions of dollars to cancer research, Mr. David Koch. He's since, God rest his soul, passed away. And he's a controversial guy, big time business titan. And, you know, there was, the press was just brutalizing him one day over something to do with his politics. Now, I'm a union guy, proudly served in unions, still in a union, you know? And he was not, you know, most business guys don't like unions, right? But, you know, most guys like me don't like working for $3 an hour, so we like our unions, right? And I reached out across the table, so to speak, and I sent him a handwritten letter to thank him, to say, we may not agree on everything, but I can't thank you enough. There's just this regular dude out there who is now living his life, watching his kids grow. Thanks to generous people like you who believe enough in cancer research, you've saved my life. Maybe, I can't say his exact dollars, but people like him. And he reached back out and his secretary said, oh, he'd like to talk to you on the phone. I go, well, he's kind of a busy guy, he wants to talk to me, he's a billionaire. And he got on the phone, he was like the greatest guy in the world. Invited me up to Sloan Kettering to dedicate a new cancer wing. It was like I was hanging out with my dad. And the sweetest man, just so kind, so empathy, because he was a cancer survivor. But now he's got the means to help people who've suffered his fate to a better place. And he was so real and it was so beautiful just to get to know, say, hey, you know what? This guy is a big time guy, but yeah, he's just a regular human like you and I. I'm a guy who went to night college and I went to the army and I'm a blue collar kind of dude. And here's this guy who went to MIT, like you, and he's a wildly successful billionaire, a genius. But yet he can sit down and mix it up with me and know that I was truly grateful. And that to me was just like one of the coolest little relationships I've ever had. It wasn't like we were hanging out, having barbecues together, but like, you know, it was just, I was so touched by his decency. Well, the basics of the, like cancer reveals, you know, it's like fundamental to the human experience. It's trauma, it's tragedy. It's like money, who gives a shit about money? Education, all of that is like weird new inventions. You know, life is short. You suffer with the various diseases. And that is a reminder that life is short and a reminder of the basic human connection. And that's why you can bridge that gap. Oh yeah. All sparked by a handwritten letter, which just makes for a hell of a story. And you know what, Lex? This is the commonality between us. A guy with three jobs to a billionaire. We both had that sense of a sledgehammer to the chest. Boom, you have cancer and you can't breathe for like 30 seconds. And then when your heart's just about to kick off and you take a breath and you go, I'm sorry, what'd you say, doc? You have cancer. And it don't matter what kind. One of my best buddies, Bobby's going through right now, a prostate, and I got way too many of my buddies with cancer, right? My buddy, Hugh, who became a vet since his first cancer, he was a fireman, he's now a veterinarian, right? He diagnosed me actually over the phone, by the way. When they couldn't figure out what was wrong with me. Well, Dr. Hugh, he nailed it to the T. And we talk. And the same thing that the dozen of my close friends that have cancer, the same thing we say is the fear. So Mr. Koch and I, we shared that same sledgehammer to the chest and that same fear. And it didn't matter how much money he had and how much I didn't. And you know, it's just like the morning of the trade center. There was big time brokers who went to their demise, right? Working in these firms, God rest them. And there was dishwashers, excuse me, dishwashers up on the windows on the world restaurant on the 107th floor, making five bucks an hour. And they died together, it didn't matter. It didn't matter if you had an armored car loaded with bills, you were done that day. And that's, I think where people need to humanize each other. Just because you drive around in a nice car and you got your own jet and you got this and you got that, don't mean nothing. When you're going, when you're in that vulnerable spot, you could have more money than the US reserves. Federal reserve, or you could have a welfare check. You're going. I learned that in a cancer ward. I had people in my ward that died on me. I was going around as a little bit of an ambassador because I was trying to, I was putting on a fake, I was putting on a fake like I got this, I got this. I was so scared. But when I got past that seven days of torture and the days leading up to it, I'd go around and try to comfort the other cancer patients. I had this one older African American gentleman, he couldn't talk because he had such advanced throat cancer. He was my roommate for a little while, but then he got worse so they had to put him by himself. And you couldn't understand what he was saying because his throat was just so radiated from the radiation. But if you put your ear down to him, you could make out what he was saying. And I'm not faulting the nurses for maybe not wanting to do that, right? They're busy, they got a ton going on, they can't spend, you know. So if he was in need, I'd put my ear down and I'd find out and I'd go get it for him. So when they moved me down the hall, they asked me to come down with my IV tower. He needed me. And I knew it was bad because he just, his look was gone. And I said, sir, what do you need? And he whispered, call my sister, I'm going. He had only one survivor in his whole life. And she was in North Carolina and he wanted her to know she couldn't get up, she was elderly. And I got the nurse and I got on the phone and I called his sister and I said, ma'm, I explained who I was. And I said, he can't really talk. He can't really verbalize too well right now, but he wants to say he loves you. And I put the phone down and he told her he loved her and he said, I'm going home. And that was it. And I hung the phone up and I said, ma'm, I'm so sorry. I said, you know, they'll notify you. And I stayed with him for a while holding his hand and then, you know, they wanted him to rest. And then I left and then I got the tap an hour later and they said, I'm sorry, he's gone. And then there was another girl and she was a young girl from one of the areas I work, young African American girl where I used to respond and I didn't know her, but I knew her neighborhood. And she had what I had, but they weren't sure which one. You know, leukemias, they're an elusive beast. There's 49 of them, right? And each one of them is like, they got their own little nuances, own specific treatments. So if they don't know what you have, they don't know what to do for you. And she refused to let him drill into her hip to take the marrow because it's vicious. It hurts so much. It's like someone born into your hip with a wood drill and it's no joke. And they asked me to try to convince her to let them do that or she was gonna die. Cause if they couldn't figure it out, it was advancing quickly. She was, so I talked to her and she said, I can't, I can't, I'm too scared. I said, but are you more scared to die? And she said, I am. I said, okay, I'll stay with you. I'll hold your hand. You squeeze it as hard as you want. I said, if you want, they'll give you like a towel or something to bite on, whatever. I said, but you get that pain out, but you need to do this so you can get saved. And she said, okay. And they came in and they, this huge thick needle, they just bore it into you. And she's screaming for her life and she's squeezing my fingers so hard and so hard. And I said, that's okay, hon. You keep going, you keep going. We got it. It's just 10 more seconds, 10 more seconds. They got it. They figured out her treatment and they got her onto her road to recovery. And then I spent a long time asking God, why do I have cancer? Then I stopped and I went, wait a minute. I didn't die that day with my friends. Shame on me for asking them why I have cancer. I had 10 years after 9 11 with such great ears. And I got to watch my little girl being born when John never got to see his son. So it was all gravy after that. And I said, but now I know why I have my cancer because I can empathize with people who have it. And I can try to be their voice when they can't talk, be their shield to try to take that pain because I can understand, I can walk their walk. And now I thank God for my cancer because it's made me a better human being. It's made me, I'm not gonna lie, it brought a lot of anger for a while and my family suffered it, but I really tried to go past that and heal and part of living out in the country. It's very, very healing for the mind and the soul. But I now thank God for the cancer because it humbled me. I didn't really need humbling. I wasn't an arrogant puffed up type of person at all, but maybe I was running away at myself a little bit and working on a TV show, I'm fine, man. 30 at the time, well, I was 42, I got sick. Life was cruising, man, it was great. And then all of a sudden it was like a blow out on the highway in the middle of the night and you were just veering off towards the guardrail. Yeah, you remembered, you're reminded that you're mortal and that's ultimately a connection to all the rest of us. Oh yeah, it's a good thing though, because that's the problem, I think. There's a lot of people running around and thinking they're immortal, right? You know, when you look at it, Lex, right? You look at the heartache in a lot of segments of people and anytime like someone that's got fame and wealth and success and they die tragically, a lot of times it's from a substance abuse or just some horrible death. And I used to say to myself, how the hell would someone with that much money and that much fame and this freaking mansion and I love cars, my son and I are just big car heads, you know, I'm like, you know, this guy's got a collection of cars and he overdosed because he was sad. And I'm going, how the frig are you sad? But then I stop and I go, okay, because maybe he doesn't have any idea who loves him. He's got a lot of people clinging onto him because of his success. And he just, he can't fill that void, you know? And then they fill the void with something destructive. And I'm not bashing people that have substance abuse problems or alcohol problems, I don't mean it that way. But what I mean is it's just sad that their level of despair is so high, on the surface, they look like they just got everything going on. It's all great, right? They're still human, still got to deal with the same. Yeah, exactly, because they want love, right? They want love and they can't really find it. Well, first of all, that's true for all of us. I think we're deeply lonely and looking for love and when we find it, that's what friendship is. Absolutely. And then that's true for whether you're super rich or super poor, it's all the same journey. My dad said all the time, kid, you're gonna end up working with hundreds of guys and you'll love a lot of them but he says when it's all said and done and you're all like me and if you've still got two or three of them that you talk to and you'll love. And I tell you what, I mean, I have thanked the Lord more than two or three of them and I have my six, I call it my six, it's the six guys that are gonna carry my coffin when I'm gone, right? Because I know this cancer's gonna come back, I know it. Like we get multiples, right? My friend Yvette just got his second. My friend Mike's had five of them. My other Mike has two of them, yeah. But I wasn't ready to accept it in 2011. There was so much more to do and it was so much, I was so scared, I'm like wow, who's gonna take care of my kids and who, you know, they were little. Nine, 11 and 14, right? It's like what the hell, I have two girls and a boy in between and they're beautiful kids. They're such good, good children, adults now. I mean, but you know, my wife's a drill sergeant, she's tough, she don't mess, you know, she's this big. So you're the softy in the family, I'm just kidding. Well, you know, it's funny because my son said to me, my son's 21 now, he's a good kid, you know. And he says to me, back when he was like 12, he goes, dad, I don't want you to be offended but I'm really scared of mom, I'm not really that scared of you. And you know, like I cracked up because it's true, she's gotta stand on like a milk crate to reach him because you know, she's tiny and he's tall, but it's true, but you know, but she was hard but fair, but loved, that's, see, this is the thing, you take any child anywhere from any background, if you love them, you nurture them, you teach them and you guide them, you have a successful adult. And see, that's the problem in our society, it's not judgmental, I'm not judging anyone, but we need to try harder as parents as siblings, as friends, but especially when we're blessed with a child, it's like, you gotta put that child first, it's like being a military personal responder, it's not about you anymore, now it's the team. So that little child is now the team and you know, your wife or your significant other, you know, like it's not about you anymore. And see, that's the problem is people have a hard time not making it about them, you know, like now it's really weird, my kids are 19, 21 and 24 and they hardly wanna hang with me because they're busy in their life, we love each other, they're probably tired of hearing me go on and you know, preach and whatever, but like, but they're adults, we did pretty much the crux of what we had to do to put them into adulthood. And I look back and I go, wow, I wish I didn't work so much and I wish, but then I say, no, but it was okay, my wife stayed home, good lessons, good, you know, just like. But ultimately, like you said, it's love. It is, it's the common, love is the most important ingredient on this earth and that's the problem what's going on right now, like take politics out of it, right? Take polarizing each other against each other, take all that crap out of it and just airdrop a bunch of love, right? Like when I worked on Rescuing Me, right? I love those people so much, they were such great, we had such a great crew and they worked so hard. You're a celebrity. No, no, no, not at all. If I was, it didn't really work out so good. I went on to be in the stagehand, no, I'm not pretty, but they don't want old guys waving bye bye hairdos, but it was funny, the crew, we became really tight, we had like, shoot, like 80, 90 people on a set, right? And you know, the first few episodes, everybody's trying to feel each other out because you know, you work with different crews, different people and this is going back, starting in 2004, so it was a different time and I love to hug people because to me, a hug is a true expression of love and caring. You may not know a person a long time, but you say, I care about you with a hug. Can I add just a tiny tangent? This was in the midst of COVID when I was in Boston and it was, you know, masks, like triple masks, nobody. And when I went to see Joe here when he was trying to convince me to move to Austin, Joe Rogan, and then the first time I see him, he's like, ah, you motherfucking big ass hug. And it felt so good. But people probably looked horrified. They're hugging. It was just him. Oh, okay, I know what I'm saying, but if you do it in public now, it's like you committed. But that expression, because I was so, you forget how powerful that is. Oh, I got some of my buddies. I give them a huge hug and a big sloppy kiss on their cheek and I, cause I love them. They, these are my brothers, you know? But on this set, I swear to God, it got to the point and I'm not trying to whatever, but there was people that would come up to me for the daily hug. And I said, what are you doing? And they said, come on, bring it in. And I give them the hug and they said, you don't understand. It just makes me feel so good. It makes me feel like you give a crap about me. I said, I really do. I said, but it touched my heart that people were seeking me out to get that hug to start the day. And I remember there was a guy in Manhattan, he was selling hugs for like 50 cents and I think he got arrested, right? It was just before COVID. But like, I wouldn't sell them if, but now. You've given them away for free. Well, now I got leukemia. I'd be kind of concerned to get into COVID. I mean, but like, I really think we need that. We need hugging booths, like in each city or each town. Like, because there's so many people that just want to know someone gives a shit about them. And that's the problem. It's like, like, you know, that's what I love about small little towns like where I am now in Tennessee. And I'm not knocking New York. I'm not knocking big towns, but I guess it's easier to do in a smaller area because it's just not this mass of humanity. But they'll stop and check on you. Like you're out in the road and you know, like I'm cutting and cleaning or whatever. Occasionally I'll roll a lawnmower or a tractor into a ditch cause I'm not a farmer, too good. But it's easier to drive a fire truck in New York. But they literally, oh, I was worried. I haven't seen you. And I'm like, no, no, I'm okay. But they literally like check on you. They're worried about you. And I'm going, these people hardly know me, but yet they're so caring. And that's the problem. Like this is what I love about my life. I spent a lot of time as, especially as a young boy and a lot of time in Ireland at my grandma's farm. And my mom comes from this tiny, tiny little village. She's out in the middle of nowhere. And the childhood home she grew up in still, my aunt and uncle live in it still. I just love it there so much. Cause everyone waves. Tennessee's similar. They wave, driving by and you're like, who the hell's that? I just wave, you know. But my cousin will point it out. Actually third cousin, second removed by, you know, Johnny. Like, holy shoot, I'm related to everyone here, right? But like everyone stops to say hello and how are you? And I have a problem doing that because my wife goes, people think you're crazy. Why are you talking to everybody? I said, like, I'll literally stop someone and say, how's your day going? Like, I mean, I'll randomly on the sidewalk. Then it looks a little nuts. But like, if I'm buying a cup of coffee. Oh, that happens here in Austin all the time. That's why I love it here on the sidewalk randomly. Yeah, no, it's just so nice. They'll say hi to me. I thought they recognized me or something. I don't give a shit who you are. They're just being nice. I was on the road coming back, driving from my family up north down to Tennessee last week. I stopped in a bathroom and it was closed. The girl was cleaning it, whatever. She's working so hard, whatever. And she goes, sir, she goes, if you go down the hall, there's a family restroom. Feel free to use it. You know, she didn't have to do that. And I went down and I'm old. You need a bathroom, you need a bathroom, right? And I walked back out and I said, ma'm, I said, I want to thank you for being here today. I says, the bathroom was immaculate. It was, it was like my army bathroom in the barracks. It was spotless, right? And I gave her $10. I said, I'd really like you to buy lunch with me today. I said, you really didn't have to do me that favor. And she goes, no, sir. I said, no, no. I said, I want. And it was like I gave her a million bucks. And I say to my wife now, I've been praying to be a billionaire. She goes, that's a sin. I said, no, no, you don't understand, right? She goes, oh, you're Mr., you know, Mr. God. I said, no, no, no. I said, you're getting it wrong. I said, I'm praying to be like a multi gazillionaire because I want to give it all away. We used to have a sign in ladder 114 until some other rival truck company stole it, right? Cause that's what we do. You know, they get sent to cover your district when you're at a fire and now your stuff's missing. And the old timers had a sign that says, I am content. Because if you got to ladder 114, that was considered such a great place, such a great assignment, such great guys. You had to be vetted to get there. You couldn't just randomly go. And it was a little exclusionary, but they wanted good guys. And I said to myself, that's who I am in life right now. I am content, but I'm restless because I want to really do a lot more good. It's like this podcast. I want to make sure that it's not forgotten. And I want to make sure that these charities that are really, really helping people get recognized. But I'd like to take it a step further, right? A friend of mine runs this foundation for young folks suffering mental illness and in crisis. It's for someone that we love dearly. And he's on a mission now to get therapy dogs for really, really mentally wounded warriors, right? A lot of these young soldiers are having a really hard time. And now they could be out a while. They may have come back in country two, three years ago. Now it's just starting to set in. And there's a waiting list for thousands of therapy dogs. And he said that they can't get enough of them quick enough. But he said, when you see the response, the way these veterans just light up when they get these dogs, it just changes their life radically, immediately. And I said, that's it. God, I don't know how I'm going to do it, but I want to be a gazillionaire. And I don't want any picture, photo ops, this, that. I just want to go, there's a dog, there's a dog, there's a dog, there's a dog. And then I want to build veterans land for these vets who just need a nice clean place to live. So why don't we take these old army bases and Marine bases and Navy bases that have been shut down. They're just sitting there rotting away. I was in the army in Alabama. My old Fort McClellan is three quarters vacant. It's sitting there. They just did a documentary on it. It just looks like zombie land going back to zombies. So why don't we take that and renovate it and say to vets who are struggling, hey guys, you're going to live here. And they take the old army, the places where they had all the supplies, there's massive buildings where you could just retrofit it and make light manufacturing within two weeks. Give these guys jobs. There they live, there they work. They'll take care of it. Military guys, they teach you how to take care of stuff. How the hell in this country should any vet come back home and be homeless? Because now they have to dedicate their lives for six, seven, 10, 12 years, five, six deployments making $7.50 an hour. And then they spend seven years or they get a whopping $16 an hour. They walk out making 35 grand. And now no one gives them a job. No one gives them a chance. So very quickly they end up homeless by no fault of their own. And I don't know how that's even possible. The people in this country who've given the very most and they're struggling, they're hurting. That's not fair. And my whole thing is if I can have this dream of succeeding, so to speak, I want to try to change it. So that's why I'm praying to be a billionaire. My Irish mother probably wouldn't agree either because you're not supposed to, right? Well, I'm the same with you. The more money you have, the more you're able to help. Yeah, you can put smiles on people's faces. I have to ask you, the US invaded Afghanistan in October, 2001 in response to terror attacks. Now 20 years later, we still had a presence and abruptly withdrew all troops. What do you think about this war across the world that was sparked by this tragedy? Whenever you do something quickly without thinking it out, thinking it through and planning, it doesn't succeed. I understand that we needed to exit. I mean, how long were you gonna stay over there? And we've lost over 7,000 of our young souls over there. For sometimes people, I don't know if they're grateful for it or not, right? I mean, I don't know. So there's the other element, and sorry to interrupt. One is the financial of $6 trillion and that money is not just money, it's education, it's everything, it's money that could have gone towards, first of all, the first responders, but all the servicemen and women of all kinds throughout this country. And then there's the other side, which is the over 800,000 people who died in direct result of this conflict. So not just the American side of the troops, but just people who died, those humans. And those humans, many of them civilians, that's spreading hate, especially if you have leaders on the other side who frame the death of those civilians in certain ways that just spreads hate throughout the world. And so you think about this kind of 20 year saga and think, what are the ways that money could be spent better and what was the way that we could have spread more love in the world versus hate? And you wonder, but then the other side, what is it? I'm not sure who says this line, but it's something like we sleep at night because there's a rough men out there ready to fight for you. There is some sense in which we have to make sure that there's strength coupled with the love, right? Otherwise evil men will do evil onto the world. So it's a very difficult decision, but then you look at the final picture and it's like, what have we gotten for this $6 trillion? What have we gotten for this 20 years? The thousands of American soldiers who died, the hundreds of thousands of civilians who have died. You know, it's a troubling subject for me. I'm a patriot, I love this country. I love it with my soul. And I was just about to head over to the first Iraqi war and we went out for desert warfare training and then it ended. I was at that time a combat medic assigned to an armored cav unit. So basically tanks driving around an armored personnel carrier and when it gets hit, then you tend to that guy, try to save his life. I didn't wanna go. I may sound like a coward, I did not wanna go to war. I would have went willingly if I was sent to defend my country, I took my oath. I didn't join the military to kill, but if necessary, I would. I'll use the analogy of cancer. If you have a cancer and you're aware of its presence and you don't annihilate those cells and take them out quickly, it's gonna spread and it's gonna kill you. Those evil bastards that flew those airplanes, one of those airplanes had a little three year old child in it from Ireland where my mom's hometown. A friend of mine who since died of a heart attack from 9 11 toxins, he found her shoe with human remains in it. And he thought someone was messing with us because we didn't know there was any kids in the building. He says, boss, there's a baby shoe and it looks like there's something in it, but there's no kids in the trade center. I went, the plane, it's a little girl shoe. I can never get that shoe out of my mind. The evil bastards who perpetrated that needed to have missiles strike and rain down upon them and annihilate them like a cancer that they are. What just fascinates me is they'll show videos of these guys flying around and pick up trucks with 50 cows on the back. It's like, well, wait a minute. If a camera crew can get this footage, you think all these freaking drones and planes and radar assisted systems can't just go whist, whist, whist, goodnight, you're gone. So kill the cancer, kill the cells, get rid of it, get rid of it quickly and go into remission. Like an undeniable show of force that sends a message that gets rid of most of the obvious centers of terrorism. And that note, that's though, because we offline mentioned a discussion with Jaco and maybe romanticize view and mentioning brothers in arms by dire straits and saying we're all brothers in arms even when it's on the opposite side of fighting, which is more of a vision and growing up in the Soviet Union you saw about World War II, that it's all just kids thrown into the kids sent to die in all sides. But then presenting that to Jaco who was in Iraq, he did not see as brothers in arms, which is his basic statement is there's evil people and some people don't deserve the compassion. You give them a few chances, they don't take the chances they have to go because they're spreading evil onto the world. And so it's not, we're not, all of us deserve a chance. Oh no, absolutely, but the difference though, and believe me, I, Jaco, I am from a way, way minor league compared to him, right? I mean, this man was right there in the firing line, but I can understand his analogy because when you think about it, right, those young conscripts back in Germany and Russia and all the countries where they were being drafted, even our guys were being drafted and thrown into this. They were gallantly and bravely defending their country. Now, I'm sure the young Germans felt, well, hey, Hitler must be right, right? And young Russians felt, hey, Stalin must be right. And the young Americans figured, hey, President Roosevelt must be right. So they were romantically in a sense defending the honor of their country, of their motherland. The difference between those, so they did have that commonality. If you and I were firing across each other from France to Germany or, you know, from Germany to Russia or whatever, we're just these two kids who got thrown into this. We didn't freaking ask for this, right? But the difference with Jaco's enemy is no one was attacking their country over there, right? No one was taking their country over. Maybe in their mind, they didn't want people trying to build their government, this and that. I don't know, I don't know enough about the history there to really elaborate. We didn't attack them. And if a soldier attacks a soldier, that's an understood concept amongst warriors. But when a soldier attacks a civilian, now you're after a different beast, and you've written that beast off, if that makes any sense. Yeah, and the enemy, I mean, as Jaco explains, the enemy in Iraq and just certain parts of the Middle East is essentially terrorists who don't value the lives of the civilians of their own country. They don't. And so it becomes like this weird guerrilla warfare slash game of violence that ultimately allows them to gain more power within their country, but they don't care if they're playing with civilian lives as pawns. If you have a child who dies that's a civilian in their country, that could be seen as a positive for them because they can use that to leverage for more and more power within that country. So when you're fighting an enemy like that, that's a vicious, that's an evil enemy. Absolutely. It's like snakes are beautiful, but if you go pet a rattler, you're getting bit and you're getting dead, right? And that's with terrorists, you've got to cut the head of the snake off. And I feel, no, don't commit our guys to be there anymore. But what we need to do is go with tech warfare. If we have intel from drones or planes or whatever it is that so and so and so and so and so and so are driving down in that pickup or whatever, take it out and do it again tomorrow and tomorrow and tomorrow. And maybe they'll get the message after a while, oh shit, these guys aren't messing around. Instead of throwing wave after wave of our brave warriors, brave SEALs, brave special ops guys, and God bless them for what they do, I couldn't do it. I could not have done it. But they have to be now sitting home going, what the hell? My friends, my body, myself, like they must feel so betrayed because they passionately went over there to cure a cancer, the cancer of terrorism. And now the cancer is back. And I hate to say it, but I think the cancer might start running wild. We need to change our tactics up. This is just my opinion. I can't see committing all of our guys to a continuous eternal war. But I think what we need to do is hit surgically and hit hard at that cancer that is over there. We are never gonna rebuild that region. It's just, it's thousands of years of traditions that you're not going to change. It's just some people are unchangeable because they don't want to. And we have so many social problems here in our country, I think that we need to fix first. I heard this spoken in the past by many people. It's like the garden theory. You have your garden with a fence around it. You tend to your garden. There may be weeds on the outside of the fence, but as long as they're not inside your garden, your garden will prosper. And I know some people don't agree to that America first and the whole take care of our own, but it's like, how are we gonna take in more people now? And I have a human feeling for them, but it's almost like the lifeboat theory. How many people can we take into the lifeboat before the lifeboat itself sinks as the ship is going down? So if we can't take care of our own homeless vets and our own homeless people, and it's just gonna become worse. And it doesn't make any sense. It's just like, we need to just take a timeout and I think switch our tactics a little bit. And invest into helping people here at home. Absolutely, absolutely. There's very few as obvious of cases as the first responders in 9 11. That one of the things that I really wanna kind of talk about at least a little bit, we've already talked about the amazing project that you're doing the 20 for 20 podcast that you host. We mentioned one story, Steven Siller, is there other stories or maybe you can speak out at a high level, what are you hoping to tell? And all these different stories that are weaved about that connect the tragedies and the triumphs, the heroism of that day and the days and the years that followed. You know, Lex, it seems like the common few themes, the common threads are being selfless, helping out others even though they might be a stranger, in acts of kindness, acts of love, and it seems to all be weaved together with faith. They all seem to have some sort of faith. I mean, we have one gentleman, Mark Hanna, and he's a Coptic Egyptian priest, and he's an immigrant to the United States. He was a port authority building engineer. And with his crew who subsequently passed away, the crew did, he was effectively rescuing dozens of people on the upper floors, and his boss ordered him to assist an elderly gentleman who was 89 down 78 flights of stairs to get him out. And in stopping on the 21st floor, he figured they would just wait there for medics. He came across Captain Patty Brown of Ladder Company 3, who told him, no, sir, you need to evacuate. And Captain Brown picked his brain a little bit about the structure because he figured, found out he was an engineer. And Captain Patty Brown continued on to effect rescues, and he and his crew were killed. But father, he's now, Mark was able to effectively evacuate this gentleman. They were the two known last survivors to come out of the tower. He now has dedicated his life to becoming a Coptic priest in St. Mary's Church in East Brunswick, New Jersey. He did this for a total stranger. And he said he was inspired by his bosses who died and his friends. One of his best friends was an Italian man. The other man was a retired Navy SEAL, Hispanic man. And they were part of this melting pot. And no one looked at each other that day, what color, what race, what belief are you? They just said, hey, you're a human in need, let's go. And we have the story about John Field on his mission to help the responders. We have a young lady, Mariah, whose birth father was on flight 93. She had not even met him. And she had this premonition that somebody in her family was killed that day. And her adopted mom said, no, everyone's fine. Three years later, when she was legally able to find out who her dad was, she found out that her dad, Tom, was actually on that plane as part of the Let's Roll team. And we have a gentleman, Robert Burke, who's an actor, sweetheart of a man. He's a gentleman and he's a very, very popular actor in Hollywood. He was on Rescue Me, Blue Bloods, Gossip Girls. And Bobby, my friend, as I call him, is a volunteer fireman now. This man doesn't need to get out of bed at two oclock in the morning and help people with a stroke or a burning garage or a burning house, but he does because he wants to. Because his best friend was Captain Patty Brown. And his other best friend was Father Michael Judge, who was our chaplain, who was killed, literally blessing the victims at the site, had just given last rites to the firefighter I mentioned earlier, Danny, who was killed. And Father Judge was in the lobby of the building, giving a blessing, praying to God to please stop this. And he was struck by debris and he was killed. And Bobby goes on to elaborate about Father Judge's story. Father Judge used to walk the streets of New York City, helping AIDS patients just with whatever they needed. And he was a Franciscan friar. They wear sandals and a robe. They just live very humble lives. And it's just a common denominator is loving each other and helping each other, regardless of you know the person or not. And really, when you think about it, that's how America was made. We fought for independence. Stranger fought next to stranger and fought tyranny because they wanted freedom. They wanted to be able to live, love, pray and prosper. And they fought and died alongside of strangers. And it's sort of symbolic of what happened that day. And then strangers from around this great country just flocked in by the thousands to help. They didn't know who was in that pile, but they didn't care. That was another American. And what I ultimately am trying to do involved in this beautiful project is spread the message of doing the right thing. Look at these examples. These brave people who didn't have to, especially the civilians, they weren't paid to run back in there and help person after person. And they had no obligation. They could have just said, hey man, I'm out of here and just bolted. But they didn't. So we're just trying to say to people, let's bring back that unity and that feeling of 912. As strange as 912 of a day it was, it was so sad because it was the first dawn of the sun where we realized this wasn't a dream. This was real and it's not going away. But the beauty of it was there was thousands of people lined up along the West Side Highway with signs and American flags. And they were from every country and every race and every creed. And it didn't matter who they were, but they all shared one bond, love. And they were hugging and crying and thanking rescuers. And it brought the morale so high for a group of people that was so beaten down the day before. It just started lifting the morale and making us realize, you know what? People really do give a crap. They really do love each other. And now I'm gonna be honest with you, I've been doubting that a little bit lately. I still have these examples of it. You know, that lady who helped me last night with the phone and just, you know, I know there's these shining little examples, but sometimes I think, I don't know, are we running out of them? Well, I gotta give you some advice. So there's two words that were repeated often in the days and the years after 911, which is never forget. So might I remind you to never forget about 912. I mean, those words, you talked about that, you know, there's people, what is it, college freshmen, maybe. They weren't even born. They weren't even born. And there's people in the 20s that were too young to remember or to understand the events of that day. But I think what that day, as you're describing, means, it's not about a terrorist attack. It's about the unity that followed. It was tremendous, Lex. I never felt so proud. I was always proud of this country. You know, I remember my grandpa Nels used to walk by, I'd see a flag, I'd hear the Star Spangled Banner and he'd tear up and I'd say, Grant, why are you crying? He said, I'm not crying, it's the tears of joy. I love this country so much. And I just remember like feeling that way. I felt that way 910. I felt that way on 911, but then on 912, I was just so proud of just the people, the way they stepped up. And I just want to try to see if that can happen again. And I hope it's not necessary for us to have another tragedy to bring that about. Let's do that without the tragedy. Let's just stop and say, hey, you know what? Let me listen to what this guy has to say. And maybe he's, he probably won't convince me, but maybe I'll go, well, you know, I never thought of it that way. Stop the finger pointing, the bickering, the tantrums, the fighting. It's just not necessary. It gets you nowhere, right? It's like, you know, I was two years old and I'd stomp around because I wanted a cookie or a piece of candy. I still didn't get it, right? You know, turned blue in the face and whatever, got a little swat in the rear end, but it didn't get the candy. And that's what we got going on right now. Everybody's just stomping around, being a baby. Stop, just stop. We're really lucky. Look, the country's not perfect, right? You know, but it's damn good. It gives us all these opportunities, you know? Like I said, no one's rushing out the gates to get out of here. They're freaking, I got a cousin of mine. I love him dearly. My cousin Tony in Ireland. And he said, he's just a little older than me. He's in his fifties. He said, man, I should have done it. I should have went to America. My dad said, go to America. I went to England and he went back to Ireland. And you know, but he's happy in Ireland. It's his home. But he said, wow, what a place of opportunity. And I said, it's never too late. He goes, yeah, but you know what? You get tied down. And I understand that. I thank God my mom came here at 16. I thank God my grandpa got on that ship. But in his 20s, 27, I think, you know, with not a nickel to rub together. I thank God they did it. Cause I don't know where else I would have ended up. There's no place else I want to be. And I thank God that there's people like you who rushed towards ground zero to help other human beings. And I believe that that human spirit is ultimately represents the best of this country and the best of this world. Thank you for the stories you're telling, for your perseverance in that. And thank you for welcoming me to the crew. You're very welcome. I'm proud. And we'll take you any day. You look like you could do the job just fine. I love lifting heavy things and doing dangerous things. So I'm proud to be part of this country and part of the Tally Ho now. Well, you are definitely an attribute to America and we're glad you chose to come here. You know, Lex, it's such a beautiful place. It's a beautiful melting pot. You know, if we were all the same, it would be kind of a boring place, right? Kind of boring. It really would. But it's just such a great place. And I just want to say thanks. It's an honor. It's an honor to have someone to let me sound off and it'll be even bigger honor if somebody will listen to me and just say, hey, you know, let me just try to do something good today. And you know, that's the tunnel to towers mantra is let us do good. And I just, you know, I got a really big credit card with God, a big balance, right? I need to pay him back a lot and I need to pay him forward. And I'm just going to spend the rest of my days trying my best. I don't know where this is going to go, what it'll lead into, but I really would like to get those dogs or those vets and build them that village and just keep going on from project to project to just say, when my final day comes and I'm laying there and I say, you know what? I really made the most of that second chance God gave me way back in 2011. I mean, I hope it's 30, 40 years from now, but even if it's 30 months from now, I'm giving it the best shot. So thank you, sir. I appreciate it and wishing you blessings and success in your career. Keep up the good fight and you're always welcome back to Texas. Oh, I love it. It's great food and a little hot, a little hot, but I can deal with it. We don't do so good Irish in the sun, you know? Well, the barbecue and the people are worth it. No, they are, they're awesome. I was down here for some storm relief a few years ago and I tell you what, I fell in love with it. The people are great, it's a great state and yeah, I'll definitely be back again for sure. Thanks for talking to me, Neil. Thank you, sir. Appreciate it. Thanks for listening to this conversation with Niels Jorgensen. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Franklin D. Roosevelt. Human kindness has never weakened the stamina or softened the fiber of a free people. A nation does not have to be cruel, to be tough. Thank you for listening and hope to see you next time.\",\n          \" The following is a conversation with Sean Carroll. He's a theoretical physicist at Caltech specializing in quantum mechanics, gravity, and cosmology. He's the author of several popular books, one on the arrow of time called From Eternity to Here, one on the Higgs boson called Particle at the End of the Universe, and one on science and philosophy called The Big Picture on the Origins of Life, Meaning, and the Universe Itself. He has an upcoming book on quantum mechanics that you can preorder now called Something Deeply Hidden. He writes one of my favorite blogs on his website, preposterousuniverse.com. I recommend clicking on the Greatest Hits link that lists accessible, interesting posts on the arrow of time, dark matter, dark energy, the Big Bang, general relativity, string theory, quantum mechanics, and the big meta questions about the philosophy of science, God, ethics, politics, academia, and much, much more. Finally, and perhaps most famously, he's the host of a podcast called Mindscape that you should subscribe to and support on Patreon. Along with the Joe Rogan experience, Sam Harris's Making Sense, and Dan Carlin's Hardcore History, Sean's Mindscape podcast is one of my favorite ways to learn new ideas or explore different perspectives and ideas that I thought I understood. It was truly an honor to meet and spend a couple hours with Sean. It's a bit heartbreaking to say that for the first time ever, the audio recorder for this podcast died in the middle of our conversation. There's technical reasons for this, having to do with phantom power that I now understand and will avoid. It took me one hour to notice and fix the problem. So, much like the universe is 68% dark energy, roughly the same amount from this conversation was lost, except in the memories of the two people involved and in my notes. I'm sure we'll talk again and continue this conversation on this podcast or on Sean's. And of course, I look forward to it. This is the Artificial Intelligence podcast. If you enjoy it, subscribe on YouTube, iTunes, support it on Patreon, or simply connect with me on Twitter at Lex Friedman. And now, here's my conversation with Sean Carroll. What do you think is more interesting and impactful, understanding how the universe works at a fundamental level or understanding how the human mind works? You know, of course this is a crazy, meaningless, unanswerable question in some sense, because they're both very interesting and there's no absolute scale of interestingness that we can rate them on. There's a glib answer that says the human brain is part of the universe, right? And therefore, understanding the universe is more fundamental than understanding the human brain. But do you really believe that once we understand the fundamental way the universe works at the particle level, the forces, we would be able to understand how the mind works? No, certainly not. We cannot understand how ice cream works just from understanding how particles work, right? So I'm a big believer in emergence. I'm a big believer that there are different ways of talking about the world beyond just the most fundamental microscopic one. You know, when we talk about tables and chairs and planets and people, we're not talking the language of particle physics and cosmology. So, but understanding the universe, you didn't say just at the most fundamental level, right? So understanding the universe at all levels is part of that. I do think, you know, to be a little bit more fair to the question, there probably are general principles of complexity, biology, information processing, memory, knowledge, creativity that go beyond just the human brain, right? And maybe one could count understanding those as part of understanding the universe. The human brain, as far as we know, is the most complex thing in the universe. So there's, it's certainly absurd to think that by understanding the fundamental laws of particle physics, you get any direct insight on how the brain works. But then there's this step from the fundamentals of particle physics to information processing, which a lot of physicists and philosophers may be a little bit carelessly take when they talk about artificial intelligence. Do you think of the universe as a kind of a computational device? No, to be like, the honest answer there is no. There's a sense in which the universe processes information, clearly. There's a sense in which the universe is like a computer, clearly. But in some sense, I think, I tried to say this once on my blog and no one agreed with me, but the universe is more like a computation than a computer because the universe happens once. A computer is a general purpose machine, right? That you can ask it different questions, even a pocket calculator, right? And it's set up to answer certain kinds of questions. The universe isn't that. So information processing happens in the universe, but it's not what the universe is. And I know your MIT colleague, Seth Lloyd, feels very differently about this, right? Well, you're thinking of the universe as a closed system. I am. So what makes a computer more like a PC, like a computing machine is that there's a human that every once comes up to it and moves the mouse around. So input. Gives it input. Gives it input. And that's why you're saying it's just a computation, a deterministic thing that's just unrolling. But the immense complexity of it is nevertheless like processing. There's a state and then it changes with good rules. And there's a sense for a lot of people that if the brain operates, the human brain operates within that world, then it's simply just a small subset of that. And so there's no reason we can't build arbitrarily great intelligences. Yeah. Do you think of intelligence in this way? Intelligence is tricky. I don't have a definition of it offhand. So I remember this panel discussion that I saw on YouTube. I wasn't there, but Seth Lloyd was on the panel. And so was Martin Rees, the famous astrophysicist. And Seth gave his shtick for why the universe is a computer and explained this. And Martin Rees said, so what is not a computer? And Seth was like, oh, that's a good question. I'm not sure. Because if you have a sufficiently broad definition of what a computer is, then everything is, right? And the simile or the analogy gains force when it excludes some things. You know, is the moon going around the earth performing a computation? I can come up with definitions in which the answer is yes, but it's not a very useful computation. I think that it's absolutely helpful to think about the universe in certain situations, certain contexts, as an information processing device. I'm even guilty of writing a paper called Quantum Circuit Cosmology, where we modeled the whole universe as a quantum circuit. As a circuit. As a circuit, yeah. With qubits kind of thing? With qubits basically, right, yeah. So, and qubits becoming more and more entangled. So do we wanna digress a little bit? Let's do it. It's kind of fun. So here's a mystery about the universe that is so deep and profound that nobody talks about it. Space expands, right? And we talk about, in a certain region of space, a certain number of degrees of freedom, a certain number of ways that the quantum fields and the particles in that region can arrange themselves. That number of degrees of freedom in a region of space is arguably finite. We actually don't know how many there are, but there's a very good argument that says it's a finite number. So as the universe expands and space gets bigger, are there more degrees of freedom? If it's an infinite number, it doesn't really matter. Infinity times two is still infinity. But if it's a finite number, then there's more space, so there's more degrees of freedom. So where did they come from? That would mean the universe is not a closed system. There's more degrees of freedom popping into existence. So what we suggested was that there are more degrees of freedom, and it's not that they're not there to start, but they're not entangled to start. So the universe that you and I know of, the three dimensions around us that we see, we said those are the entangled degrees of freedom making up space time. And as the universe expands, there are a whole bunch of qubits in their zero state that become entangled with the rest of space time through the action of these quantum circuits. So what does it mean that there's now more degrees of freedom as they become more entangled? Yeah, so. As the universe expands. That's right, so there's more and more degrees of freedom that are entangled, that are playing part, playing the role of part of the entangled space time structure. So the basic, the underlying philosophy is that space time itself arises from the entanglement of some fundamental quantum degrees of freedom. Wow, okay, so at which point is most of the entanglement happening? Are we talking about close to the Big Bang? Are we talking about throughout the time of the life? Throughout history, yeah. So the idea is that at the Big Bang, almost all the degrees of freedom that the universe could have were there, but they were unentangled with anything else. And that's a reflection of the fact that the Big Bang had a low entropy. It was a very simple, very small place. And as space expands, more and more degrees of freedom become entangled with the rest of the world. Well, I have to ask John Carroll, what do you think of the thought experiment from Nick Bostrom that we're living in a simulation? So I think, let me contextualize that a little bit more. I think people don't actually take this thought experiments. I think it's quite interesting. It's not very useful, but it's quite interesting. From the perspective of AI, a lot of the learning that can be done usually happens in simulation from artificial examples. And so it's a constructive question to ask, how difficult is our real world to simulate? Right. Which is kind of a dual part of, if we're living in a simulation and somebody built that simulation, if you were to try to do it yourself, how hard would it be? So obviously we could be living in a simulation. If you just want the physical possibility, then I completely agree that it's physically possible. I don't think that we actually are. So take this one piece of data into consideration. You know, we live in a big universe, okay? There's two trillion galaxies in our observable universe with 200 billion stars in each galaxy, et cetera. It would seem to be a waste of resources to have a universe that big going on just to do a simulation. So in other words, I want to be a good Bayesian. I want to ask under this hypothesis, what do I expect to see? So the first thing I would say is I wouldn't expect to see a universe that was that big, okay? The second thing is I wouldn't expect the resolution of the universe to be as good as it is. So it's always possible that if our superhuman simulators only have finite resources, that they don't render the entire universe, right? That the part that is out there, the two trillion galaxies, isn't actually being simulated fully, okay? But then the obvious extrapolation of that is that only I am being simulated fully. Like the rest of you are just non player characters, right? I'm the only thing that is real. The rest of you are just chat bots. Beyond this wall, I see the wall, but there is literally nothing on the other side of the wall. That is sort of the Bayesian prediction. That's what it would be like to do an efficient simulation of me. So like none of that seems quite realistic. I don't see, I hear the argument that it's just possible and easy to simulate lots of things. I don't see any evidence from what we know about our universe that we look like a simulated universe. Now, maybe you can say, well, we don't know what it would look like, but that's just abandoning your Bayesian responsibilities. Like your job is to say under this theory, here's what you would expect to see. Yeah, so certainly if you think about simulation as a thing that's like a video game where only a small subset is being rendered. But say the entire, all the laws of physics, the entire closed system of the quote unquote universe, it had a creator. Yeah, it's always possible. Right, so that's not useful to think about when you're thinking about physics. The way Nick Bostrom phrases it, if it's possible to simulate a universe, eventually we'll do it. Right. You can use that by the way for a lot of things. Well, yeah. But I guess the question is, how hard is it to create a universe? I wrote a little blog post about this and maybe I'm missing something, but there's an argument that says not only that it might be possible to simulate a universe, but probably if you imagine that you actually attribute consciousness and agency to the little things that we're simulating, to our little artificial beings, there's probably a lot more of them than there are ordinary organic beings in the universe or there will be in the future, right? So there's an argument that not only is being a simulation possible, it's probable because in the space of all living consciousnesses, most of them are being simulated, right? Most of them are not at the top level. I think that argument must be wrong because it follows from that argument that, if we're simulated, but we can also simulate other things, well, but if we can simulate other things, they can simulate other things, right? If we give them enough power and resolution and ultimately we'll reach a bottom because the laws of physics in our universe have a bottom, we're made of atoms and so forth, so there will be the cheapest possible simulations. And if you believe the original argument, you should conclude that we should be in the cheapest possible simulation because that's where most people are. But we don't look like that. It doesn't look at all like we're at the edge of resolution, that we're 16 bit things. It seems much easier to make much lower level things than we are. And also, I questioned the whole approach to the anthropic principle that says we are typical observers in the universe. I think that that's not actually, I think that there's a lot of selection that we can do that we're typical within things we already know, but not typical within all of the universe. So do you think there's intelligent life, however you would like to define intelligent life, out there in the universe? My guess is that there is not intelligent life in the observable universe other than us, simply on the basis of the fact that the likely number of other intelligent species in the observable universe, there's two likely numbers, zero or billions. And if there had been billions, you would have noticed already. For there to be literally like a small number, like, you know, Star Trek, there's a dozen intelligent civilizations in our galaxy, but not a billion, that's weird. That's sort of bizarre to me. It's easy for me to imagine that there are zero others because there's just a big bottleneck to making multicellular life or technological life or whatever. It's very hard for me to imagine that there's a whole bunch out there that have somehow remained hidden from us. The question I'd like to ask is what would intelligent life look like? What I mean by that question and where it's going is what if intelligent life is just in some very big ways different than the one that has on Earth? That there's all kinds of intelligent life that operates at different scales of both size and temporal. Right, that's a great possibility because I think we should be humble about what intelligence is, what life is. We don't even agree on what life is, much less what intelligent life is, right? So that's an argument for humility, saying there could be intelligent life of a very different character, right? Like you could imagine the dolphins are intelligent but never invent space travel because they live in the ocean and they don't have thumbs, right? So they never invent technology, they never invent smelting. Maybe the universe is full of intelligent species that just don't make technology, right? That's compatible with the data, I think. And I think maybe what you're pointing at is even more out there versions of intelligence, intelligence in intermolecular clouds or on the surface of a neutron star or in between the galaxies in giant things where the equivalent of a heartbeat is 100 million years. On the one hand, yes, we should be very open minded about those things. On the other hand, all of us share the same laws of physics. There might be something about the laws of physics, even though we don't currently know exactly what that thing would be, that makes meters and years the right length and timescales for intelligent life. Maybe not, but we're made of atoms, atoms have a certain size, we orbit stars or stars have a certain lifetime. It's not impossible to me that there's a sweet spot for intelligent life that we find ourselves in. So I'm open minded either way, I'm open minded either being humble and there's all sorts of different kinds of life or no, there's a reason we just don't know it yet why life like ours is the kind of life that's out there. Yeah, I'm of two minds too, but I often wonder if our brains is just designed to quite obviously to operate and see the world in these timescales and we're almost blind and the tools we've created for detecting things are blind to the kind of observation needed to see intelligent life at other scales. Well, I'm totally open to that, but so here's another argument I would make, we have looked for intelligent life, but we've looked at for it in the dumbest way we can, by turning radio telescopes to the sky. And why in the world would a super advanced civilization randomly beam out radio signals wastefully in all directions into the universe? That just doesn't make any sense, especially because in order to think that you would actually contact another civilization, you would have to do it forever, you have to keep doing it for millions of years, that sounds like a waste of resources. If you thought that there were other solar systems with planets around them, where maybe intelligent life didn't yet exist, but might someday, you wouldn't try to talk to it with radio waves, you would send a spacecraft out there and you would park it around there and it would be like, from our point of view, it'd be like 2001, where there was a monolith. Monolith. There could be an artifact, in fact, the other way works also, right? There could be artifacts in our solar system that have been put there by other technologically advanced civilizations and that's how we will eventually contact them. We just haven't explored the solar system well enough yet to find them. The reason why we don't think about that is because we're young and impatient, right? Like, it would take more than my lifetime to actually send something to another star system and wait for it and then come back. So, but if we start thinking on hundreds of thousands of years or million year time scales, that's clearly the right thing to do. Are you excited by the thing that Elon Musk is doing with SpaceX in general? Space, but the idea of space exploration, even though your, or your species is young and impatient? Yeah. No, I do think that space travel is crucially important, long term. Even to other star systems. And I think that many people overestimate the difficulty because they say, look, if you travel 1% the speed of light to another star system, we'll be dead before we get there, right? And I think that it's much easier. And therefore, when they write their science fiction stories, they imagine we'd go faster than the speed of light because otherwise they're too impatient, right? We're not gonna go faster than the speed of light, but we could easily imagine that the human lifespan gets extended to thousands of years. And once you do that, then the stars are much closer effectively, right? And then what's a hundred year trip, right? So I think that that's gonna be the future, the far future, not my lifetime once again, but baby steps. Unless your lifetime gets extended. Well, it's in a race against time, right? A friend of mine who actually thinks about these things said, you know, you and I are gonna die, but I don't know about our grandchildren. That's, I don't know, predicting the future is hard, but that's at least a plausible scenario. And so, yeah, no, I think that as we discussed earlier, there are threats to the earth, known and unknown, right? Having spread humanity and biology elsewhere is a really important longterm goal. What kind of questions can science not currently answer, but might soon? When you think about the problems and the mysteries before us that may be within reach of science. I think an obvious one is the origin of life. We don't know how that happened. There's a difficulty in knowing how it happened historically actually, you know, literally on earth, but starting life from non life is something I kind of think we're close to, right? We're really. You really think so? Like how difficult is it to start life? Well, I've talked to people, including on the podcast about this. You know, life requires three things. Life as we know it. So there's a difference with life, which who knows what it is, and life as we know it, which we can talk about with some intelligence. So life as we know it requires compartmentalization. You need like a little membrane around your cell. Metabolism, you need to take in food and eat it and let that make you do things. And then replication, okay? So you need to have some information about who you are that you pass down to future generations. In the lab, compartmentalization seems pretty easy. Not hard to make lipid bilayers that come into little cellular walls pretty easily. Metabolism and replication are hard, but replication we're close to. People have made RNA like molecules in the lab that I think the state of the art is, they're not able to make one molecule that reproduces itself, but they're able to make two molecules that reproduce each other. So that's okay. That's pretty close. Metabolism is harder, believe it or not, even though it's sort of the most obvious thing, but you want some sort of controlled metabolism and the actual cellular machinery in our bodies is quite complicated. It's hard to see it just popping into existence all by itself. It probably took a while, but we're making progress. And in fact, I don't think we're spending nearly enough money on it. If I were the NSF, I would flood this area with money because it would change our view of the world if we could actually make life in the lab and understand how it was made originally here on earth. And I'm sure it'd have some ripple effects that help cure disease and so on. I mean, just that understanding. So synthetic biology is a wonderful big frontier where we're making cells. Right now, the best way to do that is to borrow heavily from existing biology, right? Well, Craig Venter several years ago created an artificial cell, but all he did was, not all he did, it was a tremendous accomplishment, but all he did was take out the DNA from a cell and put in entirely new DNA and let it boot up and go. What about the leap to creating intelligent life on earth? Yeah. Again, we define intelligence, of course, but let's just even say Homo sapiens, the modern intelligence in our human brain. Do you have a sense of what's involved in that leap and how big of a leap that is? So AI would count in this, or do you really want life? Do you want really an organism in some sense? AI would count, I think. Okay. Yeah, of course, of course AI would count. Well, let's say artificial consciousness, right? So I do not think we are on the threshold of creating artificial consciousness. I think it's possible. I'm not, again, very educated about how close we are, but my impression is not that we're really close because we understand how little we understand of consciousness and what it is. So if we don't have any idea what it is, it's hard to imagine we're on the threshold of making it ourselves. But it's doable, it's possible. I don't see any obstacles in principle. So yeah, I would hold out some interest in that happening eventually. I think in general, consciousness, I think we would be just surprised how easy consciousness is once we create intelligence. I think consciousness is a thing that's just something we all fake. Well, good. No, actually, I like this idea that in fact, consciousness is way less mysterious than we think because we're all at every time, at every moment, less conscious than we think we are, right? We can fool things. And I think that plus the idea that you not only have artificial intelligent systems, but you put them in a body, right, give them a robot body, that will help the faking a lot. Yeah, I think creating consciousness in artificial consciousness is as simple as asking a Roomba to say, I'm conscious, and refusing to be talked out of it. Could be, it could be. And I mean, I'm almost being silly, but that's what we do. That's what we do with each other. This is the kind of, that consciousness is also a social construct. And a lot of our ideas of intelligence is a social construct. And so reaching that bar involves something that's beyond, that doesn't necessarily involve the fundamental understanding of how you go from electrons to neurons to cognition. No, actually, I think that is an extremely good point. And in fact, what it suggests is, so yeah, you referred to Kate Darling, who I had on the podcast, and who does these experiments with very simple robots, but they look like animals, and they can look like they're experiencing pain, and we human beings react very negatively to these little robots looking like they're experiencing pain. And what you wanna say is, yeah, but they're just robots. It's not really pain, right? It's just some electrons going around. But then you realize, you and I are just electrons going around, and that's what pain is also. And so what I would have an easy time imagining is that there is a spectrum between these simple little robots that Kate works with and a human being, where there are things that sort of by some strict definition, Turing test level thing are not conscious, but nevertheless walk and talk like they're conscious. And it could be that the future is, I mean, Siri is close, right? And so it might be the future has a lot more agents like that. And in fact, rather than someday going, aha, we have consciousness, we'll just creep up on it with more and more accurate reflections of what we expect. And in the future, maybe the present, for example, we haven't met before, and you're basically assuming that I'm human as it's a high probability at this time because the yeah, but in the future, there might be question marks around that, right? Yeah, no, absolutely. Certainly videos are almost to the point where you shouldn't trust them already. Photos you can't trust, right? Videos is easier to trust, but we're getting worse that, we're getting better at faking them, right? Yeah, so physical embodied people, what's so hard about faking that? So this is very depressing, this conversation we're having right now. So I mean, To me, it's exciting. To me, you're doing it. So it's exciting to you, but it's a sobering thought. We're very bad, right? At imagining what the next 50 years are gonna be like when we're in the middle of a phase transition as we are right now. Yeah, and I, in general, I'm not blind to all the threats. I am excited by the power of technology to solve, to protect us against the threats as they evolve. I'm not as much as Steven Pinker optimistic about the world, but in everything I've seen, all of the brilliant people in the world that I've met are good people. So the army of the good in terms of the development of technology is large. Okay, you're way more optimistic than I am. I think that goodness and badness are equally distributed among intelligent and unintelligent people. I don't see much of a correlation there. Interesting. Neither of us have proof. Yeah, exactly. Again, opinions are free, right? Nor definitions of good and evil. We come without definitions or without data opinions. So what kind of questions can science not currently answer and may never be able to answer in your view? Well, the obvious one is what is good and bad? What is right and wrong? I think that there are questions that, science tells us what happens, what the world is and what it does. It doesn't say what the world should do or what we should do, because we're part of the world. But we are part of the world and we have the ability to feel like something's right, something's wrong. And to make a very long story very short, I think that the idea of moral philosophy is systematizing our intuitions of what is right and what is wrong. And science might be able to predict ahead of time what we will do, but it won't ever be able to judge whether we should have done it or not. So, you're kind of unique in terms of scientists. Listen, it doesn't have to do with podcasts, but even just reaching out, I think you referred to as sort of doing interdisciplinary science. So you reach out and talk to people that are outside of your discipline, which I always hope that's what science was for. In fact, I was a little disillusioned when I realized that academia is very siloed. Yeah. And so the question is, how, at your own level, how do you prepare for these conversations? How do you think about these conversations? How do you open your mind enough to have these conversations? And it may be a little bit broader, how can you advise other scientists to have these kinds of conversations? Not at the podcast, the fact that you're doing a podcast is awesome, other people get to hear them, but it's also good to have it without mics in general. It's a good question, but a tough one to answer. I think about a guy I know who's a personal trainer, and he was asked on a podcast, how do we psych ourselves up to do a workout? How do we make that discipline to go and work out? And he's like, why are you asking me? I can't stop working out. I don't need to psych myself up. So, and likewise, he asked me, how do you get to have interdisciplinary conversations on all sorts of different things, all sorts of different people? I'm like, that's what makes me go, right? Like that's, I couldn't stop doing that. I did that long before any of them were recorded. In fact, a lot of the motivation for starting recording it was making sure I would read all these books that I had purchased, right? Like all these books I wanted to read, not enough time to read them. And now if I have the motivation, cause I'm gonna interview Pat Churchland, I'm gonna finally read her book. You know, and it's absolutely true that academia is extraordinarily siloed, right? We don't talk to people. We rarely do. And in fact, when we do, it's punished. You know, like the people who do it successfully generally first became very successful within their little siloed discipline. And only then did they start expanding out. If you're a young person, you know, I have graduate students. I try to be very, very candid with them about this, that it's, you know, most graduate students are to not become faculty members, right? It's a tough road. And so live the life you wanna live, but do it with your eyes open about what it does to your job chances. And the more broad you are and the less time you spend hyper specializing in your field, the lower your job chances are. That's just an academic reality. It's terrible, I don't like it, but it's a reality. And for some people, that's fine. Like there's plenty of people who are wonderful scientists who have zero interest in branching out and talking to things, to anyone outside their field. But it is disillusioning to me. Some of the, you know, romantic notion I had of the intellectual academic life is belied by the reality of it. The idea that we should reach out beyond our discipline and that is a positive good is just so rare in universities that it may as well not exist at all. But that said, even though you're saying you're doing it like the personal trainer, because you just can't help it, you're also an inspiration to others. Like I could speak for myself. You know, I also have a career I'm thinking about, right? And without your podcast, I may have not have been doing this at all, right? So it makes me realize that these kinds of conversations is kind of what science is about in many ways. The reason we write papers, this exchange of ideas, is it's much harder to do interdisciplinary papers, I would say. And conversations are easier. So conversations is the beginning. And in the field of AI, it's obvious that we should think outside of pure computer vision competitions on a particular data sets. We should think about the broader impact of how this can be, you know, reaching out to physics, to psychology, to neuroscience and having these conversations so that you're an inspiration. And so never know how the world changes. I mean, the fact that this stuff is out there and I've a huge number of people come up to me, grad students, really loving the podcast, inspired by it. And they will probably have that, they'll be ripple effects when they become faculty and so on and so on. We can end on a balance between pessimism and optimism. And Sean, thank you so much for talking to me, it was awesome. No, Lex, thank you very much for this conversation. It was great.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 319,\n        \"samples\": [\n          \"Michael I. Jordan: Machine Learning, Recommender Systems, and Future of AI | Lex Fridman Podcast #74\",\n          \"Niels Jorgensen: New York Firefighters and the Heroes of 9/11 | Lex Fridman Podcast #220\",\n          \"Sean Carroll: The Nature of the Universe, Life, and Intelligence | Lex Fridman Podcast #26\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset.set_format('pandas')\n",
        "df = dataset['train'].to_pandas()\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "95b3e787-8e66-44a8-99f6-992434b500bf",
      "metadata": {
        "id": "95b3e787-8e66-44a8-99f6-992434b500bf",
        "outputId": "954736e8-1358-41a0-d406-30438284f7f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20371.0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df['Palabras por podcast'] = df['captions'].str.split().apply(len)\n",
        "df['Palabras por podcast'].median()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "928e859f-2e2f-4f33-b5cd-2a50c51ba71f",
      "metadata": {
        "id": "928e859f-2e2f-4f33-b5cd-2a50c51ba71f"
      },
      "source": [
        "### 5. Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00001f0a-87de-43c6-b2ae-d9c94f778883",
      "metadata": {
        "id": "00001f0a-87de-43c6-b2ae-d9c94f778883"
      },
      "source": [
        "Aquí podemos observar que la mediana de longitud en terminos de palabras es de 20371. Esto es esperado, pues los podcast son largos por naturaleza.\n",
        "\n",
        "Ahora, prepararémos el conjunto de datos para entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "58300f97-fb72-474c-8135-938f2673da7e",
      "metadata": {
        "id": "58300f97-fb72-474c-8135-938f2673da7e"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(max_len):\n",
        "    def _preprocess_function(examples):\n",
        "        return tokenizer(examples['captions'], max_length=max_len, truncation=True, padding='max_length')\n",
        "    return _preprocess_function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec571bbe-43be-425f-bd5e-c7904e8f5ab5",
      "metadata": {
        "id": "ec571bbe-43be-425f-bd5e-c7904e8f5ab5"
      },
      "source": [
        "Los modelos GPT no esperan otra cosa más que los `input_ids`, por lo que retirarémos todas las demás columnas del dataset ya que no nos son de utilidad en este momento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a2ba16ac-6c6b-4423-8cbc-6585847769fe",
      "metadata": {
        "id": "a2ba16ac-6c6b-4423-8cbc-6585847769fe",
        "outputId": "25868c6d-8b71-4316-a101-6659139f49b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 287\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 32\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "dataset.reset_format()\n",
        "tokenized_dataset = dataset['train'].map(preprocess_function(max_len=256), batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col != 'input_ids'])\n",
        "tokenized_dataset = tokenized_dataset.train_test_split(train_size=0.9)\n",
        "tokenized_dataset.set_format('torch')\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4119c4a6-d90b-4e4e-bdfd-993bfb11334a",
      "metadata": {
        "id": "4119c4a6-d90b-4e4e-bdfd-993bfb11334a"
      },
      "source": [
        "Finalmente procedemos a definir el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5a337587-ae60-48f0-b3c7-065d950605eb",
      "metadata": {
        "id": "5a337587-ae60-48f0-b3c7-065d950605eb"
      },
      "outputs": [],
      "source": [
        "batch_size = 2 if IN_COLAB else 1\n",
        "logging_steps = len(tokenized_dataset['train']) // batch_size\n",
        "# Definimos los parámetros globales de entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./hf-gpt',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    disable_tqdm=False,\n",
        "    logging_steps=logging_steps,\n",
        "    report_to='none'\n",
        ")\n",
        "\n",
        "# Y definimos el entrenador, especificando el modelo, datasets y el tokenizador\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['test'],\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "06f68b82-4760-4d6f-9e73-0f43574f2b2e",
      "metadata": {
        "id": "06f68b82-4760-4d6f-9e73-0f43574f2b2e",
        "outputId": "58c7db02-46a8-4e91-eb6e-907b1999be8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1440' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1440/1440 09:48, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.063700</td>\n",
              "      <td>2.843010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.632400</td>\n",
              "      <td>2.722750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.412000</td>\n",
              "      <td>2.671136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.244700</td>\n",
              "      <td>2.656195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.121500</td>\n",
              "      <td>2.658655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.005800</td>\n",
              "      <td>2.665448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.912100</td>\n",
              "      <td>2.680070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.872700</td>\n",
              "      <td>2.692358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.807300</td>\n",
              "      <td>2.703142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.774400</td>\n",
              "      <td>2.709343</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4min 37s, sys: 30 s, total: 5min 7s\n",
            "Wall time: 9min 50s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1440, training_loss=2.1821275379922653, metrics={'train_runtime': 588.9846, 'train_samples_per_second': 4.873, 'train_steps_per_second': 2.445, 'total_flos': 374832184688640.0, 'train_loss': 2.1821275379922653, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "%%time\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d02ce5b2-865e-4495-a2f8-a79e7174c7dd",
      "metadata": {
        "id": "d02ce5b2-865e-4495-a2f8-a79e7174c7dd"
      },
      "source": [
        "### 6. Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e59fc7d-626c-4941-a1c7-e34e019ba2be",
      "metadata": {
        "id": "4e59fc7d-626c-4941-a1c7-e34e019ba2be"
      },
      "source": [
        "Ahora observemos los resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3efd3a04-1cea-4cbd-ae48-6d7985fb8380",
      "metadata": {
        "id": "3efd3a04-1cea-4cbd-ae48-6d7985fb8380",
        "outputId": "5f713724-992f-4159-db11-989acbfd092b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As part of MIT course 6S099, Artificial General Intelligence, we are studying the nature of the brain and its function in the brain and in the human mind. The brain is a computer algorithm that, at its core, is the most accessible and complete visual memory in the human brain. The idea of visual memory is that we can see our environment, our environment, our brain in a way that doesn't rely on computers at all, but is largely in the brain. Our brain is in the visual cortex where we can see images and sounds, we can read and write, and, ultimately, we can understand our world, our world's future, and in general, our world. It's the brain of the 21st century. It's the brain of the 21st century, and for that, I'd like to start by talking about artificial intelligence and artificial intelligence, especially AI that is out there, in the scientific and artificial intelligence community, that is, the AI community, and that is really useful if we can understand the technology that can solve the problem and to think about how to solve it. We are interested in understanding the nature of the brain, of the brain, how it's function, and the nature of the mind. The following is a conversation with Jeff Hawkins, a researcher at MIT, and his second time on this podcast. Jeff explores AI and the artificial intelligence community at large. He also explores the psychology of AI at MIT and at Stanford. This is the Artificial General Intelligence podcast. To support it, please check out our sponsors in the description. And now, dear friends, here's Jeff Hawkins. You and I have been talking since day one, back in September, just days before MIT and the DeepMind Initiative was launched. We're going to talk about our current research on artificial intelligence and artificial intelligence, artificial intelligence, AI, and the brain from our perspective. We have conversations with a lot of scientists, philosophers, economists, psychologists, and maybe even historians, and we're looking at the future of artificial intelligence and artificial intelligence from a very different perspective, and just a different perspective, and just a new perspective. I think it's just going to be interesting, that is, as we try to explore the nature of the mind. I'll talk about the mind in general with Jeff Hawkins and his second time on this podcast. He's a professor at MIT, and I recently started on his second time, and I'm also on the research team at MIT. So for me, if you're a researcher focused on the neuroscience of artificial intelligence, and you think that it's possible to understand the nature of the mind in this way, then I think that's a very good idea. I think that the mind is the structure of the brain, and that's an active part of the brain, and there's quite a lot of information in the brain. But the mind, and the mind and the mind, the mind and the mind, that's the structure of the brain, and that's what I believe is the most fascinating to me. When we first started talking about AI research, I was a little bit of a theoretical physicist as a kid, studying the physics of biology. I was a little bit of a theoretical physicist at MIT, and I was just a theoretical physicist at Stanford, and then I was a theoretical physicist at Berkeley, and then I was a theoretical physicist at Berkeley, and then I was a theoretical physicist there, and then I was a theoretical physicist there, and then I was a theoretical physicist there, and then I was a theoretical physicists there, and then I was a theoretical physicists there. And so I'm studying those kinds of things, maybe in general, the structure, the structure of the mind, that is what I think is the most fascinating to me in this conversation with Jeff Hawkins, and I think that's the mind that is at the core of our research. And so from that perspective, I think that artificial intelligence and AI, and the brain, the mind, and the mind are really something I think that we've studied over and over and over again in science, including in my mind, in general, the mind, in general, and maybe in maybe one of the most fascinating minds in science, including probably in the field of artificial intelligence, maybe in the field of AI, which is the field of AI, and maybe in the field of AI that's a particular field, the field of AI that's a particular field in the world, which is also in general, and this is the field of AI, and so we have been studying the nature of the mind in the scientific world, the nature of the mind, and in general, the nature of the mind, and in general, in general, the mind, and in general, the mind, and in general, the mind, and in general, the mind, and in general, the mind, in general, the mind, and in general, the mind, the mind, and in general, the mind, the mind and the mind, and in general, the mind, the mind, the mind, and in general, the mind, the mind, the mind, the mind and the mind, and in general, the mind, and in general, the mind, the mind, the mind, the mind, and in general, the mind, the mind, the mind, the mind, the mind, the mind, the mind, and in general, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, and in general, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, and in general, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, and in general, the mind, the mind, the mind, the mind, and in general, the mind, the mind, the mind, the mind, and in general, the mind, the mind, the mind, the mind, the mind, and in general, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, and in general, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, and in general, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, and in general, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the mind, the\n"
          ]
        }
      ],
      "source": [
        "output = model.generate(tokens, pad_token_id=tokenizer.eos_token_id, max_length=2000, do_sample=True, temperature=0.8)\n",
        "print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a278ca09-2779-4a24-8227-6cfd878d8317",
      "metadata": {
        "id": "a278ca09-2779-4a24-8227-6cfd878d8317"
      },
      "source": [
        "Tras generar un texto de 2 000 tokens, se identificaron algunas inconsistencias, aunque el resultado es aceptable para el tamaño del modelo. No obstante, a partir de cierto punto el modelo entra en un bucle repitiendo la frase “the mind”, lo que podría deberse a patrones presentes en los datos de entrenamiento o a una configuración de decodificación poco adecuada. Este comportamiento pone de manifiesto la fuerte influencia que ejerce el conjunto de datos de entrenamiento en el desempeño final, un aspecto crucial a considerar al utilizar o realizar fine-tuning de modelos de lenguaje."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54",
      "metadata": {
        "id": "54"
      },
      "source": [
        "### 7. Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55",
      "metadata": {
        "id": "55"
      },
      "source": [
        "#### Eficacia del flujo de análisis\n",
        "\n",
        "- Al comparar arquitecturas de modelos de lenguaje como BERT y GPT, se identifican similitudes en su capacidad de generar representaciones ricas del lenguaje, algunas diferencias clave en su estructura y en su proceso de entrenamiento.\n",
        "\n",
        "- Ambos modelos demuestran que un pre-entrenamiento sólido y la construcción de embeddings de alta calidad son factores críticos para alcanzar buenos resultados en tareas posteriores, evitando costos de entrenamiento desde cero.\n",
        "\n",
        "#### Rendimiento del modelo\n",
        "\n",
        "- Tanto BERT como GPT pueden adaptarse a una amplia variedad de tareas posteriores (clasificación, generación de texto, análisis semántico, etc.), lo que valida la versatilidad de los enfoques de transfer learning y fine tuning.\n",
        "\n",
        "- La elección del modelo y la estrategia de entrenamiento depende de la tarea: BERT sobresale en comprensión y análisis de contexto bidireccional, mientras que GPT destaca en generación de texto coherente y fluido.\n",
        "\n",
        "#### Limitaciones observadas\n",
        "\n",
        "- Los modelos generativos enfrentan un dilema de exploración–explotación: una decodificación enfocada en la explotación (p. ej. greedy search) brinda mayor precisión pero tiende a producir textos monótonos; en cambio, la exploración (p. ej. sampling con temperatura alta) promueve creatividad y diversidad, pero con riesgo de incoherencia o “alucinaciones”.\n",
        "\n",
        "- La calidad del modelo depende en gran medida de los datos de entrenamiento. Conjuntos de datos sesgados, poco representativos o de baja calidad pueden degradar el desempeño e introducir sesgos o errores difíciles de corregir.\n",
        "\n",
        "#### Áreas de mejora\n",
        "\n",
        "- Profundizar en la selección y curaduría de datasets, asegurando diversidad, equilibrio y relevancia para el dominio de aplicación.\n",
        "\n",
        "- Experimentar con estrategias de decodificación y con la afinación de hiperparámetros para encontrar el punto óptimo entre creatividad, coherencia y precisión.\n",
        "\n",
        "- Explorar técnicas de optimización y compresión que permitan desplegar modelos grandes en entornos de recursos limitados.\n",
        "\n",
        "#### Valor práctico\n",
        "\n",
        "- La adopción de modelos pre-entrenados como GPT ofrece una base sólida y flexible para proyectos de NLP, equilibrando costo, tiempo y calidad.\n",
        "\n",
        "- El entendimiento de los trade-offs entre exploración y explotación, así como la adecuada selección de datos y métodos de decodificación, es esencial para alinear el modelo con los objetivos específicos del negocio y minimizar riesgos de sesgos o resultados no deseados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1089b9-ae81-4ae0-91ec-d323c7c84352",
      "metadata": {
        "id": "4b1089b9-ae81-4ae0-91ec-d323c7c84352"
      },
      "source": [
        "### 8. Apendice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "636a28e4-5609-4f01-b6f9-60343d6cb413",
      "metadata": {
        "id": "636a28e4-5609-4f01-b6f9-60343d6cb413",
        "outputId": "46f584cf-356e-4e24-9e72-9bc45abc1420",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy==1.26.4\n",
            "pandas==2.3.1\n",
            "datasets==4.0.0\n",
            "torch==2.2.0\n",
            "pytorch-lightning==2.5.5\n",
            "torchmetrics==1.4.0\n",
            "tqdm==4.67.1\n",
            "transformers==4.41.2\n",
            "scikit-learn==1.6.1\n"
          ]
        }
      ],
      "source": [
        "import pkg_resources\n",
        "\n",
        "libs = [\n",
        "    \"numpy\",\n",
        "    \"pandas\",\n",
        "    \"datasets\",\n",
        "    \"torch\",\n",
        "    \"pytorch-lightning\",\n",
        "    \"torchmetrics\",\n",
        "    \"tqdm\",\n",
        "    \"transformers\",\n",
        "    \"scikit-learn\"\n",
        "]\n",
        "\n",
        "for lib in libs:\n",
        "    try:\n",
        "        version = pkg_resources.get_distribution(lib).version\n",
        "        print(f\"{lib}=={version}\")\n",
        "    except Exception:\n",
        "        print(f\"{lib}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "646eaeca-8b47-46c4-9995-16c1f2ceb0e0",
      "metadata": {
        "id": "646eaeca-8b47-46c4-9995-16c1f2ceb0e0"
      },
      "outputs": [],
      "source": [
        " ## Solo correr en local\n",
        "\n",
        "# import nbformat\n",
        "\n",
        "## Cargar notebook\n",
        "# with open(\"nlp_with_gpt.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
        "    # nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "## Eliminar widgets corruptos si existen\n",
        "# if \"widgets\" in nb[\"metadata\"]:\n",
        "    # del nb[\"metadata\"][\"widgets\"]\n",
        "\n",
        "## Guardar reparado\n",
        "# with open(\"nlp_with_gpt.ipynb\", \"w\", encoding=\"utf-8\") as f:\n",
        "    # nbformat.write(nb, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}