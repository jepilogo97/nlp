{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jepilogo97/nlp/blob/main/chatbot-rag/chatbot_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# ChatBot - RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "##### Jean Pierre Londoño González\n",
        "##### Mini-Proyecto de chatbot utilizando RAG\n",
        "##### 28SEP2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "source": [
        "Partiendo de los conceptos establecidos en el [1-ollama-rag.ipynb](./1-ollama-rag.ipynb) ahora exploraremos una implementación con herramientas más maduras que facilitan el manejo de RAGs y LLMs. Además de eso, continuaremos el uso de [Ollama](https://ollama.com) y añadiremos el uso de [LangChain](https://www.langchain.com) un framework orientado al desarrollo de LLMs y agentes de IA. El objetivo final es el mismo, crear un RAG a partir de los documentos de wikihow. Observaremos que algunos de los pasos del notebook anterior se hacen de forma explicita.\n",
        "\n",
        "**Nota:** En este notebook no se realizará el entrenamiento de ningún modelo, se utilizarán modelos pre-entrenados y se combinaran de una forma interesante para hacer uso de cada componente a su modo y crear un agente de conversación\n",
        "\n",
        "#### Referencias\n",
        "- Dataset: https://huggingface.co/datasets/RamAnanth1/lex-fridman-podcasts\n",
        "- https://huggingface.co/EleutherAI/gpt-neo-125m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "source": [
        "### 1. Importación de librerias y carga de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "source": [
        "Inicio importando las librerías necesarias para el procesamiento de lenguaje natural, la manipulación de datos y la construcción del modelo. Esto incluye NumPy y pandas para el manejo y análisis de datos; Hugging Face Datasets y Transformers para la carga de corpus y la tokenización; y PyTorch junto con PyTorch Lightning para definir, entrenar y evaluar el modelo de manera estructurada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "232f9b07-b6e9-4e9b-96fa-697d4f04b78a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "232f9b07-b6e9-4e9b-96fa-697d4f04b78a",
        "outputId": "ebff0337-98c5-4780-d378-dde0de9ab0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2396000874.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "import pkg_resources\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "installed_packages = [package.key for package in pkg_resources.working_set]\n",
        "IN_COLAB = 'google-colab' in installed_packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5",
        "outputId": "4f0dfbaa-2e92-4291-dfc4-6af1148b7e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-28 20:37:14--  https://raw.githubusercontent.com/jepilogo97/nlp/main/chatbot-rag/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 449 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "\rrequirements.txt      0%[                    ]       0  --.-KB/s               \rrequirements.txt    100%[===================>]     449  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-28 20:37:14 (13.8 MB/s) - ‘requirements.txt’ saved [449/449]\n",
            "\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pandas==2.3.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.3.1)\n",
            "Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.0.0)\n",
            "Collecting matplotlib==3.9.2 (from -r requirements.txt (line 4))\n",
            "  Using cached matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.2.0)\n",
            "Requirement already satisfied: torchvision==0.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.17.0)\n",
            "Requirement already satisfied: lightning==2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.2.0)\n",
            "Requirement already satisfied: torchmetrics==1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: torchinfo==1.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (1.8.0)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (1.6.1)\n",
            "Requirement already satisfied: optuna==4.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (4.5.0)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (4.67.1)\n",
            "Requirement already satisfied: transformers==4.41.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (4.41.2)\n",
            "Collecting huggingface_hub==0.24.6 (from -r requirements.txt (line 14))\n",
            "  Using cached huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: evaluate==0.4.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (0.4.3)\n",
            "Requirement already satisfied: peft==0.11.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (0.11.1)\n",
            "Collecting sentence-transformers==3.0.1 (from -r requirements.txt (line 17))\n",
            "  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting ollama==0.1.7 (from -r requirements.txt (line 18))\n",
            "  Using cached ollama-0.1.7-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting langchain==0.2.16 (from -r requirements.txt (line 19))\n",
            "  Using cached langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 0.1.2\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-ollama==0.0.4 (from versions: 0.1.0rc0, 0.1.0, 0.1.1, 0.1.3, 0.2.0.dev1, 0.2.0, 0.2.1, 0.2.2rc1, 0.2.2, 0.2.3, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.5, 0.3.6, 0.3.7, 0.3.8)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-ollama==0.0.4\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!wget -O requirements.txt https://raw.githubusercontent.com/jepilogo97/nlp/main/chatbot-rag/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6",
      "metadata": {
        "id": "6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "bea6f0cb-d099-4d85-e91c-eefc1b429ae0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ollama'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1074672546.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# =======================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst_util\u001b[0m  \u001b[0;31m# Generación y comparación de embeddings de texto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mollama\u001b[0m  \u001b[0;31m# Cliente para interactuar con modelos locales/servidores de Ollama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# =======================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ollama'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# =======================\n",
        "# Procesamiento de datos y utilidades\n",
        "# =======================\n",
        "import numpy as np  # Cálculo numérico eficiente y manejo de arreglos multidimensionales\n",
        "import pandas as pd  # Manipulación y análisis de datos en estructuras tipo DataFrame\n",
        "\n",
        "# Configuración de pandas para mostrar todos los datos en consola sin recorte\n",
        "pd.set_option(\"display.max_rows\", None)     # Muestra todas las filas del DataFrame\n",
        "pd.set_option(\"display.max_columns\", None)  # Muestra todas las columnas\n",
        "pd.set_option(\"display.width\", None)        # Evita el corte de líneas largas al imprimir\n",
        "\n",
        "# =======================\n",
        "# Manejo de datasets de Hugging Face\n",
        "# =======================\n",
        "from datasets import Dataset, load_dataset, concatenate_datasets  # Carga, creación y combinación de datasets\n",
        "from collections import Counter  # Conteo de frecuencias de elementos (tokens, palabras, etc.)\n",
        "\n",
        "import matplotlib.pyplot as plt   # Visualización de datos en gráficos estáticos\n",
        "\n",
        "# =======================\n",
        "# Tipado y utilidades para funciones\n",
        "# =======================\n",
        "from typing import Callable, Dict, List, Optional, Any, Generator  # Anotaciones de tipo para mayor claridad y validación\n",
        "from abc import abstractmethod                                     # Definición de métodos abstractos en clases base\n",
        "\n",
        "# =======================\n",
        "# Progreso en bucles y NLP con Transformers\n",
        "# =======================\n",
        "from tqdm.auto import tqdm  # Barra de progreso adaptable a terminal o Jupyter\n",
        "\n",
        "# =======================\n",
        "# Sistema y utilidades\n",
        "# =======================\n",
        "import os                  # Operaciones del sistema de archivos y variables de entorno\n",
        "from huggingface_hub import login  # Autenticación y subida/descarga de modelos/datasets a Hugging Face Hub\n",
        "from pathlib import Path   # Manejo de rutas de archivos y directorios de forma multiplataforma\n",
        "import time               # Medición de tiempos y pausas en procesos\n",
        "import random             # Generación de números aleatorios\n",
        "\n",
        "# =======================\n",
        "# Modelos de embeddings y búsqueda semántica\n",
        "# =======================\n",
        "from sentence_transformers import SentenceTransformer, util as st_util  # Generación y comparación de embeddings de texto\n",
        "import ollama  # Cliente para interactuar con modelos locales/servidores de Ollama\n",
        "\n",
        "# =======================\n",
        "# Integración con LangChain\n",
        "# =======================\n",
        "from langchain_ollama import ChatOllama  # Conector LangChain para modelos de Ollama\n",
        "from langchain_community.vectorstores import FAISS  # Almacenamiento de vectores (búsqueda semántica) usando FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings  # Embeddings usando modelos de Hugging Face\n",
        "from langchain.prompts import PromptTemplate  # Plantillas para prompts\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain  # Encadenamiento de documentos en consultas\n",
        "from langchain_core.prompts import ChatPromptTemplate  # Plantillas de chat estructuradas\n",
        "from langchain.chains import (\n",
        "    create_history_aware_retriever,  # Recuperador con contexto de conversación previa\n",
        "    create_retrieval_chain,          # Cadena de recuperación de información\n",
        "    RetrievalQA                      # Pipeline de preguntas y respuestas sobre documentos\n",
        ")\n",
        "from langchain.schema import HumanMessage, AIMessage  # Representación de mensajes en el flujo conversacional\n",
        "\n",
        "# =======================\n",
        "# Interfaz web\n",
        "# =======================\n",
        "import gradio as gr  # Creación de interfaces web ligeras para demos y aplicaciones de ML\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "source": [
        "### 2. Exploración de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "source": [
        "Se utilizará el dataset RagQuAS, el cuál contiene ejemplos en una gran cantidad de dominios: Hobbies, Lingüística, Mascotas, Salud, astronomía, atención al cliente, coches, cotidiano, documentación, energía, esquí, estafas, gastronomía, hobbies, idiomas, juegos, lenguaje, manicura, música, patinaje, primeros auxilios, receta, reciclaje, reclamaciones, seguros, tenis, transporte, turismo, veterinaria, viajes, yoga.\n",
        "\n",
        "- https://huggingface.co/datasets/IIC/RagQuAS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "login()"
      ],
      "metadata": {
        "id": "VxldeIJk1ZHH"
      },
      "id": "VxldeIJk1ZHH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9",
      "metadata": {
        "id": "9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('IIC/RagQuAS', split='test')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92fe6ed-e49e-475b-b883-228d3745744e",
      "metadata": {
        "id": "f92fe6ed-e49e-475b-b883-228d3745744e"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', 100)\n",
        "dataset.set_format(type='pandas')\n",
        "df = dataset.to_pandas()\n",
        "df.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f6d06a4-d24b-440d-a35c-8d7ab4ce6477",
      "metadata": {
        "id": "1f6d06a4-d24b-440d-a35c-8d7ab4ce6477"
      },
      "outputs": [],
      "source": [
        "word_counts = (\n",
        "    df[[\"text_1\", \"text_2\", \"text_3\", \"text_4\", \"text_5\"]]\n",
        "    .apply(lambda row: sum(len(str(text).split()) for text in row), axis=1)\n",
        ")\n",
        "\n",
        "# Graficar la distribución\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "word_counts.hist(ax=ax, bins=30, edgecolor='black')\n",
        "ax.set_title(\"Distribución del número total de palabras por documento\")\n",
        "ax.set_xlabel(\"Número de palabras\")\n",
        "ax.set_ylabel(\"Frecuencia\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este notebook usarémos LangChain y FAISS para crear el document store. Por ahora vamos a saltarnos el paso de chunking ya que esta combinación de librerías realiza lo suficientemente bien la tarea con los documentos enteros."
      ],
      "metadata": {
        "id": "VZxMr8HO6k9A"
      },
      "id": "VZxMr8HO6k9A"
    },
    {
      "cell_type": "markdown",
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "source": [
        "### 2. Carga de modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!if ! type ollama > /dev/null; then curl -fsSL https://ollama.com/install.sh | sh; else echo \"Ollama ya está instalado.\"; fi"
      ],
      "metadata": {
        "id": "ig-MtWyI64WI"
      },
      "id": "ig-MtWyI64WI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext colabxterm\n",
        "%xterm"
      ],
      "metadata": {
        "id": "mQd_bbu17Adp"
      },
      "id": "mQd_bbu17Adp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este caso, vamos a trabajar con un modelo más moderno y pequeño: llama3.2:3b"
      ],
      "metadata": {
        "id": "Ufb2n8tp7DPR"
      },
      "id": "Ufb2n8tp7DPR"
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2:3b"
      ],
      "metadata": {
        "id": "oD24O0o27CHn"
      },
      "id": "oD24O0o27CHn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOllama(model=\"llama3.2:3b\", validate_model_on_init=True)\n",
        "llm.invoke(\"El primer hombre en la luna fue...\").content"
      ],
      "metadata": {
        "id": "SE-6CKJ07N9o"
      },
      "id": "SE-6CKJ07N9o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Creando el document store"
      ],
      "metadata": {
        "id": "Qe2YrFzk7X03"
      },
      "id": "Qe2YrFzk7X03"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a crear el document store. Para ello, utilizaremos la librería FAISS (a través de LangChain) que está especializada para la indexación y búsqueda de vectores."
      ],
      "metadata": {
        "id": "CCbFnRWS7hsH"
      },
      "id": "CCbFnRWS7hsH"
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
        "texts = [t[0] for t in documents['text'].to_list()]\n",
        "\n",
        "index_path = './faiss_index'\n",
        "if os.path.exists(index_path):\n",
        "    vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
        "else:\n",
        "\n",
        "    vectorstore = FAISS.from_texts(texts, embeddings)\n",
        "    vectorstore.save_local(index_path)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "ewIbud0i7Thb"
      },
      "id": "ewIbud0i7Thb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Poniendo a prueba un QA simple\n"
      ],
      "metadata": {
        "id": "DKDKs1Ay74Tl"
      },
      "id": "DKDKs1Ay74Tl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora configuamos un prompt y un RetrievalQA para nuestro caso. Aquí observamos la simpleza para crear la interfaz. Esta es una de las formas de crearla. Esta forma, según la librería está deprecada y podría ser removida en el futuro."
      ],
      "metadata": {
        "id": "u7AQecZz8OO3"
      },
      "id": "u7AQecZz8OO3"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Utiliza los siguientes fragmentos de contexto para responder la pregunta al final.\n",
        "    Si no sabes la respuesta, di que no lo sabes.\n",
        "    No menciones que te he proporcionado fragmentos, simula que ya tenías esta información en tu conocimiento y responde como en una conversación natural.\n",
        "    Incluye las citas a las fuentes de información\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Pregunta: {question}\n",
        "    Respuesta Útil:\"\"\"\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "result = qa_chain({\"query\": \"Qué es el feng shui?\"})\n",
        "\n",
        "print(result[\"result\"])\n",
        "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
        "    idx = vectorstore.index.reconstruct_n(0, vectorstore.index.ntotal)[0]\n",
        "    try:\n",
        "        original_idx = texts.index(doc.page_content)\n",
        "    except ValueError:\n",
        "        original_idx = None\n",
        "\n",
        "    if original_idx is not None:\n",
        "        meta = documents[original_idx]\n",
        "        print(f\"- [{i}] {meta['title'][0]} ({meta['url'][0]})\")\n",
        "    else:\n",
        "        print(f\"- [{i}] Unknown source: {doc.page_content[:50]}...\")"
      ],
      "metadata": {
        "id": "RBDEjgM-8Ug5"
      },
      "id": "RBDEjgM-8Ug5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora lo intentamos de nuevo con la forma recomendada. Observamos que no es muy diferente, salvo el template y el nombre de los inputs. Otra ventaja de esta forma es que en la respuesta obtenemos automáticamente el contexto en forma de los documentos que fueron recuperados para construir la respuesta. Ya no es necesario pasar explicitamente una propiedad para recuperarlos."
      ],
      "metadata": {
        "id": "6awpBiys-NV7"
      },
      "id": "6awpBiys-NV7"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Utiliza los siguientes fragmentos de contexto para responder la pregunta al final.\n",
        "    Si no sabes la respuesta, di que no lo sabes.\n",
        "    No menciones que te he proporcionado fragmentos, simula que ya tenías esta información en tu conocimiento y responde como en una conversación natural.\n",
        "    Incluye las citas a las fuentes de información\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Pregunta: {input}\n",
        "    Respuesta Útil:\"\"\"\n",
        ")\n",
        "\n",
        "def format_answer(reply):\n",
        "  answer = reply[\"answer\"] + \"\\n\"\n",
        "  for i, doc in enumerate(reply[\"context\"], 1):\n",
        "    idx = vectorstore.index.reconstruct_n(0, vectorstore.index.ntotal)[0]\n",
        "    try:\n",
        "        original_idx = texts.index(doc.page_content)\n",
        "    except ValueError:\n",
        "        original_idx = None\n",
        "\n",
        "    if original_idx is not None:\n",
        "        meta = documents[original_idx]\n",
        "        answer += f\"- [{i}] {meta['title'][0]} ({meta['url'][0]})\\n\"\n",
        "    else:\n",
        "        answer += f\"- [{i}] Unknown source: {doc.page_content[:50]}...\\n\"\n",
        "  return answer\n",
        "\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "qa_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
        "\n",
        "result = qa_chain.invoke({\"input\": \"Qué es el feng shui?\"})\n",
        "\n",
        "print(format_answer(result))"
      ],
      "metadata": {
        "id": "Qvjh6zwt8mDu"
      },
      "id": "Qvjh6zwt8mDu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y una vez más, observamos que el artículo del feng shui aparece en las referencias obtenidas.\n",
        "\n",
        "Ahora, si hacemos una nueva pregunta observamos que las referencias cambian. Es buena señal de que nuestro RAG está funcionando como se debe."
      ],
      "metadata": {
        "id": "Orw7bw0J8yhC"
      },
      "id": "Orw7bw0J8yhC"
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain.invoke({\"input\": \"Cómo usar un computador?\"})\n",
        "print(format_answer(result))"
      ],
      "metadata": {
        "id": "pM3gGknP8n7A"
      },
      "id": "pM3gGknP8n7A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que por lo menos dentro de los resultados hemos recuperado el articulo sobre feng shui, lo cual nos indica que el retriever si podría entregarnos resultados relevantes."
      ],
      "metadata": {
        "id": "4qVb23QM82CD"
      },
      "id": "4qVb23QM82CD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Creando una cadena conversacional"
      ],
      "metadata": {
        "id": "5kGesj6M844l"
      },
      "id": "5kGesj6M844l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "El ejemplo anterior sirve más que todo para responder únicas preguntas, no guarda el historial de conversación por lo que la cadena no puede usarla como parte del contexto.\n",
        "\n",
        "Esto lo podemos arreglar mediante un history retrieval:"
      ],
      "metadata": {
        "id": "YMrnV4g5-m5t"
      },
      "id": "YMrnV4g5-m5t"
    },
    {
      "cell_type": "code",
      "source": [
        "condense_question_system_template = (\n",
        "    \"Utiliza el historial de conversación y fragmentos de contexto para reformular la pregunta al final sin tener que incluir todo el historial.\"\n",
        "    \"No menciones que te he proporcionado fragmentos, simula que ya tenías esta información en tu conocimiento y responde como en una conversación natural.\"\n",
        ")\n",
        "\n",
        "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", condense_question_system_template),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, condense_question_prompt\n",
        ")\n",
        "\n",
        "system_prompt = (\n",
        "    \"Eres un asistente para tareas de tipo preguntas y respuestas.\"\n",
        "    \"Utiliza las siguientes piezas del contexto recuperado para responder la pregunta.\"\n",
        "    \"Si no sabes la respuesta, di que no lo sabes.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "convo_qa_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
        "\n",
        "chat_history = []\n",
        "response = convo_qa_chain.invoke(\n",
        "    {\n",
        "        \"input\": \"qué es el feng shui?\",\n",
        "        \"chat_history\": chat_history,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Aquí debemos ir construyendo el historial para pasarlo a subsecuentes invocaciones\n",
        "chat_history.append(HumanMessage(content=response[\"input\"]))\n",
        "chat_history.append(AIMessage(content=response[\"answer\"]))\n",
        "print(format_answer(response))\n",
        "\n"
      ],
      "metadata": {
        "id": "DgNcxIlW8-p3"
      },
      "id": "DgNcxIlW8-p3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "pgHVSVj59Zhk"
      },
      "id": "pgHVSVj59Zhk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Lanzando la interfáz de usuario del ChatBot"
      ],
      "metadata": {
        "id": "4mk_KETx_ACs"
      },
      "id": "4mk_KETx_ACs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, utilizarémos gradio nuevamente para construir la interfáz conversacional.\n",
        "\n",
        "Nótese el manejo que le damos al historial de la conversación. Tenémos dos:\n",
        "\n",
        "1. `lc_history`: Exclusivo al historial de LangChain, contiene estructuras propias de LangChain\n",
        "2. `chat_history`: Es el historial de la interfaz de gradio, es mas simple y no debe mezclarse con el otro."
      ],
      "metadata": {
        "id": "_DUxQADi_Dce"
      },
      "id": "_DUxQADi_Dce"
    },
    {
      "cell_type": "code",
      "source": [
        "convo_qa_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
        "lc_history = []\n",
        "\n",
        "with gr.Blocks() as gr_blocks:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(\n",
        "        label=\"Sobre qué quieres conversar?\",\n",
        "        placeholder=\"Ház tu pregunta aquí y presiona enter.\"\n",
        "    )\n",
        "    clear = gr.Button(\"Limpiar\")\n",
        "\n",
        "    def respond(question, chat_history):\n",
        "        reply = convo_qa_chain.invoke({\"input\": question, \"chat_history\": lc_history})\n",
        "\n",
        "        lc_history.append(HumanMessage(content=question))\n",
        "        lc_history.append(AIMessage(content=reply[\"answer\"]))\n",
        "\n",
        "        answer = format_answer(reply)\n",
        "        chat_history.append((question, answer))\n",
        "        return \"\", chat_history\n",
        "\n",
        "    def reset_chat():\n",
        "        lc_history.clear()\n",
        "        return \"\"\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "    clear.click(reset_chat, None, chatbot, queue=False)\n",
        "\n",
        "gr_blocks.launch(inline=False)"
      ],
      "metadata": {
        "id": "vkWhZ34e_E6p"
      },
      "id": "vkWhZ34e_E6p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr_blocks.close()"
      ],
      "metadata": {
        "id": "GTg62h1Y_PX-"
      },
      "id": "GTg62h1Y_PX-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "54",
      "metadata": {
        "id": "54"
      },
      "source": [
        "### 7. Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55",
      "metadata": {
        "id": "55"
      },
      "source": [
        "#### Eficacia del flujo de análisis\n",
        "\n",
        "- Al comparar arquitecturas de modelos de lenguaje como BERT y GPT, se identifican similitudes en su capacidad de generar representaciones ricas del lenguaje, algunas diferencias clave en su estructura y en su proceso de entrenamiento.\n",
        "\n",
        "- Ambos modelos demuestran que un pre-entrenamiento sólido y la construcción de embeddings de alta calidad son factores críticos para alcanzar buenos resultados en tareas posteriores, evitando costos de entrenamiento desde cero.\n",
        "\n",
        "#### Rendimiento del modelo\n",
        "\n",
        "- Tanto BERT como GPT pueden adaptarse a una amplia variedad de tareas posteriores (clasificación, generación de texto, análisis semántico, etc.), lo que valida la versatilidad de los enfoques de transfer learning y fine tuning.\n",
        "\n",
        "- La elección del modelo y la estrategia de entrenamiento depende de la tarea: BERT sobresale en comprensión y análisis de contexto bidireccional, mientras que GPT destaca en generación de texto coherente y fluido.\n",
        "\n",
        "#### Limitaciones observadas\n",
        "\n",
        "- Los modelos generativos enfrentan un dilema de exploración–explotación: una decodificación enfocada en la explotación (p. ej. greedy search) brinda mayor precisión pero tiende a producir textos monótonos; en cambio, la exploración (p. ej. sampling con temperatura alta) promueve creatividad y diversidad, pero con riesgo de incoherencia o “alucinaciones”.\n",
        "\n",
        "- La calidad del modelo depende en gran medida de los datos de entrenamiento. Conjuntos de datos sesgados, poco representativos o de baja calidad pueden degradar el desempeño e introducir sesgos o errores difíciles de corregir.\n",
        "\n",
        "#### Áreas de mejora\n",
        "\n",
        "- Profundizar en la selección y curaduría de datasets, asegurando diversidad, equilibrio y relevancia para el dominio de aplicación.\n",
        "\n",
        "- Experimentar con estrategias de decodificación y con la afinación de hiperparámetros para encontrar el punto óptimo entre creatividad, coherencia y precisión.\n",
        "\n",
        "- Explorar técnicas de optimización y compresión que permitan desplegar modelos grandes en entornos de recursos limitados.\n",
        "\n",
        "#### Valor práctico\n",
        "\n",
        "- La adopción de modelos pre-entrenados como GPT ofrece una base sólida y flexible para proyectos de NLP, equilibrando costo, tiempo y calidad.\n",
        "\n",
        "- El entendimiento de los trade-offs entre exploración y explotación, así como la adecuada selección de datos y métodos de decodificación, es esencial para alinear el modelo con los objetivos específicos del negocio y minimizar riesgos de sesgos o resultados no deseados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1089b9-ae81-4ae0-91ec-d323c7c84352",
      "metadata": {
        "id": "4b1089b9-ae81-4ae0-91ec-d323c7c84352"
      },
      "source": [
        "### 8. Apendice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636a28e4-5609-4f01-b6f9-60343d6cb413",
      "metadata": {
        "id": "636a28e4-5609-4f01-b6f9-60343d6cb413"
      },
      "outputs": [],
      "source": [
        "import pkg_resources\n",
        "\n",
        "libs = [\n",
        "    \"numpy\",\n",
        "    \"pandas\",\n",
        "    \"datasets\",\n",
        "    \"torch\",\n",
        "    \"pytorch-lightning\",\n",
        "    \"torchmetrics\",\n",
        "    \"tqdm\",\n",
        "    \"transformers\",\n",
        "    \"scikit-learn\"\n",
        "]\n",
        "\n",
        "for lib in libs:\n",
        "    try:\n",
        "        version = pkg_resources.get_distribution(lib).version\n",
        "        print(f\"{lib}=={version}\")\n",
        "    except Exception:\n",
        "        print(f\"{lib}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646eaeca-8b47-46c4-9995-16c1f2ceb0e0",
      "metadata": {
        "id": "646eaeca-8b47-46c4-9995-16c1f2ceb0e0"
      },
      "outputs": [],
      "source": [
        " ## Solo correr en local\n",
        "\n",
        "# import nbformat\n",
        "\n",
        "## Cargar notebook\n",
        "# with open(\"nlp_with_gpt.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
        "    # nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "## Eliminar widgets corruptos si existen\n",
        "# if \"widgets\" in nb[\"metadata\"]:\n",
        "    # del nb[\"metadata\"][\"widgets\"]\n",
        "\n",
        "## Guardar reparado\n",
        "# with open(\"nlp_with_gpt.ipynb\", \"w\", encoding=\"utf-8\") as f:\n",
        "    # nbformat.write(nb, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}