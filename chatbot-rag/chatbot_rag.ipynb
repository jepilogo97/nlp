{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07822504",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jepilogo97/nlp/blob/main/nlp-with-gpt/nlp_with_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "# ChatBot - RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "1"
   },
   "source": [
    "##### Jean Pierre Londoño González\n",
    "##### Mini-Proyecto de chatbot utilizando RAG\n",
    "##### 28SEP2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "2"
   },
   "source": [
    "Partiendo de los conceptos establecidos en el [1-ollama-rag.ipynb](./1-ollama-rag.ipynb) ahora exploraremos una implementación con herramientas más maduras que facilitan el manejo de RAGs y LLMs. Además de eso, continuaremos el uso de [Ollama](https://ollama.com) y añadiremos el uso de [LangChain](https://www.langchain.com) un framework orientado al desarrollo de LLMs y agentes de IA. El objetivo final es el mismo, crear un RAG a partir de los documentos de wikihow. Observaremos que algunos de los pasos del notebook anterior se hacen de forma explicita.\n",
    "\n",
    "**Nota:** En este notebook no se realizará el entrenamiento de ningún modelo, se utilizarán modelos pre-entrenados y se combinaran de una forma interesante para hacer uso de cada componente a su modo y crear un agente de conversación\n",
    "\n",
    "#### Referencias\n",
    "- Dataset: https://huggingface.co/datasets/RamAnanth1/lex-fridman-podcasts\n",
    "- https://huggingface.co/EleutherAI/gpt-neo-125m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "3"
   },
   "source": [
    "### 1. Importación de librerias y carga de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "4"
   },
   "source": [
    "Inicio importando las librerías necesarias para el procesamiento de lenguaje natural, la manipulación de datos y la construcción del modelo. Esto incluye NumPy y pandas para el manejo y análisis de datos; Hugging Face Datasets y Transformers para la carga de corpus y la tokenización; y PyTorch junto con PyTorch Lightning para definir, entrenar y evaluar el modelo de manera estructurada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232f9b07-b6e9-4e9b-96fa-697d4f04b78a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "232f9b07-b6e9-4e9b-96fa-697d4f04b78a",
    "outputId": "e5dea922-a470-44f7-da49-d21f7739a7bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jepil\\AppData\\Local\\Temp\\ipykernel_7308\\2396000874.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "installed_packages = [package.key for package in pkg_resources.working_set]\n",
    "IN_COLAB = 'google-colab' in installed_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5",
    "outputId": "bf1bc792-e5f5-420c-f40a-136e125bb44d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "!wget -O requirements.txt https://raw.githubusercontent.com/jepilogo97/nlp/main/chatbot-rag/requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6",
   "metadata": {
    "id": "6"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing lib: No se encontró el proceso especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Todas las columnas\u001b[39;00m\n\u001b[0;32m      7\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.width\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)        \u001b[38;5;66;03m# No cortar líneas\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, load_dataset, concatenate_datasets  \u001b[38;5;66;03m# Carga y combinación de datasets de Hugging Face\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_dict\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetDict\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter  \u001b[38;5;66;03m# Conteo de frecuencias de elementos (tokens, palabras, etc.)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\datasets\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.3.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\datasets\\arrow_dataset.py:62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m url_to_fs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\pyarrow\\__init__.py:65\u001b[0m\n\u001b[0;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[0;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[0;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing lib: No se encontró el proceso especificado."
     ]
    }
   ],
   "source": [
    "# Procesamiento de lenguaje natural y utilidades\n",
    "import numpy as np  # Cálculo numérico y manejo de arreglos multidimensionales\n",
    "import pandas as pd  # Manipulación y análisis de datos en estructuras tipo DataFrame\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)     # Todas las filas\n",
    "pd.set_option(\"display.max_columns\", None)  # Todas las columnas\n",
    "pd.set_option(\"display.width\", None)        # No cortar líneas\n",
    "\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets  # Carga y combinación de datasets de Hugging Face\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from collections import Counter  # Conteo de frecuencias de elementos (tokens, palabras, etc.)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Deep Learning con PyTorch\n",
    "import torch  # Librería principal de tensores y operaciones en GPU/CPU\n",
    "import torch.nn as nn  # Definición de capas y módulos de redes neuronales\n",
    "import torch.nn.functional as F  # Funciones de activación y operaciones matemáticas de redes\n",
    "from torch.utils.data import random_split, DataLoader, Subset  # Utilidades para crear y dividir datasets, cargar lotes y trabajar con subconjuntos\n",
    "from torchinfo import summary\n",
    "\n",
    "# Entrenamiento estructurado con PyTorch Lightning\n",
    "from pytorch_lightning import LightningModule, Trainer  # Clase base y manejador de entrenamiento de modelos\n",
    "from pytorch_lightning.loggers import TensorBoardLogger  # Registro de métricas e historial en TensorBoard\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint  # Detener entrenamiento si no mejora la métrica\n",
    "from torchmetrics import Accuracy  # Métrica de precisión para clasificación supervisada\n",
    "\n",
    "# Tipado para mayor legibilidad y validación de funciones\n",
    "from typing import Optional, Tuple # Definición de tipos de datos para funciones y estructuras\n",
    "\n",
    "from tqdm.auto import tqdm  # Barra de progreso adaptable para bucles\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments  # Tokenizador automático de modelos preentrenados de Hugging Face\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode  # Conversión de bytes a caracteres Unicode (usado en tokenización tipo GPT-2)\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "\n",
    "\n",
    "# Métricas de evaluación con Scikit-learn\n",
    "from sklearn.model_selection import train_test_split  # División de datos en conjuntos de entrenamiento y prueba\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # Métricas de evaluación de modelos de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "7"
   },
   "source": [
    "### 2. Exploración de dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "source": [
    "Se utilizará el dataset RagQuAS, el cuál contiene ejemplos en una gran cantidad de dominios: Hobbies, Lingüística, Mascotas, Salud, astronomía, atención al cliente, coches, cotidiano, documentación, energía, esquí, estafas, gastronomía, hobbies, idiomas, juegos, lenguaje, manicura, música, patinaje, primeros auxilios, receta, reciclaje, reclamaciones, seguros, tenis, transporte, turismo, veterinaria, viajes, yoga.\n",
    "\n",
    "- https://huggingface.co/datasets/IIC/RagQuAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9",
    "outputId": "7e1fa604-c357-4f57-f696-23944f144717",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('IIC/RagQuAS')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fe6ed-e49e-475b-b883-228d3745744e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f92fe6ed-e49e-475b-b883-228d3745744e",
    "outputId": "0a2851a3-64f0-4c46-80e5-888eaba5773c"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "dataset.set_format(type='pandas')\n",
    "df = dataset.to_pandas()\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d06a4-d24b-440d-a35c-8d7ab4ce6477",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f6d06a4-d24b-440d-a35c-8d7ab4ce6477",
    "outputId": "ec882483-3208-4b21-de08-f07f10dbb8e2"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "df.document.apply(lambda text: len(text.split(' '))).hist(ax=ax)\n",
    "ax.set_title(\"Distribución por número de palabras por documento\")\n",
    "ax.set_xlabel(\"Número de palabras\")\n",
    "ax.set_ylabel(\"Frecuencia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f95fa-a576-4846-ba55-4d6248e298a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 5000\n",
    "documents = dataset.take(limit).map(lambda example: {\"url\": example['url'], \"title\": example['title'], \"text\": example['title'] + '\\n' + example['summary'] + '\\n' + example['document']})\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "20"
   },
   "source": [
    "### 2. Implementando una función de generación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "21"
   },
   "source": [
    "Ahora, la idea es que este modelo nos sirva para generar texto de forma recurrente e incremental. En la última capa de los modelos tipo GPT encontrarémos un tensor con forma $(b, s, v)$, donde:\n",
    "\n",
    "- $b$: Es el tamaño del bache, o la cantidad de secuencias a procesar.\n",
    "- $s$: Es la longitud de la secuencia de entrada.\n",
    "- $v$: Es el tamaño del vocabulario del modelo, cuantos tokens soporta.\n",
    "\n",
    "Pero este es el tensor de salida, por qué tiene la forma de la secuencia de entrada?, porque cada posición en la salida corresponde a la la predicción del siguiente token de esa posición en la secuencia de entrada. En otras palabras, lo que obtenemos como predicción, es una secuencia de igual tamaño a la de entrada, movida un token hacia adelante, lo que efectivamente nos predice un solo token a la vez y por ende, el token que nos insteresa, es el último.\n",
    "\n",
    "Lo que obtenemos en este tensor es además los logits de TODO el vocabulario del modelo, con los cuales podemos calcular las probabilidades de que cada uno sea el que continue en la secuencia. Hay varias formas de decodificar el siguiente token, la más fácil de implementar sería una decodificación codiciosa (greedy) del siguiente token, que consiste simplemente en seleccionar el token con la probabilidad más alta. Este es un enfoque simple y efectivo para algunos casos, pero al mismo tiempo sufre de poca variabilidad e incluso puede caer en generación repetitiva.\n",
    "\n",
    "Otra opción es el muestreo, ya que justamente podemos obtener probabilidades del siguiente token, lo más lógico sería muestrear con esas opciones de probabilidad, de este modo podemos obtener mayor diversidad a la hora de generar el texto, al costo eso si de que haya mayor aleatoridad ya que se le daría la oportunidad a incluso tokens con baja probabilidad, de ser seleccionados.\n",
    "\n",
    "Otra opción podría ser un balanceo entre una decodificación greedy y una por muestreo, en función de otro hiperparámetro que podemos definir. Esta sería una técnica muy común en el contexto de Reinforcement Learning llamade e-greedy. Se hace la aclaración de que en este ejemplo no harémos nada de RL, solamente se hace mención de esta técnica para balancear entre explotación y exploración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e473a39-4f5c-4ee4-80b5-c84a5eaa6094",
   "metadata": {
    "id": "1e473a39-4f5c-4ee4-80b5-c84a5eaa6094"
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "        model: nn.Module,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        start: str,\n",
    "        max_length: int = 1000,\n",
    "        eps: float = 0.5,\n",
    "        top_n: int = 5,\n",
    "        return_iterations: bool = False,\n",
    "        device: str = \"cpu\") -> Tuple[str, Optional[pd.DataFrame]]:\n",
    "\n",
    "    output = [start]\n",
    "    iterations = []\n",
    "    with torch.no_grad():\n",
    "        input_ids = tokenizer(output[-1], return_tensors='pt')['input_ids'].to(device)\n",
    "        for _ in range(max_length):\n",
    "            # Tomamos los logits producidos por la última capa del modelo\n",
    "            # Estos corresponden al siguiente token por cada posición de la cadena\n",
    "            logits = model(input_ids=input_ids).logits\n",
    "            # Por lo tanto, el que nos interesa es el último, que correspondería a la\n",
    "            # predicción del siguiente token después del final de la cadena original\n",
    "            # A este aplicamos un softmax para obtener las probabilidades por cada\n",
    "            # token del vocabulario para estar presente en la cadena.\n",
    "            probs = torch.softmax(logits[0, -1, :], dim=-1)\n",
    "            # Simplemente ordenamos por probabilidad de forma descendente\n",
    "            sorted_tokens = torch.argsort(probs, dim=-1, descending=True)\n",
    "\n",
    "            # Utilizamos una politica tipo e-greedy para obtener el siguiente token de la secuencia\n",
    "            # Un eps>=1 quiere decir que siempre se va seleccionar el token de forma 'greedy', es decir\n",
    "            # siempre se toma el token con probabilidad más alta.\n",
    "\n",
    "            # Un eps=0 quiere decir que siempre se va a muestrear el siguiente token en función\n",
    "            # de las probabilidades de cada token\n",
    "\n",
    "            # Un 0<eps<1 va a balancear de forma binomial entre tomar el token con la\n",
    "            # probabilidad más alta y muestrear el token en función de sus probabilidades.\n",
    "            if np.random.random_sample(1)[0] < eps:\n",
    "                # Se toma el mejor token\n",
    "                next_token = sorted_tokens[0].unsqueeze(dim=0)\n",
    "            else:\n",
    "                # Se muetrea el token de la probabilidad de distribución\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "            if return_iterations:\n",
    "                # Mantenemos pista de todas las iteraciones para análisis\n",
    "                iteration = {'input': ''.join(output)}\n",
    "                best_n = sorted_tokens[:top_n].cpu().tolist()\n",
    "                choices = {f'Choice #{choice+1}': f'{tokenizer.decode(token)} ({prob:.4f})' for choice, (token, prob) in enumerate(zip(best_n, probs[best_n].cpu().tolist()))}\n",
    "                iteration.update(choices)\n",
    "                iterations.append(iteration)\n",
    "\n",
    "            output.append(tokenizer.decode(next_token))\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(dim=0)], dim=-1)\n",
    "\n",
    "        output_text = ''.join(output)\n",
    "        if not return_iterations:\n",
    "            return output_text, None\n",
    "        else:\n",
    "            df = pd.DataFrame(iterations)\n",
    "            return output_text, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba98f34-26fc-4f8d-9340-99950a1cdba1",
   "metadata": {
    "id": "cba98f34-26fc-4f8d-9340-99950a1cdba1"
   },
   "source": [
    "Ahora observemos que pasa cuando generamos texto con nuestra función y algunos parámetros.\n",
    "\n",
    "Primero, observemos que pasa cuando pasamos un `eps=1` que quiere decir que la generación va a ser de tipo greedy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476e165-88bd-4612-8a85-38187028cae6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a476e165-88bd-4612-8a85-38187028cae6",
    "outputId": "e05161f9-933a-4a07-b0d7-0a60d9d7f4d1"
   },
   "outputs": [],
   "source": [
    "output_text, iterations_df = generate(model, tokenizer, text, max_length=15, eps=1.0, top_n=10, return_iterations=True, device=device)\n",
    "print(output_text)\n",
    "iterations_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb81ac-2dec-44ca-a289-40abc66f20fc",
   "metadata": {
    "id": "d7fb81ac-2dec-44ca-a289-40abc66f20fc"
   },
   "source": [
    "Observamos como el input progresa a la vez que las opciones de tokens que hay. Sin importar cuantas veces invoquemos a la función con los mismos parámetros, siempre vamos a obtener los mismos resultados.\n",
    "\n",
    "Ahora, observemos que pasa si introducimos exploración al reducir el `eps=0.5`, lo cual nos dice que aproximadamente la mitad de las veces va a elegir el siguiente token muestreando y la otra mitad explotando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b01b10-52a5-4133-a552-1df11138a136",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77b01b10-52a5-4133-a552-1df11138a136",
    "outputId": "2eea83a8-0751-4ac5-85d6-b73f6fa5b2bb"
   },
   "outputs": [],
   "source": [
    "output_text, _ = generate(model, tokenizer, text, max_length=100, eps=0.5, device=device)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3rX7rhp3qwbc",
   "metadata": {
    "id": "3rX7rhp3qwbc"
   },
   "source": [
    "Lo primero observado es que aparecen extrañamente muchos signos de interrogación dentro del texto generado.\n",
    "Al intentar aumentar el max length se evidencia que es demasiado costoso, pasar de 100 a 1000 toma un tiempo bastante considerable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "28"
   },
   "source": [
    "### 3. Generando texto con las utilidades del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "29"
   },
   "source": [
    "Ahora, la clase de Huggingface implementa la función `generate` que hace la labor de generación por nosotros, incluyendo las opciones de muestreo y explotación como hemos observado. Solo que además permite otra serie de parámetros y opciones para controlar la generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717a0ac-3717-4d5b-a858-d6763cd859cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4717a0ac-3717-4d5b-a858-d6763cd859cb",
    "outputId": "9827c2f4-d5fa-4aa6-fc0b-5264d6f81001"
   },
   "outputs": [],
   "source": [
    "output = model.generate(tokens, pad_token_id=tokenizer.eos_token_id, max_length=100, do_sample=True, temperature=0.5, top_k=0)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R05T73HcrHcj",
   "metadata": {
    "id": "R05T73HcrHcj"
   },
   "source": [
    "La exploración de la temperatura promueve creatividad y diversidad, pero con casos que evidencian incoherencia o “alucinaciones”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0bf6ea-a2bb-4276-b355-9e2083eda65f",
   "metadata": {
    "id": "6e0bf6ea-a2bb-4276-b355-9e2083eda65f"
   },
   "source": [
    "### 4. Carga de dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56251dff-d7d1-42dc-82d8-4cafa039d2dd",
   "metadata": {
    "id": "56251dff-d7d1-42dc-82d8-4cafa039d2dd"
   },
   "source": [
    "Ahora, intentemos hacer fine tuning a nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b6c2ae-c471-467f-a22d-08363a8e36b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7b6c2ae-c471-467f-a22d-08363a8e36b6",
    "outputId": "cfb43e98-03a3-49a7-fd91-9ad24671312e"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"RamAnanth1/lex-fridman-podcasts\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb3f821-996a-4ada-9cb4-b1c50765c020",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cb3f821-996a-4ada-9cb4-b1c50765c020",
    "outputId": "2f5615b5-3af9-4bbd-9ef7-8e75936c626e"
   },
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58999edc-3464-4fe1-8e02-f0248ac6ad48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "58999edc-3464-4fe1-8e02-f0248ac6ad48",
    "outputId": "54657d7d-2f6b-4bca-aa9b-7676dfa98328"
   },
   "outputs": [],
   "source": [
    "dataset.set_format('pandas')\n",
    "df = dataset['train'].to_pandas()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3e787-8e66-44a8-99f6-992434b500bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95b3e787-8e66-44a8-99f6-992434b500bf",
    "outputId": "954736e8-1358-41a0-d406-30438284f7f2"
   },
   "outputs": [],
   "source": [
    "df['Palabras por podcast'] = df['captions'].str.split().apply(len)\n",
    "df['Palabras por podcast'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e859f-2e2f-4f33-b5cd-2a50c51ba71f",
   "metadata": {
    "id": "928e859f-2e2f-4f33-b5cd-2a50c51ba71f"
   },
   "source": [
    "### 5. Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00001f0a-87de-43c6-b2ae-d9c94f778883",
   "metadata": {
    "id": "00001f0a-87de-43c6-b2ae-d9c94f778883"
   },
   "source": [
    "Aquí podemos observar que la mediana de longitud en terminos de palabras es de 20371. Esto es esperado, pues los podcast son largos por naturaleza.\n",
    "\n",
    "Ahora, prepararémos el conjunto de datos para entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58300f97-fb72-474c-8135-938f2673da7e",
   "metadata": {
    "id": "58300f97-fb72-474c-8135-938f2673da7e"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(max_len):\n",
    "    def _preprocess_function(examples):\n",
    "        return tokenizer(examples['captions'], max_length=max_len, truncation=True, padding='max_length')\n",
    "    return _preprocess_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec571bbe-43be-425f-bd5e-c7904e8f5ab5",
   "metadata": {
    "id": "ec571bbe-43be-425f-bd5e-c7904e8f5ab5"
   },
   "source": [
    "Los modelos GPT no esperan otra cosa más que los `input_ids`, por lo que retirarémos todas las demás columnas del dataset ya que no nos son de utilidad en este momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba16ac-6c6b-4423-8cbc-6585847769fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2ba16ac-6c6b-4423-8cbc-6585847769fe",
    "outputId": "25868c6d-8b71-4316-a101-6659139f49b5"
   },
   "outputs": [],
   "source": [
    "dataset.reset_format()\n",
    "tokenized_dataset = dataset['train'].map(preprocess_function(max_len=256), batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col != 'input_ids'])\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(train_size=0.9)\n",
    "tokenized_dataset.set_format('torch')\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4119c4a6-d90b-4e4e-bdfd-993bfb11334a",
   "metadata": {
    "id": "4119c4a6-d90b-4e4e-bdfd-993bfb11334a"
   },
   "source": [
    "Finalmente procedemos a definir el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a337587-ae60-48f0-b3c7-065d950605eb",
   "metadata": {
    "id": "5a337587-ae60-48f0-b3c7-065d950605eb"
   },
   "outputs": [],
   "source": [
    "batch_size = 2 if IN_COLAB else 1\n",
    "logging_steps = len(tokenized_dataset['train']) // batch_size\n",
    "# Definimos los parámetros globales de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./hf-gpt',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Y definimos el entrenador, especificando el modelo, datasets y el tokenizador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f68b82-4760-4d6f-9e73-0f43574f2b2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "06f68b82-4760-4d6f-9e73-0f43574f2b2e",
    "outputId": "58c7db02-46a8-4e91-eb6e-907b1999be8a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ce5b2-865e-4495-a2f8-a79e7174c7dd",
   "metadata": {
    "id": "d02ce5b2-865e-4495-a2f8-a79e7174c7dd"
   },
   "source": [
    "### 6. Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59fc7d-626c-4941-a1c7-e34e019ba2be",
   "metadata": {
    "id": "4e59fc7d-626c-4941-a1c7-e34e019ba2be"
   },
   "source": [
    "Ahora observemos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd3a04-1cea-4cbd-ae48-6d7985fb8380",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3efd3a04-1cea-4cbd-ae48-6d7985fb8380",
    "outputId": "5f713724-992f-4159-db11-989acbfd092b"
   },
   "outputs": [],
   "source": [
    "output = model.generate(tokens, pad_token_id=tokenizer.eos_token_id, max_length=2000, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278ca09-2779-4a24-8227-6cfd878d8317",
   "metadata": {
    "id": "a278ca09-2779-4a24-8227-6cfd878d8317"
   },
   "source": [
    "Tras generar un texto de 2 000 tokens, se identificaron algunas inconsistencias, aunque el resultado es aceptable para el tamaño del modelo. No obstante, a partir de cierto punto el modelo entra en un bucle repitiendo la frase “the mind”, lo que podría deberse a patrones presentes en los datos de entrenamiento o a una configuración de decodificación poco adecuada. Este comportamiento pone de manifiesto la fuerte influencia que ejerce el conjunto de datos de entrenamiento en el desempeño final, un aspecto crucial a considerar al utilizar o realizar fine-tuning de modelos de lenguaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "id": "54"
   },
   "source": [
    "### 7. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "id": "55"
   },
   "source": [
    "#### Eficacia del flujo de análisis\n",
    "\n",
    "- Al comparar arquitecturas de modelos de lenguaje como BERT y GPT, se identifican similitudes en su capacidad de generar representaciones ricas del lenguaje, algunas diferencias clave en su estructura y en su proceso de entrenamiento.\n",
    "\n",
    "- Ambos modelos demuestran que un pre-entrenamiento sólido y la construcción de embeddings de alta calidad son factores críticos para alcanzar buenos resultados en tareas posteriores, evitando costos de entrenamiento desde cero.\n",
    "\n",
    "#### Rendimiento del modelo\n",
    "\n",
    "- Tanto BERT como GPT pueden adaptarse a una amplia variedad de tareas posteriores (clasificación, generación de texto, análisis semántico, etc.), lo que valida la versatilidad de los enfoques de transfer learning y fine tuning.\n",
    "\n",
    "- La elección del modelo y la estrategia de entrenamiento depende de la tarea: BERT sobresale en comprensión y análisis de contexto bidireccional, mientras que GPT destaca en generación de texto coherente y fluido.\n",
    "\n",
    "#### Limitaciones observadas\n",
    "\n",
    "- Los modelos generativos enfrentan un dilema de exploración–explotación: una decodificación enfocada en la explotación (p. ej. greedy search) brinda mayor precisión pero tiende a producir textos monótonos; en cambio, la exploración (p. ej. sampling con temperatura alta) promueve creatividad y diversidad, pero con riesgo de incoherencia o “alucinaciones”.\n",
    "\n",
    "- La calidad del modelo depende en gran medida de los datos de entrenamiento. Conjuntos de datos sesgados, poco representativos o de baja calidad pueden degradar el desempeño e introducir sesgos o errores difíciles de corregir.\n",
    "\n",
    "#### Áreas de mejora\n",
    "\n",
    "- Profundizar en la selección y curaduría de datasets, asegurando diversidad, equilibrio y relevancia para el dominio de aplicación.\n",
    "\n",
    "- Experimentar con estrategias de decodificación y con la afinación de hiperparámetros para encontrar el punto óptimo entre creatividad, coherencia y precisión.\n",
    "\n",
    "- Explorar técnicas de optimización y compresión que permitan desplegar modelos grandes en entornos de recursos limitados.\n",
    "\n",
    "#### Valor práctico\n",
    "\n",
    "- La adopción de modelos pre-entrenados como GPT ofrece una base sólida y flexible para proyectos de NLP, equilibrando costo, tiempo y calidad.\n",
    "\n",
    "- El entendimiento de los trade-offs entre exploración y explotación, así como la adecuada selección de datos y métodos de decodificación, es esencial para alinear el modelo con los objetivos específicos del negocio y minimizar riesgos de sesgos o resultados no deseados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1089b9-ae81-4ae0-91ec-d323c7c84352",
   "metadata": {
    "id": "4b1089b9-ae81-4ae0-91ec-d323c7c84352"
   },
   "source": [
    "### 8. Apendice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a28e4-5609-4f01-b6f9-60343d6cb413",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "636a28e4-5609-4f01-b6f9-60343d6cb413",
    "outputId": "46f584cf-356e-4e24-9e72-9bc45abc1420"
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "libs = [\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"datasets\",\n",
    "    \"torch\",\n",
    "    \"pytorch-lightning\",\n",
    "    \"torchmetrics\",\n",
    "    \"tqdm\",\n",
    "    \"transformers\",\n",
    "    \"scikit-learn\"\n",
    "]\n",
    "\n",
    "for lib in libs:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(lib).version\n",
    "        print(f\"{lib}=={version}\")\n",
    "    except Exception:\n",
    "        print(f\"{lib}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646eaeca-8b47-46c4-9995-16c1f2ceb0e0",
   "metadata": {
    "id": "646eaeca-8b47-46c4-9995-16c1f2ceb0e0"
   },
   "outputs": [],
   "source": [
    " ## Solo correr en local\n",
    "\n",
    "# import nbformat\n",
    "\n",
    "## Cargar notebook\n",
    "# with open(\"nlp_with_gpt.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
    "    # nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "## Eliminar widgets corruptos si existen\n",
    "# if \"widgets\" in nb[\"metadata\"]:\n",
    "    # del nb[\"metadata\"][\"widgets\"]\n",
    "\n",
    "## Guardar reparado\n",
    "# with open(\"nlp_with_gpt.ipynb\", \"w\", encoding=\"utf-8\") as f:\n",
    "    # nbformat.write(nb, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
